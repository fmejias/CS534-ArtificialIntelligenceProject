{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI_Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fmejias/CS534-ArtificialIntelligenceProject/blob/main/AI_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pj5-J2tBsoGW"
      },
      "source": [
        "# **CS 534 - Artificial Intelligence**\n",
        "\n",
        "## **Project Title: Text Mining and Sentiment Analysis on Twitter for predicting students dropping out during the pandemic.**\n",
        "\n",
        "### **Students**\n",
        "\n",
        "\n",
        "*   Merzia Adamjee\n",
        "*   Alketa Guxha\n",
        "*   Felipe Mejias\n",
        "*   Nikita Boguslavskii\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ia8Bio_dt21i"
      },
      "source": [
        "# **Initial configuration of the environment for the development of the project**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OIfGOowqmL3"
      },
      "source": [
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from imblearn.over_sampling import SMOTE"
      ],
      "execution_count": 445,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHwMawOhc6wI"
      },
      "source": [
        "# **Install Textblob and Imbalanced Learn**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_JgmJsnc9rW"
      },
      "source": [
        "!pip install textblob\n",
        "!pip install imbalanced-learn\n",
        "!pip3 install flair"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_FDX7NI0dTp"
      },
      "source": [
        "# **Google Authentication to read CSV File from Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3moATrP0lQ_"
      },
      "source": [
        "# Needed for Google Authentication Step\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzTa-Jj31bEs"
      },
      "source": [
        "# **Upload Dataset from Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DR3T3xz-1erF"
      },
      "source": [
        "DATASET_PATH = \"/content/drive/My Drive/AI_Project_CS_534/Datasets/dropping_out_tweets_part1_labeled.csv\"\n",
        "dataset_df = pd.read_csv(DATASET_PATH, sep=\";\")"
      ],
      "execution_count": 585,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYx-KyA43FAu"
      },
      "source": [
        "# **Dataset Information**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDtxCzzf3K6d"
      },
      "source": [
        "# Print a summary of the Dataset\n",
        "result = dataset_df.head(10)\n",
        "print(\"First 10 rows of the DataFrame:\")\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbbjj9bAOYQS"
      },
      "source": [
        "# **Dataset Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8rCPzSqPLp9"
      },
      "source": [
        "## **Select labeled dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNL3ca_PPPDJ"
      },
      "source": [
        "# NOTE: This selection is because they are the only label rows\n",
        "labeled_dataset_df = dataset_df.head(1200)"
      ],
      "execution_count": 586,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKc6gdgtOjJi"
      },
      "source": [
        "## **Filtering irrelevant examples**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WdCZhSjOawL",
        "outputId": "2e229e5e-45a6-4446-8d30-3e836816a515",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "IRRELEVANT_KEYWORDS = [\"Bernie\", \"Trump\", \"Sanders\", \"to become\", \n",
        "                       \"to pursue\", \"and becoming\", \"and going\",\n",
        "                       \"and be\", \"so I can\", \"so i can\", \"to run\",\n",
        "                       \"to spend\", \"to focus\", \"and living\", \"marry\",\n",
        "                       \"stripper\", \"and joining\", \"and pursuing\",\n",
        "                       \"bts\", \"BTS\", \"and running\", \"to go\", \"and making\",\n",
        "                       \"to dedicate\"]\n",
        "\n",
        "def filtering_irrelevant_examples(twitter_dataset):\n",
        "  \"\"\"\n",
        "  Filtering irrelevant tweets from the Twitter dataset.\n",
        "  \"\"\"\n",
        "  def check_tweet_relevance(tweet):\n",
        "    \"\"\"\n",
        "    Filtering irrelevant tweets from the Twitter dataset.\n",
        "    \"\"\"\n",
        "    if any(indicator in tweet for indicator in IRRELEVANT_KEYWORDS):\n",
        "      return \"irrelevant\"\n",
        "    return \"relevant\"\n",
        "  return twitter_dataset[twitter_dataset[\"tweet\"].apply(check_tweet_relevance) \\\n",
        "                         != \"irrelevant\"]\n",
        "\n",
        "# Filter the irrelevant tweets\n",
        "labeled_dataset_df = filtering_irrelevant_examples(labeled_dataset_df)\n",
        "rows, columns = labeled_dataset_df.shape\n",
        "print(\"New number of rows: \", rows)"
      ],
      "execution_count": 587,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "New number of rows:  936\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aW0f3PDbT4X8"
      },
      "source": [
        "## **Convert all letters to lower case**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJ2YCm5cT8ST",
        "outputId": "a8c5d7e2-77e0-4b4c-b314-641815c8a36b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def convert_letters_to_lower_case(twitter_dataset):\n",
        "  \"\"\"\n",
        "  Convert all letters to lower case.\n",
        "  \"\"\"\n",
        "  def tweet_to_lower_case(tweet):\n",
        "    \"\"\"\n",
        "    Convert tweet text to lower case.\n",
        "    \"\"\"\n",
        "    return tweet.lower()\n",
        "\n",
        "  twitter_dataset[\"tweet\"] = twitter_dataset[\"tweet\"].apply(tweet_to_lower_case)\n",
        "\n",
        "# Convert all tweets to lower case\n",
        "convert_letters_to_lower_case(labeled_dataset_df)\n",
        "\n",
        "# Show results\n",
        "result = labeled_dataset_df.head(10)\n",
        "print(\"First 10 rows of the DataFrame after lower case:\")\n",
        "print(result)"
      ],
      "execution_count": 588,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 10 rows of the DataFrame after lower case:\n",
            "                     id  ...                     label\n",
            "0   1309219160828895233  ...      Intention of dropout\n",
            "1   1308809031583236096  ...  Not intention of dropout\n",
            "2   1308716552229998593  ...  Not intention of dropout\n",
            "3   1308483739584835585  ...  Not intention of dropout\n",
            "4   1308351921875345409  ...  Not intention of dropout\n",
            "5   1307763236062650368  ...  Not intention of dropout\n",
            "7   1306800980307083267  ...  Not intention of dropout\n",
            "8   1305869148463992832  ...  Not intention of dropout\n",
            "9   1305659343971311616  ...  Not intention of dropout\n",
            "10  1305617685020053512  ...  Not intention of dropout\n",
            "\n",
            "[10 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDE-F9yJVifX"
      },
      "source": [
        "## **Remove usernames that appear within a tweet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpJ-HfSPVm_s",
        "outputId": "e2b25fc5-8977-464d-ee3e-550e98517455",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def remove_usernames_from_tweets(twitter_dataset):\n",
        "  \"\"\"\n",
        "  Remove all usernames that appear on a tweet.\n",
        "  \"\"\"\n",
        "  def remove_username(tweet):\n",
        "    \"\"\"\n",
        "    Remove username from tweet.\n",
        "    \"\"\"\n",
        "    return re.sub('@[\\w]+','', tweet)\n",
        "\n",
        "  twitter_dataset[\"tweet\"] = twitter_dataset[\"tweet\"].apply(remove_username)\n",
        "\n",
        "# Remove all usernames that appear in a tweet\n",
        "remove_usernames_from_tweets(labeled_dataset_df)\n",
        "\n",
        "# Show results\n",
        "result = labeled_dataset_df.head(10)\n",
        "print(\"First 10 rows of the DataFrame after removing usernames:\")\n",
        "print(result)"
      ],
      "execution_count": 589,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 10 rows of the DataFrame after removing usernames:\n",
            "                     id  ...                     label\n",
            "0   1309219160828895233  ...      Intention of dropout\n",
            "1   1308809031583236096  ...  Not intention of dropout\n",
            "2   1308716552229998593  ...  Not intention of dropout\n",
            "3   1308483739584835585  ...  Not intention of dropout\n",
            "4   1308351921875345409  ...  Not intention of dropout\n",
            "5   1307763236062650368  ...  Not intention of dropout\n",
            "7   1306800980307083267  ...  Not intention of dropout\n",
            "8   1305869148463992832  ...  Not intention of dropout\n",
            "9   1305659343971311616  ...  Not intention of dropout\n",
            "10  1305617685020053512  ...  Not intention of dropout\n",
            "\n",
            "[10 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEFpLAeDbxLM"
      },
      "source": [
        "## **Remove hashtags that appear within a tweet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGTsQQ8Hbzav",
        "outputId": "259e4b85-6e5d-4a0a-d9cf-b8421ebb668d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def remove_hashtags_from_tweets(twitter_dataset):\n",
        "  \"\"\"\n",
        "  Remove all hashtags that appear on a tweet.\n",
        "  \"\"\"\n",
        "  def remove_hashtags(tweet):\n",
        "    \"\"\"\n",
        "    Remove hashtags from tweet.\n",
        "    \"\"\"\n",
        "    return tweet.replace(\"#\", \"\").replace(\"_\", \" \")\n",
        "\n",
        "  twitter_dataset[\"tweet\"] = twitter_dataset[\"tweet\"].apply(remove_hashtags)\n",
        "\n",
        "# Remove all hashtags that appear in a tweet\n",
        "remove_hashtags_from_tweets(labeled_dataset_df)\n",
        "\n",
        "# Show results\n",
        "result = labeled_dataset_df.head(10)\n",
        "print(\"First 10 rows of the DataFrame after removing usernames:\")\n",
        "print(result)"
      ],
      "execution_count": 590,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 10 rows of the DataFrame after removing usernames:\n",
            "                     id  ...                     label\n",
            "0   1309219160828895233  ...      Intention of dropout\n",
            "1   1308809031583236096  ...  Not intention of dropout\n",
            "2   1308716552229998593  ...  Not intention of dropout\n",
            "3   1308483739584835585  ...  Not intention of dropout\n",
            "4   1308351921875345409  ...  Not intention of dropout\n",
            "5   1307763236062650368  ...  Not intention of dropout\n",
            "7   1306800980307083267  ...  Not intention of dropout\n",
            "8   1305869148463992832  ...  Not intention of dropout\n",
            "9   1305659343971311616  ...  Not intention of dropout\n",
            "10  1305617685020053512  ...  Not intention of dropout\n",
            "\n",
            "[10 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qXBpdT2Z0HB"
      },
      "source": [
        "## **Remove special characters and punctuation that appear within a tweet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8F884syZ3d2",
        "outputId": "6c3d13ed-1e1d-4ef6-9cb4-a2da9f3a7da0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def remove_special_characters_and_punctuation_from_tweets(twitter_dataset):\n",
        "  \"\"\"\n",
        "  Remove all special characters and punctuation that appear on a tweet.\n",
        "  \"\"\"\n",
        "  def remove_special_characters_and_punctuation(tweet):\n",
        "    \"\"\"\n",
        "    Remove special characters and punctuation from tweet.\n",
        "    \"\"\"\n",
        "    return re.sub('[^A-Za-z0-9 ]+', '', tweet)\n",
        "\n",
        "  twitter_dataset[\"tweet\"] = twitter_dataset[\"tweet\"].apply(remove_special_characters_and_punctuation)\n",
        "\n",
        "# Remove all special characters and punctuation that appear in a tweet\n",
        "remove_special_characters_and_punctuation_from_tweets(labeled_dataset_df)\n",
        "\n",
        "# Show results\n",
        "result = labeled_dataset_df.head(10)\n",
        "print(\"First 10 rows of the DataFrame after removing usernames:\")\n",
        "print(result)"
      ],
      "execution_count": 591,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 10 rows of the DataFrame after removing usernames:\n",
            "                     id  ...                     label\n",
            "0   1309219160828895233  ...      Intention of dropout\n",
            "1   1308809031583236096  ...  Not intention of dropout\n",
            "2   1308716552229998593  ...  Not intention of dropout\n",
            "3   1308483739584835585  ...  Not intention of dropout\n",
            "4   1308351921875345409  ...  Not intention of dropout\n",
            "5   1307763236062650368  ...  Not intention of dropout\n",
            "7   1306800980307083267  ...  Not intention of dropout\n",
            "8   1305869148463992832  ...  Not intention of dropout\n",
            "9   1305659343971311616  ...  Not intention of dropout\n",
            "10  1305617685020053512  ...  Not intention of dropout\n",
            "\n",
            "[10 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alLP6ZB6aq3t"
      },
      "source": [
        "## **Remove URLs that appear within a tweet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQ2MbNsFat1g",
        "outputId": "1357ebd3-3473-4971-ea3a-747beddd185d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def remove_urls_from_tweets(twitter_dataset):\n",
        "  \"\"\"\n",
        "  Remove all urls that appear on a tweet.\n",
        "  \"\"\"\n",
        "  def remove_urls(tweet):\n",
        "    \"\"\"\n",
        "    Remove urls from tweet.\n",
        "    \"\"\"\n",
        "    return re.sub(r'http\\S+', '', tweet)\n",
        "\n",
        "  twitter_dataset[\"tweet\"] = twitter_dataset[\"tweet\"].apply(remove_urls)\n",
        "\n",
        "# Remove all urls that appear in a tweet\n",
        "remove_urls_from_tweets(labeled_dataset_df)\n",
        "\n",
        "# Show results\n",
        "result = labeled_dataset_df.head(10)\n",
        "print(\"First 10 rows of the DataFrame after removing usernames:\")\n",
        "print(result)"
      ],
      "execution_count": 592,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 10 rows of the DataFrame after removing usernames:\n",
            "                     id  ...                     label\n",
            "0   1309219160828895233  ...      Intention of dropout\n",
            "1   1308809031583236096  ...  Not intention of dropout\n",
            "2   1308716552229998593  ...  Not intention of dropout\n",
            "3   1308483739584835585  ...  Not intention of dropout\n",
            "4   1308351921875345409  ...  Not intention of dropout\n",
            "5   1307763236062650368  ...  Not intention of dropout\n",
            "7   1306800980307083267  ...  Not intention of dropout\n",
            "8   1305869148463992832  ...  Not intention of dropout\n",
            "9   1305659343971311616  ...  Not intention of dropout\n",
            "10  1305617685020053512  ...  Not intention of dropout\n",
            "\n",
            "[10 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4Al-2K5MdTG"
      },
      "source": [
        "## **Remove stop words that appear within a tweet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlpChgY8MfdW",
        "outputId": "0e0e43bc-28f2-4f93-8492-e0a711a963a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "\n",
        "def remove_stop_words_from_tweets(twitter_dataset):\n",
        "  \"\"\"\n",
        "  Remove all stop_words that appear on a tweet.\n",
        "  \"\"\"\n",
        "  def remove_stop_words(tweet):\n",
        "    \"\"\"\n",
        "    Remove stop_words from tweet.\n",
        "    \"\"\"\n",
        "    return remove_stopwords(tweet)\n",
        "\n",
        "  twitter_dataset[\"tweet\"] = twitter_dataset[\"tweet\"].apply(remove_stop_words)\n",
        "\n",
        "# Remove all stop_words that appear in a tweet\n",
        "remove_stop_words_from_tweets(labeled_dataset_df)\n",
        "\n",
        "# Show results\n",
        "result = labeled_dataset_df.head(10)\n",
        "print(\"First 10 rows of the DataFrame after removing usernames:\")\n",
        "print(result)"
      ],
      "execution_count": 593,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 10 rows of the DataFrame after removing usernames:\n",
            "                     id  ...                     label\n",
            "0   1309219160828895233  ...      Intention of dropout\n",
            "1   1308809031583236096  ...  Not intention of dropout\n",
            "2   1308716552229998593  ...  Not intention of dropout\n",
            "3   1308483739584835585  ...  Not intention of dropout\n",
            "4   1308351921875345409  ...  Not intention of dropout\n",
            "5   1307763236062650368  ...  Not intention of dropout\n",
            "7   1306800980307083267  ...  Not intention of dropout\n",
            "8   1305869148463992832  ...  Not intention of dropout\n",
            "9   1305659343971311616  ...  Not intention of dropout\n",
            "10  1305617685020053512  ...  Not intention of dropout\n",
            "\n",
            "[10 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjQ2xaSrck31"
      },
      "source": [
        "# **Approach using Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCmqa8TicrPF"
      },
      "source": [
        "## **Create features using sentiment analysis and unigrams and Textblob**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbFqaoU3cq2D",
        "outputId": "76488e9d-f3ec-4bc9-e84b-f7b769c643e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "def calculate_features_using_polarity(twitter_dataset):\n",
        "  \"\"\"\n",
        "  Use Textblob polarity to calculate the number of positive and negative words.\n",
        "  \"\"\"\n",
        "  def calculate_positive_words(tweet):\n",
        "    \"\"\"\n",
        "    Count number of positive words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_positive_words = 0\n",
        "    for word in tweet.split():\n",
        "      if TextBlob(word).polarity > 0:\n",
        "        number_of_positive_words = number_of_positive_words + 1\n",
        "    return number_of_positive_words\n",
        "  \n",
        "  def calculate_positive_tweet_score(tweet):\n",
        "    \"\"\"\n",
        "    Count number of positive words in a tweet.\n",
        "    \"\"\"\n",
        "    score = 0\n",
        "    for word in tweet.split():\n",
        "      polarity_score = TextBlob(word).polarity\n",
        "      if polarity_score > 0:\n",
        "        score = score + polarity_score\n",
        "    return score\n",
        "  \n",
        "  def calculate_ratio_positive_words(tweet):\n",
        "    \"\"\"\n",
        "    Calculate the ratio of positive words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_positive_words = 0\n",
        "    for word in tweet.split():\n",
        "      if TextBlob(word).polarity > 0:\n",
        "        number_of_positive_words = number_of_positive_words + 1\n",
        "    return number_of_positive_words/len(tweet.split())\n",
        "  \n",
        "  def calculate_negative_words(tweet):\n",
        "    \"\"\"\n",
        "    Count number of negative words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_negative_words = 0\n",
        "    for word in tweet.split():\n",
        "      if TextBlob(word).polarity < 0:\n",
        "        number_of_negative_words = number_of_negative_words + 1\n",
        "    return number_of_negative_words\n",
        "  \n",
        "  def calculate_negative_tweet_score(tweet):\n",
        "    \"\"\"\n",
        "    Count number of negative words in a tweet.\n",
        "    \"\"\"\n",
        "    score = 0\n",
        "    for word in tweet.split():\n",
        "      polarity_score = TextBlob(word).polarity\n",
        "      if polarity_score < 0:\n",
        "        score = score + polarity_score\n",
        "    return score\n",
        "  \n",
        "  def calculate_ratio_negative_words(tweet):\n",
        "    \"\"\"\n",
        "    Calculate the ratio of negative words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_negative_words = 0\n",
        "    for word in tweet.split():\n",
        "      if TextBlob(word).polarity < 0:\n",
        "        number_of_negative_words = number_of_negative_words + 1\n",
        "    return number_of_negative_words/len(tweet.split())\n",
        "  \n",
        "  twitter_dataset[\"unigram_number_positive_words\"] = twitter_dataset[\"tweet\"].apply(calculate_positive_words)\n",
        "  twitter_dataset[\"unigram_ratio_positive_words\"] = twitter_dataset[\"tweet\"].apply(calculate_ratio_positive_words)\n",
        "  twitter_dataset[\"unigram_number_negative_words\"] = twitter_dataset[\"tweet\"].apply(calculate_negative_words)\n",
        "  twitter_dataset[\"unigram_ratio_negative_words\"] = twitter_dataset[\"tweet\"].apply(calculate_ratio_negative_words)\n",
        "  twitter_dataset[\"unigram_positive_score\"] = twitter_dataset[\"tweet\"].apply(calculate_positive_tweet_score)\n",
        "  twitter_dataset[\"unigram_negative_score\"] = twitter_dataset[\"tweet\"].apply(calculate_negative_tweet_score)\n",
        "\n",
        "# Calculate new features using sentiment Analysis\n",
        "calculate_features_using_polarity(labeled_dataset_df)\n",
        "\n",
        "# Show results\n",
        "result = labeled_dataset_df.head(10)\n",
        "print(\"First 10 rows of the DataFrame after removing usernames:\")\n",
        "print(result)"
      ],
      "execution_count": 594,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 10 rows of the DataFrame after removing usernames:\n",
            "                     id  ... unigram_negative_score\n",
            "0   1309219160828895233  ...                  -0.40\n",
            "1   1308809031583236096  ...                   0.00\n",
            "2   1308716552229998593  ...                  -0.30\n",
            "3   1308483739584835585  ...                   0.00\n",
            "4   1308351921875345409  ...                  -0.25\n",
            "5   1307763236062650368  ...                   0.00\n",
            "7   1306800980307083267  ...                   0.00\n",
            "8   1305869148463992832  ...                   0.00\n",
            "9   1305659343971311616  ...                   0.00\n",
            "10  1305617685020053512  ...                  -0.70\n",
            "\n",
            "[10 rows x 9 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmwSKyfZ1det"
      },
      "source": [
        "## **Create features using sentiment analysis and bigrams and Textblob**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GacX5i7k1eJd",
        "outputId": "f6ee1e75-270d-44ab-9617-f439e0187f76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "def find_ngrams(n, input_sequence):\n",
        "  # Split sentence into tokens.\n",
        "  tokens = input_sequence.split()\n",
        "  ngrams = []\n",
        "  for i in range(len(tokens) - n + 1):\n",
        "    # Take n consecutive tokens in array.\n",
        "    ngram = tokens[i:i+n]\n",
        "    # Concatenate array items into string.\n",
        "    ngram = ' '.join(ngram)\n",
        "    ngrams.append(ngram)\n",
        "  return ngrams\n",
        "\n",
        "def calculate_bigram_features_using_polarity(twitter_dataset):\n",
        "  \"\"\"\n",
        "  Use Textblob polarity to calculate the number of positive and negative words.\n",
        "  \"\"\"\n",
        "  def calculate_positive_words(tweet):\n",
        "    \"\"\"\n",
        "    Count number of positive words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_positive_bigrams = 0\n",
        "    ngrams = find_ngrams(2, tweet)\n",
        "    for ngram in ngrams:\n",
        "      if TextBlob(ngram).polarity > 0:\n",
        "        number_of_positive_bigrams = number_of_positive_bigrams + 1\n",
        "    return number_of_positive_bigrams\n",
        "  \n",
        "  def calculate_positive_tweet_score(tweet):\n",
        "    \"\"\"\n",
        "    Count number of positive words in a tweet.\n",
        "    \"\"\"\n",
        "    score = 0\n",
        "    ngrams = find_ngrams(2, tweet)\n",
        "    for ngram in ngrams:\n",
        "      polarity_score = TextBlob(ngram).polarity\n",
        "      if polarity_score > 0:\n",
        "        score = score + polarity_score\n",
        "    return score\n",
        "  \n",
        "  def calculate_ratio_positive_words(tweet):\n",
        "    \"\"\"\n",
        "    Calculate the ratio of positive words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_positive_bigrams = 0\n",
        "    ngrams = find_ngrams(2, tweet)\n",
        "    for ngram in ngrams:\n",
        "      if TextBlob(ngram).polarity >= 0:\n",
        "        number_of_positive_bigrams = number_of_positive_bigrams + 1\n",
        "    return number_of_positive_bigrams/len(ngrams)\n",
        "  \n",
        "  def calculate_negative_words(tweet):\n",
        "    \"\"\"\n",
        "    Count number of negative words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_negative_bigrams = 0\n",
        "    ngrams = find_ngrams(2, tweet)\n",
        "    for ngram in ngrams:\n",
        "      if TextBlob(ngram).polarity < 0:\n",
        "        number_of_negative_bigrams = number_of_negative_bigrams + 1\n",
        "    return number_of_negative_bigrams\n",
        "  \n",
        "  def calculate_negative_tweet_score(tweet):\n",
        "    \"\"\"\n",
        "    Count number of negative words in a tweet.\n",
        "    \"\"\"\n",
        "    score = 0\n",
        "    ngrams = find_ngrams(2, tweet)\n",
        "    for ngram in ngrams:\n",
        "      polarity_score = TextBlob(ngram).polarity\n",
        "      if polarity_score < 0:\n",
        "        score = score + polarity_score\n",
        "    return score\n",
        "  \n",
        "  def calculate_ratio_negative_words(tweet):\n",
        "    \"\"\"\n",
        "    Calculate the ratio of negative words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_negative_bigrams = 0\n",
        "    ngrams = find_ngrams(2, tweet)\n",
        "    for ngram in ngrams:\n",
        "      if TextBlob(ngram).polarity < 0:\n",
        "        number_of_negative_bigrams = number_of_negative_bigrams + 1\n",
        "    return number_of_negative_bigrams/len(ngrams)\n",
        "  \n",
        "  twitter_dataset[\"bigram_number_positive_words\"] = twitter_dataset[\"tweet\"].apply(calculate_positive_words)\n",
        "  twitter_dataset[\"bigram_ratio_positive_words\"] = twitter_dataset[\"tweet\"].apply(calculate_ratio_positive_words)\n",
        "  twitter_dataset[\"bigram_number_negative_words\"] = twitter_dataset[\"tweet\"].apply(calculate_negative_words)\n",
        "  twitter_dataset[\"bigram_ratio_negative_words\"] = twitter_dataset[\"tweet\"].apply(calculate_ratio_negative_words)\n",
        "  twitter_dataset[\"bigram_positive_score\"] = twitter_dataset[\"tweet\"].apply(calculate_positive_tweet_score)\n",
        "  twitter_dataset[\"bigram_negative_score\"] = twitter_dataset[\"tweet\"].apply(calculate_negative_tweet_score)\n",
        "\n",
        "# Calculate new features using sentiment Analysis\n",
        "calculate_bigram_features_using_polarity(labeled_dataset_df)\n",
        "\n",
        "# Show results\n",
        "result = labeled_dataset_df.head(10)\n",
        "print(\"First 10 rows of the DataFrame after removing usernames:\")\n",
        "print(result)"
      ],
      "execution_count": 595,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 10 rows of the DataFrame after removing usernames:\n",
            "                     id  ... bigram_negative_score\n",
            "0   1309219160828895233  ...                 -0.80\n",
            "1   1308809031583236096  ...                  0.00\n",
            "2   1308716552229998593  ...                 -0.60\n",
            "3   1308483739584835585  ...                  0.00\n",
            "4   1308351921875345409  ...                 -0.50\n",
            "5   1307763236062650368  ...                  0.00\n",
            "7   1306800980307083267  ...                  0.00\n",
            "8   1305869148463992832  ...                  0.00\n",
            "9   1305659343971311616  ...                  0.00\n",
            "10  1305617685020053512  ...                 -0.75\n",
            "\n",
            "[10 rows x 15 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcmx83MY2wKy"
      },
      "source": [
        "## **Create features using sentiment analysis and unigrams and Vader**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7KUiYVY2zQk"
      },
      "source": [
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "nltk.download('vader_lexicon')\n",
        "sentiment_analyzer = SentimentIntensityAnalyzer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOsUQl4X23xl",
        "outputId": "1ecb2192-a61d-4510-d68e-595eaffb4956",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def calculate_features_using_polarity_vader(twitter_dataset):\n",
        "  \"\"\"\n",
        "  Use Vader polarity to calculate the number of positive and negative words.\n",
        "  \"\"\"\n",
        "  def calculate_positive_words(tweet):\n",
        "    \"\"\"\n",
        "    Count number of positive words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_positive_words = 0\n",
        "    for word in tweet.split():\n",
        "      if sentiment_analyzer.polarity_scores(word)[\"compound\"] > 0:\n",
        "        number_of_positive_words = number_of_positive_words + 1\n",
        "    return number_of_positive_words\n",
        "  \n",
        "  def calculate_ratio_positive_words(tweet):\n",
        "    \"\"\"\n",
        "    Calculate the ratio of positive words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_positive_words = 0\n",
        "    for word in tweet.split():\n",
        "      if sentiment_analyzer.polarity_scores(word)[\"compound\"] > 0:\n",
        "        number_of_positive_words = number_of_positive_words + 1\n",
        "    return number_of_positive_words/len(tweet.split())\n",
        "  \n",
        "  def calculate_negative_words(tweet):\n",
        "    \"\"\"\n",
        "    Count number of negative words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_negative_words = 0\n",
        "    for word in tweet.split():\n",
        "      if sentiment_analyzer.polarity_scores(word)[\"compound\"] < 0:\n",
        "        number_of_negative_words = number_of_negative_words + 1\n",
        "    return number_of_negative_words\n",
        "  \n",
        "  def calculate_ratio_negative_words(tweet):\n",
        "    \"\"\"\n",
        "    Calculate the ratio of negative words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_negative_words = 0\n",
        "    for word in tweet.split():\n",
        "      if sentiment_analyzer.polarity_scores(word)[\"compound\"] < 0:\n",
        "        number_of_negative_words = number_of_negative_words + 1\n",
        "    return number_of_negative_words/len(tweet.split())\n",
        "  \n",
        "  twitter_dataset[\"unigram_vader_positive_words\"] = twitter_dataset[\"tweet\"].apply(calculate_positive_words)\n",
        "  twitter_dataset[\"unigram_vader_ratio_positive_words\"] = twitter_dataset[\"tweet\"].apply(calculate_ratio_positive_words)\n",
        "  twitter_dataset[\"unigram_vader_negative_words\"] = twitter_dataset[\"tweet\"].apply(calculate_negative_words)\n",
        "  twitter_dataset[\"unigram_vader_ratio_negative_words\"] = twitter_dataset[\"tweet\"].apply(calculate_ratio_negative_words)\n",
        "\n",
        "# Calculate new features using sentiment Analysis\n",
        "calculate_features_using_polarity_vader(labeled_dataset_df)\n",
        "\n",
        "# Show results\n",
        "result = labeled_dataset_df.head(10)\n",
        "print(\"First 10 rows of the DataFrame after removing usernames:\")\n",
        "print(result)"
      ],
      "execution_count": 597,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 10 rows of the DataFrame after removing usernames:\n",
            "                     id  ... unigram_vader_ratio_negative_words\n",
            "0   1309219160828895233  ...                           0.111111\n",
            "1   1308809031583236096  ...                           0.047619\n",
            "2   1308716552229998593  ...                           0.173913\n",
            "3   1308483739584835585  ...                           0.142857\n",
            "4   1308351921875345409  ...                           0.181818\n",
            "5   1307763236062650368  ...                           0.047619\n",
            "7   1306800980307083267  ...                           0.000000\n",
            "8   1305869148463992832  ...                           0.055556\n",
            "9   1305659343971311616  ...                           0.000000\n",
            "10  1305617685020053512  ...                           0.111111\n",
            "\n",
            "[10 rows x 19 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hfyn2n3pOBrI"
      },
      "source": [
        "## **Select features to train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZeRL0YaOENd"
      },
      "source": [
        "# Select calculated features\n",
        "dataset_sentiment_features = labeled_dataset_df[[\"unigram_number_positive_words\", \n",
        "                                                 \"unigram_ratio_positive_words\", \n",
        "                                                 \"unigram_number_negative_words\", \n",
        "                                                 \"unigram_ratio_negative_words\",\n",
        "                                                 \"unigram_positive_score\",\n",
        "                                                 \"unigram_negative_score\",\n",
        "                                                 \"bigram_number_positive_words\", \n",
        "                                                 \"bigram_ratio_positive_words\", \n",
        "                                                 \"bigram_number_negative_words\", \n",
        "                                                 \"bigram_ratio_negative_words\",\n",
        "                                                 \"bigram_positive_score\",\n",
        "                                                 \"bigram_negative_score\",\n",
        "                                                 \"unigram_vader_positive_words\", \n",
        "                                                 \"unigram_vader_ratio_positive_words\", \n",
        "                                                 \"unigram_vader_negative_words\", \n",
        "                                                 \"unigram_vader_ratio_negative_words\"]]"
      ],
      "execution_count": 728,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2Wpltcvj-AX"
      },
      "source": [
        "## **Normalize calculated features using Pandas**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zanC63BTkBam",
        "outputId": "20ee1034-1af0-43f4-fa76-63738ad98460",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Normalize using the mean value\n",
        "normalized_df = (dataset_sentiment_features - dataset_sentiment_features.mean())/dataset_sentiment_features.std()\n",
        "\n",
        "# Show results\n",
        "result = normalized_df.head(10)\n",
        "print(\"First 10 rows of the DataFrame after removing usernames:\")\n",
        "print(result)"
      ],
      "execution_count": 524,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 10 rows of the DataFrame after removing usernames:\n",
            "    unigram_number_positive_words  ...  unigram_vader_ratio_negative_words\n",
            "0                        0.351245  ...                            0.592879\n",
            "1                        0.351245  ...                           -0.281145\n",
            "2                        2.610803  ...                            1.457402\n",
            "3                       -0.778534  ...                            1.029891\n",
            "4                       -0.778534  ...                            1.566223\n",
            "5                       -0.778534  ...                           -0.281145\n",
            "7                        0.351245  ...                           -0.936663\n",
            "8                        1.481024  ...                           -0.171892\n",
            "9                       -0.778534  ...                           -0.936663\n",
            "10                       1.481024  ...                            0.592879\n",
            "\n",
            "[10 rows x 16 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETkakKds9KTx"
      },
      "source": [
        "## **Normalize calculated features using Scaler**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8uJuvse9NMW"
      },
      "source": [
        "# Feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Normalize using the mean value\n",
        "sc = StandardScaler()\n",
        "normalized_df = sc.fit_transform(dataset_sentiment_features)"
      ],
      "execution_count": 430,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTIRTpijLo4U"
      },
      "source": [
        "## **Normalize calculated features using MinMaxScaler**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqEniXM2LqmE"
      },
      "source": [
        "# Feature Scaling\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Normalize using the mean value\n",
        "sc = MinMaxScaler()\n",
        "normalized_df = sc.fit_transform(dataset_sentiment_features)"
      ],
      "execution_count": 731,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZboHv80POY2G"
      },
      "source": [
        "## **Select X and Y**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mk5i1lzAOblu"
      },
      "source": [
        "# Select the data\n",
        "def select_data(normalize_data = False):\n",
        "  if normalize_data:\n",
        "    return normalized_df, labeled_dataset_df.label\n",
        "  return dataset_sentiment_features, labeled_dataset_df.label\n",
        "\n",
        "X, Y = select_data(normalize_data = True)"
      ],
      "execution_count": 732,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KszH2igXfDBk"
      },
      "source": [
        "## **Handle imbalance classes using SMOTE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRylaSqlfGm8"
      },
      "source": [
        "print(\"Number of rows with intention of dropout: \", \n",
        "      len(labeled_dataset_df[(labeled_dataset_df['label'] == \"Intention of dropout\")]))\n",
        "print(\"Number of rows with no intention of dropout: \", \n",
        "      len(labeled_dataset_df[(labeled_dataset_df['label'] == \"Not intention of dropout\")]))\n",
        "\n",
        "# Transform the dataset\n",
        "oversample = SMOTE()\n",
        "X, Y = oversample.fit_resample(X, Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVx8PKHrlbfd"
      },
      "source": [
        "## **Train logistic regression model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96YgCYl2liH8",
        "outputId": "c47def88-b813-4e45-b054-78756840aea1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Train the model\n",
        "X_train, X_test, Y_train, Y_test = tts(X, Y, test_size = 0.2)\n",
        "logistic_classifier = LogisticRegression(random_state = 0).fit(X_train,\n",
        "                                                               Y_train)\n",
        "\n",
        "# Calculate training accuracy\n",
        "Y_pred = logistic_classifier.predict(X_train)\n",
        "print(\"Training Accuracy: \", accuracy_score(Y_train, Y_pred))\n",
        "\n",
        "# Calculate test accuracy\n",
        "Y_pred = logistic_classifier.predict(X_test)\n",
        "print(\"Testing Accuracy: \", accuracy_score(Y_test, Y_pred))"
      ],
      "execution_count": 736,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy:  0.6786079836233367\n",
            "Testing Accuracy:  0.6816326530612244\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCkMOdAP5z7W"
      },
      "source": [
        "## **Train Random Forest Classifier model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B83DcWr452qk",
        "outputId": "1f18f741-5755-4c60-e361-3da66b43d147",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Train the model\n",
        "X_train, X_test, Y_train, Y_test = tts(X, Y, test_size = 0.2)\n",
        "random_forest_classifier = RandomForestClassifier(n_estimators=100)\n",
        "random_forest_classifier.fit(X_train, Y_train)\n",
        "\n",
        "# Calculate training accuracy\n",
        "Y_pred = random_forest_classifier.predict(X_train)\n",
        "print(\"Training Accuracy: \", accuracy_score(Y_train, Y_pred))\n",
        "\n",
        "# Calculate test accuracy\n",
        "Y_pred = random_forest_classifier.predict(X_test)\n",
        "print(\"Testing Accuracy: \", accuracy_score(Y_test, Y_pred))"
      ],
      "execution_count": 738,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy:  0.932446264073695\n",
            "Testing Accuracy:  0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HWCZOwZlQKY"
      },
      "source": [
        "## **Train Adaboost Classifier model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ah2nhndTlUf8",
        "outputId": "42e352ae-b516-48c3-e1d4-88e37b67e1fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Train the model\n",
        "X_train, X_test, Y_train, Y_test = tts(X, Y, test_size = 0.2)\n",
        "adaboost_classifier = AdaBoostClassifier(n_estimators = 100)\n",
        "adaboost_classifier.fit(X_train, Y_train)\n",
        "\n",
        "# Calculate training accuracy\n",
        "Y_pred = adaboost_classifier.predict(X_train)\n",
        "print(\"Training Accuracy: \", accuracy_score(Y_train, Y_pred))\n",
        "\n",
        "# Calculate test accuracy\n",
        "Y_pred = adaboost_classifier.predict(X_test)\n",
        "print(\"Testing Accuracy: \", accuracy_score(Y_test, Y_pred))"
      ],
      "execution_count": 780,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy:  0.7850562947799385\n",
            "Testing Accuracy:  0.7673469387755102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6t_EbnvJmNvJ"
      },
      "source": [
        "## **Train Gradient Boost Classifier model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fiI9mi0mQFX",
        "outputId": "b8179967-5d4d-4dc6-d717-897660ce5cc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Train the model\n",
        "X_train, X_test, Y_train, Y_test = tts(X, Y, test_size = 0.2)\n",
        "gradient_boost_classifier = GradientBoostingClassifier(n_estimators = 100)\n",
        "gradient_boost_classifier.fit(X_train, Y_train)\n",
        "\n",
        "# Calculate training accuracy\n",
        "Y_pred = gradient_boost_classifier.predict(X_train)\n",
        "print(\"Training Accuracy: \", accuracy_score(Y_train, Y_pred))\n",
        "\n",
        "# Calculate test accuracy\n",
        "Y_pred = gradient_boost_classifier.predict(X_test)\n",
        "print(\"Testing Accuracy: \", accuracy_score(Y_test, Y_pred))"
      ],
      "execution_count": 787,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy:  0.8669396110542477\n",
            "Testing Accuracy:  0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWIPexS-otho"
      },
      "source": [
        "## **Train XG Gradient Boost Classifier model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKmMlcaSovku",
        "outputId": "eceb2417-4531-4b3b-ebbc-1b870529a675",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Train the model\n",
        "X_train, X_test, Y_train, Y_test = tts(X, Y, test_size = 0.2)\n",
        "xg_gradient_boost_classifier = xgb.XGBClassifier(objective=\"binary:logistic\", \n",
        "                                                 random_state=42)\n",
        "xg_gradient_boost_classifier.fit(X_train, Y_train)\n",
        "\n",
        "# Calculate training accuracy\n",
        "Y_pred = xg_gradient_boost_classifier.predict(X_train)\n",
        "print(\"Training Accuracy: \", accuracy_score(Y_train, Y_pred))\n",
        "\n",
        "# Calculate test accuracy\n",
        "Y_pred = xg_gradient_boost_classifier.predict(X_test)\n",
        "print(\"Testing Accuracy: \", accuracy_score(Y_test, Y_pred))"
      ],
      "execution_count": 807,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy:  0.8372569089048106\n",
            "Testing Accuracy:  0.7306122448979592\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Q20s8MPK15D"
      },
      "source": [
        "## **Train Multinomial Naive Bayes Classifier model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRpCNGs9K5Xp",
        "outputId": "2e66a6d5-94e7-435e-ce74-199c007753a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Train the model\n",
        "X_train, X_test, Y_train, Y_test = tts(X, Y, test_size = 0.2)\n",
        "multinomial_naive_bayes_classifier = MultinomialNB()\n",
        "multinomial_naive_bayes_classifier.fit(X_train, Y_train)\n",
        "\n",
        "# Calculate training accuracy\n",
        "Y_pred = multinomial_naive_bayes_classifier.predict(X_train)\n",
        "\n",
        "# Calculate the accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(\"Accuracy: \", accuracy_score(Y_train, Y_pred))"
      ],
      "execution_count": 740,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  0.6212896622313203\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-byGimkU4kc"
      },
      "source": [
        "## **Train Decision Tree Classifier model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7YhwL8AU9NT",
        "outputId": "c9ce8867-69ea-4538-bcf6-f32472de0f98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Train the model\n",
        "X_train, X_test, Y_train, Y_test = tts(X, Y, test_size = 0.2)\n",
        "decision_tree_classifier = DecisionTreeClassifier()\n",
        "decision_tree_classifier.fit(X_train, Y_train)\n",
        "\n",
        "# Calculate training accuracy\n",
        "Y_pred = decision_tree_classifier.predict(X_train)\n",
        "print(\"Training Accuracy: \", accuracy_score(Y_train, Y_pred))\n",
        "\n",
        "# Calculate test accuracy\n",
        "Y_pred = decision_tree_classifier.predict(X_test)\n",
        "print(\"Testing Accuracy: \", accuracy_score(Y_test, Y_pred))"
      ],
      "execution_count": 741,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy:  0.9314227226202662\n",
            "Testing Accuracy:  0.7959183673469388\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkcyuOJ9Ug8m"
      },
      "source": [
        "## **Train SVM model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ab3T5gt0Uj7a",
        "outputId": "06f24fad-febc-49d1-bf49-b5ebb61c2318",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Train the model\n",
        "X_train, X_test, Y_train, Y_test = tts(X, Y, test_size = 0.2)\n",
        "svm_classifier = SVC()\n",
        "svm_classifier.fit(X_train, Y_train)\n",
        "\n",
        "# Calculate training accuracy\n",
        "Y_pred = svm_classifier.predict(X_train)\n",
        "print(\"Training Accuracy: \", accuracy_score(Y_train, Y_pred))\n",
        "\n",
        "# Calculate test accuracy\n",
        "Y_pred = svm_classifier.predict(X_test)\n",
        "print(\"Testing Accuracy: \", accuracy_score(Y_test, Y_pred))"
      ],
      "execution_count": 744,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy:  0.7093142272262026\n",
            "Testing Accuracy:  0.726530612244898\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZTykP8IZs0j"
      },
      "source": [
        "## **Train Stacking Ensemble model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCVOhqDNZymJ",
        "outputId": "77d83449-5b18-4f26-c8c2-213280a48949",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define the base learners models\n",
        "base_learners = list()\n",
        "base_learners.append(('bayes', GaussianNB()))\n",
        "base_learners.append(('rf', RandomForestClassifier(n_estimators=100)))\n",
        "base_learners.append(('cart', DecisionTreeClassifier()))\n",
        "base_learners.append(('svm', SVC()))\n",
        "\n",
        "# Define the meta learner model\n",
        "meta_learner = LogisticRegression()\n",
        "\n",
        "# Define the stacking ensemble\n",
        "stacking_ensemble = StackingClassifier(estimators = base_learners, \n",
        "                                       final_estimator = meta_learner, \n",
        "                                       cv = 4)\n",
        "\n",
        "# Train the model\n",
        "X_train, X_test, Y_train, Y_test = tts(X, Y, test_size = 0.2)\n",
        "stacking_ensemble.fit(X_train, Y_train)\n",
        "\n",
        "# Calculate training accuracy\n",
        "Y_pred = stacking_ensemble.predict(X_train)\n",
        "print(\"Training Accuracy: \", accuracy_score(Y_train, Y_pred))\n",
        "\n",
        "# Calculate test accuracy\n",
        "Y_pred = stacking_ensemble.predict(X_test)\n",
        "print(\"Testing Accuracy: \", accuracy_score(Y_test, Y_pred))"
      ],
      "execution_count": 814,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy:  0.9293756397134084\n",
            "Testing Accuracy:  0.7836734693877551\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}