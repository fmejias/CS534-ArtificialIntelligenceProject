{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI_Project_with_all_proposals.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2b1f996a6fc2497093fafb9b03936c8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_cb5b4fded69d4ead96092a6206f5eafc",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3574d68568b64fb486053f5c72fc710c",
              "IPY_MODEL_452b341f817b40c984424b5ec804d6f0"
            ]
          }
        },
        "cb5b4fded69d4ead96092a6206f5eafc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3574d68568b64fb486053f5c72fc710c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_66808ee5b9c242e093646d1c0f40a2d8",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_86f427375e8747948708f3f3bc6b3198"
          }
        },
        "452b341f817b40c984424b5ec804d6f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_81697c4bc7914f60b74332ed03d796ca",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 305kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0dfc82ee5d074be496f866306e633ede"
          }
        },
        "66808ee5b9c242e093646d1c0f40a2d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "86f427375e8747948708f3f3bc6b3198": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "81697c4bc7914f60b74332ed03d796ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0dfc82ee5d074be496f866306e633ede": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "565d0dd58484448882ad9ec16e96984f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_565abd3638eb447683ddbe1869f1f9f6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ab40f1ecacbd4143aa841bea29c7ed79",
              "IPY_MODEL_7d65ba2cd6e048f0a15cadccfd564fc3"
            ]
          }
        },
        "565abd3638eb447683ddbe1869f1f9f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ab40f1ecacbd4143aa841bea29c7ed79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b9f63b9311064451a357f88cf996a73e",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 442,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 442,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f293b33a124f4d52ab501b7d03123147"
          }
        },
        "7d65ba2cd6e048f0a15cadccfd564fc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6267b0a4c1e64a659aa4c7402df60086",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 442/442 [00:04&lt;00:00, 89.2B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d9eb54310deb44b7bc2ef083abf8673f"
          }
        },
        "b9f63b9311064451a357f88cf996a73e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f293b33a124f4d52ab501b7d03123147": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6267b0a4c1e64a659aa4c7402df60086": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d9eb54310deb44b7bc2ef083abf8673f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "83c95d8a5f764b1f971dee8f463a519b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_09a0350bf81b4a04be05955f729ab559",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9abb4109c19346cb89362883aa954c37",
              "IPY_MODEL_adccc33d13f0437f8ded07eaa23c13dc"
            ]
          }
        },
        "09a0350bf81b4a04be05955f729ab559": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9abb4109c19346cb89362883aa954c37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_413c1a724e874765badc16c2964dd88c",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 267967963,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 267967963,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7dc87e8f5a9043f78c6e93dd25e4b354"
          }
        },
        "adccc33d13f0437f8ded07eaa23c13dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_276cc59c95434c809a4c5885fbda9042",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 268M/268M [00:04&lt;00:00, 56.5MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2e7e3ee38b3943f98512e2361bc484c1"
          }
        },
        "413c1a724e874765badc16c2964dd88c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7dc87e8f5a9043f78c6e93dd25e4b354": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "276cc59c95434c809a4c5885fbda9042": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2e7e3ee38b3943f98512e2361bc484c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dcbb89c8757a419eada992f4b5d4cabf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a218b4a195a24402a18332255459ce24",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ee473f20642e43e6acbd9578fcf0391b",
              "IPY_MODEL_5debfe1864e34f9ea1863902312264a3"
            ]
          }
        },
        "a218b4a195a24402a18332255459ce24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ee473f20642e43e6acbd9578fcf0391b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1acb1936b86c4da5928abcb4639c7e8f",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2418d3363e3f4424af3d547ecfe9b3cc"
          }
        },
        "5debfe1864e34f9ea1863902312264a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1802d3a94d614375963eaadd2a64c365",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:00&lt;00:00, 7.66kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8f844c88fa274cd581336b19f4bf081c"
          }
        },
        "1acb1936b86c4da5928abcb4639c7e8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2418d3363e3f4424af3d547ecfe9b3cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1802d3a94d614375963eaadd2a64c365": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8f844c88fa274cd581336b19f4bf081c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6360d01d610a4b49a7e2df00c1f848e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_23b7f0b138424229968bbde93b8cb181",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e185a9aaa1af4c76abf0695bf9fc0068",
              "IPY_MODEL_74e6987787a34cd9a566f5fce63e0978"
            ]
          }
        },
        "23b7f0b138424229968bbde93b8cb181": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e185a9aaa1af4c76abf0695bf9fc0068": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b6e3b7f50ccf4b039bf875542aafc3e7",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_989198b15f0a455f8eecd4a5df9c46e8"
          }
        },
        "74e6987787a34cd9a566f5fce63e0978": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_16a45d659d864cca8b8d87f5d7a57269",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [00:09&lt;00:00, 46.7MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5162dc435e7340b9abf6f63a30f15f3e"
          }
        },
        "b6e3b7f50ccf4b039bf875542aafc3e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "989198b15f0a455f8eecd4a5df9c46e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "16a45d659d864cca8b8d87f5d7a57269": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5162dc435e7340b9abf6f63a30f15f3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e3d4013bd36746a5851199ce29ddffcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a38019a618574af9a8398fe9e9e394c2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cef930adce3246dcbda3d9e7a6068e73",
              "IPY_MODEL_d5e8c2bc71f347ea9a1e309f18819786"
            ]
          }
        },
        "a38019a618574af9a8398fe9e9e394c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cef930adce3246dcbda3d9e7a6068e73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_edf2ff01e4984573aa5f3555aa3aba2e",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5d1addb8d76e4c5587aca54278b9f43c"
          }
        },
        "d5e8c2bc71f347ea9a1e309f18819786": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_27c1692d5cb644879b212598a155b5b6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:03&lt;00:00, 71.8kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9512f0d222eb45b59840adf7462e28d2"
          }
        },
        "edf2ff01e4984573aa5f3555aa3aba2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5d1addb8d76e4c5587aca54278b9f43c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "27c1692d5cb644879b212598a155b5b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9512f0d222eb45b59840adf7462e28d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "abde53c680d04725a9b6090b295cf04a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e58fe2ae9c6d4b22870b77ab069c81f8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2c61880677f54dc6b81bea0a81c6d0d5",
              "IPY_MODEL_b2be9e2683f54677a5ff6676153acd86"
            ]
          }
        },
        "e58fe2ae9c6d4b22870b77ab069c81f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2c61880677f54dc6b81bea0a81c6d0d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d846146036ec442cafa0bbc4b8d4f232",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 466062,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 466062,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2bf1c8907b784be0b54b80c4673d3e89"
          }
        },
        "b2be9e2683f54677a5ff6676153acd86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c26a558797f64b7f9fefd741747d21b8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 466k/466k [00:00&lt;00:00, 484kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2d7d35b80ee240ea830f60daa6afbe9e"
          }
        },
        "d846146036ec442cafa0bbc4b8d4f232": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2bf1c8907b784be0b54b80c4673d3e89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c26a558797f64b7f9fefd741747d21b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2d7d35b80ee240ea830f60daa6afbe9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9150eaebc2a4441fa37016bcf8f7787c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ec95ff112f79497f9c81258bf4db5c25",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f82b46ef7c4946e2b8a0e5114d1d3ecd",
              "IPY_MODEL_9b506b1e0045408396f22a07c4208acc"
            ]
          }
        },
        "ec95ff112f79497f9c81258bf4db5c25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f82b46ef7c4946e2b8a0e5114d1d3ecd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8333ad69b2e842a4ad28ff6a1082583d",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 798011,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 798011,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_04acc9ca63c7438dbec920adb46ed6e9"
          }
        },
        "9b506b1e0045408396f22a07c4208acc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_49173aa493b945d7bfe1ea1c87fd675b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 798k/798k [00:10&lt;00:00, 73.8kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_362929522401404dbb07a2cc060ffb54"
          }
        },
        "8333ad69b2e842a4ad28ff6a1082583d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "04acc9ca63c7438dbec920adb46ed6e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "49173aa493b945d7bfe1ea1c87fd675b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "362929522401404dbb07a2cc060ffb54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fmejias/CS534-ArtificialIntelligenceProject/blob/main/AI_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pj5-J2tBsoGW"
      },
      "source": [
        "# **CS 534 - Artificial Intelligence**\n",
        "\n",
        "## **Project Title: Text Mining and Sentiment Analysis on Twitter for predicting students dropping out during the pandemic.**\n",
        "\n",
        "### **Students**\n",
        "\n",
        "\n",
        "*   Merzia Adamjee\n",
        "*   Alketa Guxha\n",
        "*   Felipe Mejias\n",
        "*   Nikita Boguslavskii\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ia8Bio_dt21i"
      },
      "source": [
        "# **Initial configuration of the environment for the development of the project**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OIfGOowqmL3",
        "outputId": "d22ee3f3-927c-496d-dfc0-0404ca61545a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHwMawOhc6wI"
      },
      "source": [
        "# **Install Textblob and Imbalanced Learn**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_JgmJsnc9rW"
      },
      "source": [
        "!pip install textblob\n",
        "!pip install imbalanced-learn\n",
        "!pip3 install flair"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_FDX7NI0dTp"
      },
      "source": [
        "# **Google Authentication to read CSV File from Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3moATrP0lQ_",
        "outputId": "1fbbc81d-4293-4a7e-f52d-20431b174737",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Needed for Google Authentication Step\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzTa-Jj31bEs"
      },
      "source": [
        "# **Upload Dataset from Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DR3T3xz-1erF"
      },
      "source": [
        "DATASET_PATH = \"/content/drive/My Drive/AI_Project_CS_534/Datasets/dropping_out_tweets_part1_labeled.csv\"\n",
        "dataset_df = pd.read_csv(DATASET_PATH, sep=\";\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYx-KyA43FAu"
      },
      "source": [
        "# **Dataset Information**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDtxCzzf3K6d"
      },
      "source": [
        "# Print a summary of the Dataset\n",
        "result = dataset_df.head(10)\n",
        "print(\"First 10 rows of the DataFrame:\")\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbbjj9bAOYQS"
      },
      "source": [
        "# **Dataset Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8rCPzSqPLp9"
      },
      "source": [
        "## **Select labeled dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNL3ca_PPPDJ"
      },
      "source": [
        "# NOTE: This selection is because all the dataset is labeled\n",
        "labeled_dataset_df = dataset_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pml68I5Kaxhl"
      },
      "source": [
        "## **Check for imbalance classes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DfR_azRa1AG",
        "outputId": "91c778a4-c5b8-4c2f-e621-876851505008",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"Number of rows with intention of dropout: \", \n",
        "      len(labeled_dataset_df[(labeled_dataset_df['label'] == \"Intention of dropout\")]))\n",
        "print(\"Number of rows with no intention of dropout: \", \n",
        "      len(labeled_dataset_df[(labeled_dataset_df['label'] == \"Not intention of dropout\")]))\n",
        "\n",
        "sns.countplot(labeled_dataset_df.label)\n",
        "plt.xlabel('Tweet intention to dropout');"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of rows with intention of dropout:  1094\n",
            "Number of rows with no intention of dropout:  2025\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXaZB-1XbicD"
      },
      "source": [
        "## **Convert label to numerical value**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYWP3kambgeo"
      },
      "source": [
        "# Convert label categorical value to numerical value\n",
        "label_numerical_value = labeled_dataset_df[\"label\"].astype('category').cat.codes\n",
        "labeled_dataset_df[\"label\"] = label_numerical_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKc6gdgtOjJi"
      },
      "source": [
        "## **Filtering irrelevant examples**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WdCZhSjOawL",
        "outputId": "87f9195a-9b2a-4275-a5bb-f1833ebea8fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "IRRELEVANT_KEYWORDS = [\"Bernie\", \"Trump\", \"Sanders\", \"to become\", \n",
        "                       \"to pursue\", \"and becoming\", \"and going\",\n",
        "                       \"and be\", \"so I can\", \"so i can\", \"to run\",\n",
        "                       \"to spend\", \"to focus\", \"and living\", \"marry\",\n",
        "                       \"stripper\", \"and joining\", \"and pursuing\",\n",
        "                       \"bts\", \"BTS\", \"and running\", \"to go\", \"and making\",\n",
        "                       \"to dedicate\"]\n",
        "\n",
        "def filtering_irrelevant_examples(twitter_dataset):\n",
        "  \"\"\"\n",
        "  Filtering irrelevant tweets from the Twitter dataset.\n",
        "  \"\"\"\n",
        "  def check_tweet_relevance(tweet):\n",
        "    \"\"\"\n",
        "    Filtering irrelevant tweets from the Twitter dataset.\n",
        "    \"\"\"\n",
        "    if any(indicator in tweet for indicator in IRRELEVANT_KEYWORDS):\n",
        "      return \"irrelevant\"\n",
        "    return \"relevant\"\n",
        "  return twitter_dataset[twitter_dataset[\"tweet\"].apply(check_tweet_relevance) \\\n",
        "                         != \"irrelevant\"]\n",
        "\n",
        "# Filter the irrelevant tweets\n",
        "labeled_dataset_df = filtering_irrelevant_examples(labeled_dataset_df)\n",
        "rows, columns = labeled_dataset_df.shape\n",
        "print(\"New number of rows: \", rows)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "New number of rows:  2577\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aW0f3PDbT4X8"
      },
      "source": [
        "## **Convert all letters to lower case**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJ2YCm5cT8ST",
        "outputId": "e2496e83-6a6e-4add-c1e9-a424bea6f970",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def convert_letters_to_lower_case(twitter_dataset):\n",
        "  \"\"\"\n",
        "  Convert all letters to lower case.\n",
        "  \"\"\"\n",
        "  def tweet_to_lower_case(tweet):\n",
        "    \"\"\"\n",
        "    Convert tweet text to lower case.\n",
        "    \"\"\"\n",
        "    return tweet.lower()\n",
        "\n",
        "  twitter_dataset[\"tweet\"] = twitter_dataset[\"tweet\"].apply(tweet_to_lower_case)\n",
        "\n",
        "# Convert all tweets to lower case\n",
        "convert_letters_to_lower_case(labeled_dataset_df)\n",
        "\n",
        "# Show results\n",
        "result = labeled_dataset_df.head(10)\n",
        "print(\"First 10 rows of the DataFrame after lower case:\")\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 10 rows of the DataFrame after lower case:\n",
            "                     id  ... label\n",
            "0   1309219160828895233  ...     0\n",
            "1   1308809031583236096  ...     1\n",
            "2   1308716552229998593  ...     1\n",
            "3   1308483739584835585  ...     1\n",
            "4   1308351921875345409  ...     1\n",
            "5   1307763236062650368  ...     1\n",
            "7   1306800980307083267  ...     1\n",
            "8   1305869148463992832  ...     1\n",
            "9   1305659343971311616  ...     1\n",
            "10  1305617685020053512  ...     1\n",
            "\n",
            "[10 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDE-F9yJVifX"
      },
      "source": [
        "## **Remove usernames that appear within a tweet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpJ-HfSPVm_s",
        "outputId": "ac08948e-bb38-41f0-8d96-6e4ac9c26c59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def remove_usernames_from_tweets(twitter_dataset):\n",
        "  \"\"\"\n",
        "  Remove all usernames that appear on a tweet.\n",
        "  \"\"\"\n",
        "  def remove_username(tweet):\n",
        "    \"\"\"\n",
        "    Remove username from tweet.\n",
        "    \"\"\"\n",
        "    return re.sub('@[\\w]+','', tweet)\n",
        "\n",
        "  twitter_dataset[\"tweet\"] = twitter_dataset[\"tweet\"].apply(remove_username)\n",
        "\n",
        "# Remove all usernames that appear in a tweet\n",
        "remove_usernames_from_tweets(labeled_dataset_df)\n",
        "\n",
        "# Show results\n",
        "result = labeled_dataset_df.head(10)\n",
        "print(\"First 10 rows of the DataFrame after removing usernames:\")\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 10 rows of the DataFrame after removing usernames:\n",
            "                     id  ... label\n",
            "0   1309219160828895233  ...     0\n",
            "1   1308809031583236096  ...     1\n",
            "2   1308716552229998593  ...     1\n",
            "3   1308483739584835585  ...     1\n",
            "4   1308351921875345409  ...     1\n",
            "5   1307763236062650368  ...     1\n",
            "7   1306800980307083267  ...     1\n",
            "8   1305869148463992832  ...     1\n",
            "9   1305659343971311616  ...     1\n",
            "10  1305617685020053512  ...     1\n",
            "\n",
            "[10 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEFpLAeDbxLM"
      },
      "source": [
        "## **Remove hashtags that appear within a tweet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGTsQQ8Hbzav",
        "outputId": "59736c2e-e374-45db-d726-223ccf84602c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def remove_hashtags_from_tweets(twitter_dataset):\n",
        "  \"\"\"\n",
        "  Remove all hashtags that appear on a tweet.\n",
        "  \"\"\"\n",
        "  def remove_hashtags(tweet):\n",
        "    \"\"\"\n",
        "    Remove hashtags from tweet.\n",
        "    \"\"\"\n",
        "    return tweet.replace(\"#\", \"\").replace(\"_\", \" \")\n",
        "\n",
        "  twitter_dataset[\"tweet\"] = twitter_dataset[\"tweet\"].apply(remove_hashtags)\n",
        "\n",
        "# Remove all hashtags that appear in a tweet\n",
        "remove_hashtags_from_tweets(labeled_dataset_df)\n",
        "\n",
        "# Show results\n",
        "result = labeled_dataset_df.head(10)\n",
        "print(\"First 10 rows of the DataFrame after removing usernames:\")\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 10 rows of the DataFrame after removing usernames:\n",
            "                     id  ... label\n",
            "0   1309219160828895233  ...     0\n",
            "1   1308809031583236096  ...     1\n",
            "2   1308716552229998593  ...     1\n",
            "3   1308483739584835585  ...     1\n",
            "4   1308351921875345409  ...     1\n",
            "5   1307763236062650368  ...     1\n",
            "7   1306800980307083267  ...     1\n",
            "8   1305869148463992832  ...     1\n",
            "9   1305659343971311616  ...     1\n",
            "10  1305617685020053512  ...     1\n",
            "\n",
            "[10 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qXBpdT2Z0HB"
      },
      "source": [
        "## **Remove special characters and punctuation that appear within a tweet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8F884syZ3d2",
        "outputId": "10819766-da48-429d-b87b-8b2bb06d65ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def remove_special_characters_and_punctuation_from_tweets(twitter_dataset):\n",
        "  \"\"\"\n",
        "  Remove all special characters and punctuation that appear on a tweet.\n",
        "  \"\"\"\n",
        "  def remove_special_characters_and_punctuation(tweet):\n",
        "    \"\"\"\n",
        "    Remove special characters and punctuation from tweet.\n",
        "    \"\"\"\n",
        "    return re.sub('[^A-Za-z0-9 ]+', '', tweet)\n",
        "\n",
        "  twitter_dataset[\"tweet\"] = twitter_dataset[\"tweet\"].apply(remove_special_characters_and_punctuation)\n",
        "\n",
        "# Remove all special characters and punctuation that appear in a tweet\n",
        "remove_special_characters_and_punctuation_from_tweets(labeled_dataset_df)\n",
        "\n",
        "# Show results\n",
        "result = labeled_dataset_df.head(10)\n",
        "print(\"First 10 rows of the DataFrame after removing usernames:\")\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 10 rows of the DataFrame after removing usernames:\n",
            "                     id  ... label\n",
            "0   1309219160828895233  ...     0\n",
            "1   1308809031583236096  ...     1\n",
            "2   1308716552229998593  ...     1\n",
            "3   1308483739584835585  ...     1\n",
            "4   1308351921875345409  ...     1\n",
            "5   1307763236062650368  ...     1\n",
            "7   1306800980307083267  ...     1\n",
            "8   1305869148463992832  ...     1\n",
            "9   1305659343971311616  ...     1\n",
            "10  1305617685020053512  ...     1\n",
            "\n",
            "[10 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alLP6ZB6aq3t"
      },
      "source": [
        "## **Remove URLs that appear within a tweet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQ2MbNsFat1g",
        "outputId": "1edea530-7009-4bec-c5d0-d3d51bf992cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def remove_urls_from_tweets(twitter_dataset):\n",
        "  \"\"\"\n",
        "  Remove all urls that appear on a tweet.\n",
        "  \"\"\"\n",
        "  def remove_urls(tweet):\n",
        "    \"\"\"\n",
        "    Remove urls from tweet.\n",
        "    \"\"\"\n",
        "    return re.sub(r'http\\S+', '', tweet)\n",
        "\n",
        "  twitter_dataset[\"tweet\"] = twitter_dataset[\"tweet\"].apply(remove_urls)\n",
        "\n",
        "# Remove all urls that appear in a tweet\n",
        "remove_urls_from_tweets(labeled_dataset_df)\n",
        "\n",
        "# Show results\n",
        "result = labeled_dataset_df.head(10)\n",
        "print(\"First 10 rows of the DataFrame after removing usernames:\")\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 10 rows of the DataFrame after removing usernames:\n",
            "                     id  ... label\n",
            "0   1309219160828895233  ...     0\n",
            "1   1308809031583236096  ...     1\n",
            "2   1308716552229998593  ...     1\n",
            "3   1308483739584835585  ...     1\n",
            "4   1308351921875345409  ...     1\n",
            "5   1307763236062650368  ...     1\n",
            "7   1306800980307083267  ...     1\n",
            "8   1305869148463992832  ...     1\n",
            "9   1305659343971311616  ...     1\n",
            "10  1305617685020053512  ...     1\n",
            "\n",
            "[10 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4Al-2K5MdTG"
      },
      "source": [
        "## **Remove stop words that appear within a tweet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlpChgY8MfdW",
        "outputId": "66c76223-5948-461b-debb-87dbf0538f3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "\n",
        "def remove_stop_words_from_tweets(twitter_dataset):\n",
        "  \"\"\"\n",
        "  Remove all stop_words that appear on a tweet.\n",
        "  \"\"\"\n",
        "  def remove_stop_words(tweet):\n",
        "    \"\"\"\n",
        "    Remove stop_words from tweet.\n",
        "    \"\"\"\n",
        "    return remove_stopwords(tweet)\n",
        "\n",
        "  twitter_dataset[\"tweet\"] = twitter_dataset[\"tweet\"].apply(remove_stop_words)\n",
        "\n",
        "# Remove all stop_words that appear in a tweet\n",
        "remove_stop_words_from_tweets(labeled_dataset_df)\n",
        "\n",
        "# Show results\n",
        "result = labeled_dataset_df.head(10)\n",
        "print(\"First 10 rows of the DataFrame after removing usernames:\")\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 10 rows of the DataFrame after removing usernames:\n",
            "                     id  ... label\n",
            "0   1309219160828895233  ...     0\n",
            "1   1308809031583236096  ...     1\n",
            "2   1308716552229998593  ...     1\n",
            "3   1308483739584835585  ...     1\n",
            "4   1308351921875345409  ...     1\n",
            "5   1307763236062650368  ...     1\n",
            "7   1306800980307083267  ...     1\n",
            "8   1305869148463992832  ...     1\n",
            "9   1305659343971311616  ...     1\n",
            "10  1305617685020053512  ...     1\n",
            "\n",
            "[10 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjQ2xaSrck31"
      },
      "source": [
        "# **Approach using Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCmqa8TicrPF"
      },
      "source": [
        "## **Create features using sentiment analysis and unigrams and Textblob**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbFqaoU3cq2D",
        "outputId": "c05614ad-8b8a-4175-dc86-ddd1cb05e4a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "def calculate_features_using_polarity(twitter_dataset):\n",
        "  \"\"\"\n",
        "  Use Textblob polarity to calculate the number of positive and negative words.\n",
        "  \"\"\"\n",
        "  def calculate_positive_words(tweet):\n",
        "    \"\"\"\n",
        "    Count number of positive words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_positive_words = 0\n",
        "    for word in tweet.split():\n",
        "      if TextBlob(word).polarity > 0:\n",
        "        number_of_positive_words = number_of_positive_words + 1\n",
        "    return number_of_positive_words\n",
        "  \n",
        "  def calculate_positive_tweet_score(tweet):\n",
        "    \"\"\"\n",
        "    Count number of positive words in a tweet.\n",
        "    \"\"\"\n",
        "    score = 0\n",
        "    for word in tweet.split():\n",
        "      polarity_score = TextBlob(word).polarity\n",
        "      if polarity_score > 0:\n",
        "        score = score + polarity_score\n",
        "    return score\n",
        "  \n",
        "  def calculate_ratio_positive_words(tweet):\n",
        "    \"\"\"\n",
        "    Calculate the ratio of positive words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_positive_words = 0\n",
        "    for word in tweet.split():\n",
        "      if TextBlob(word).polarity > 0:\n",
        "        number_of_positive_words = number_of_positive_words + 1\n",
        "    return number_of_positive_words/len(tweet.split()) if len(tweet.split()) != 0 else 0\n",
        "  \n",
        "  def calculate_negative_words(tweet):\n",
        "    \"\"\"\n",
        "    Count number of negative words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_negative_words = 0\n",
        "    for word in tweet.split():\n",
        "      if TextBlob(word).polarity < 0:\n",
        "        number_of_negative_words = number_of_negative_words + 1\n",
        "    return number_of_negative_words\n",
        "  \n",
        "  def calculate_negative_tweet_score(tweet):\n",
        "    \"\"\"\n",
        "    Count number of negative words in a tweet.\n",
        "    \"\"\"\n",
        "    score = 0\n",
        "    for word in tweet.split():\n",
        "      polarity_score = TextBlob(word).polarity\n",
        "      if polarity_score < 0:\n",
        "        score = score + polarity_score\n",
        "    return score\n",
        "  \n",
        "  def calculate_ratio_negative_words(tweet):\n",
        "    \"\"\"\n",
        "    Calculate the ratio of negative words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_negative_words = 0\n",
        "    for word in tweet.split():\n",
        "      if TextBlob(word).polarity < 0:\n",
        "        number_of_negative_words = number_of_negative_words + 1\n",
        "    return number_of_negative_words/len(tweet.split()) if len(tweet.split()) != 0 else 0\n",
        "  \n",
        "  twitter_dataset[\"unigram_number_positive_words\"] = twitter_dataset[\"tweet\"].apply(calculate_positive_words)\n",
        "  twitter_dataset[\"unigram_ratio_positive_words\"] = twitter_dataset[\"tweet\"].apply(calculate_ratio_positive_words)\n",
        "  twitter_dataset[\"unigram_number_negative_words\"] = twitter_dataset[\"tweet\"].apply(calculate_negative_words)\n",
        "  twitter_dataset[\"unigram_ratio_negative_words\"] = twitter_dataset[\"tweet\"].apply(calculate_ratio_negative_words)\n",
        "  twitter_dataset[\"unigram_positive_score\"] = twitter_dataset[\"tweet\"].apply(calculate_positive_tweet_score)\n",
        "  twitter_dataset[\"unigram_negative_score\"] = twitter_dataset[\"tweet\"].apply(calculate_negative_tweet_score)\n",
        "\n",
        "# Calculate new features using sentiment Analysis\n",
        "calculate_features_using_polarity(labeled_dataset_df)\n",
        "\n",
        "# Show results\n",
        "result = labeled_dataset_df.head(10)\n",
        "print(\"First 10 rows of the DataFrame after removing usernames:\")\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 10 rows of the DataFrame after removing usernames:\n",
            "                     id  ... unigram_negative_score\n",
            "0   1309219160828895233  ...                  -0.40\n",
            "1   1308809031583236096  ...                   0.00\n",
            "2   1308716552229998593  ...                  -0.30\n",
            "3   1308483739584835585  ...                   0.00\n",
            "4   1308351921875345409  ...                  -0.25\n",
            "5   1307763236062650368  ...                   0.00\n",
            "7   1306800980307083267  ...                   0.00\n",
            "8   1305869148463992832  ...                   0.00\n",
            "9   1305659343971311616  ...                   0.00\n",
            "10  1305617685020053512  ...                  -0.70\n",
            "\n",
            "[10 rows x 9 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmwSKyfZ1det"
      },
      "source": [
        "## **Create features using sentiment analysis and bigrams and Textblob**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GacX5i7k1eJd",
        "outputId": "6e1e1aad-0c0e-4fd4-9ef2-fe187d4a7533",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "def find_ngrams(n, input_sequence):\n",
        "  # Split sentence into tokens.\n",
        "  tokens = input_sequence.split()\n",
        "  ngrams = []\n",
        "  for i in range(len(tokens) - n + 1):\n",
        "    # Take n consecutive tokens in array.\n",
        "    ngram = tokens[i:i+n]\n",
        "    # Concatenate array items into string.\n",
        "    ngram = ' '.join(ngram)\n",
        "    ngrams.append(ngram)\n",
        "  return ngrams\n",
        "\n",
        "def calculate_bigram_features_using_polarity(twitter_dataset):\n",
        "  \"\"\"\n",
        "  Use Textblob polarity to calculate the number of positive and negative words.\n",
        "  \"\"\"\n",
        "  def calculate_positive_words(tweet):\n",
        "    \"\"\"\n",
        "    Count number of positive words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_positive_bigrams = 0\n",
        "    ngrams = find_ngrams(2, tweet)\n",
        "    for ngram in ngrams:\n",
        "      if TextBlob(ngram).polarity > 0:\n",
        "        number_of_positive_bigrams = number_of_positive_bigrams + 1\n",
        "    return number_of_positive_bigrams\n",
        "  \n",
        "  def calculate_positive_tweet_score(tweet):\n",
        "    \"\"\"\n",
        "    Count number of positive words in a tweet.\n",
        "    \"\"\"\n",
        "    score = 0\n",
        "    ngrams = find_ngrams(2, tweet)\n",
        "    for ngram in ngrams:\n",
        "      polarity_score = TextBlob(ngram).polarity\n",
        "      if polarity_score > 0:\n",
        "        score = score + polarity_score\n",
        "    return score\n",
        "  \n",
        "  def calculate_ratio_positive_words(tweet):\n",
        "    \"\"\"\n",
        "    Calculate the ratio of positive words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_positive_bigrams = 0\n",
        "    ngrams = find_ngrams(2, tweet)\n",
        "    for ngram in ngrams:\n",
        "      if TextBlob(ngram).polarity >= 0:\n",
        "        number_of_positive_bigrams = number_of_positive_bigrams + 1\n",
        "    return number_of_positive_bigrams/len(ngrams) if len(ngrams) != 0 else 0\n",
        "  \n",
        "  def calculate_negative_words(tweet):\n",
        "    \"\"\"\n",
        "    Count number of negative words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_negative_bigrams = 0\n",
        "    ngrams = find_ngrams(2, tweet)\n",
        "    for ngram in ngrams:\n",
        "      if TextBlob(ngram).polarity < 0:\n",
        "        number_of_negative_bigrams = number_of_negative_bigrams + 1\n",
        "    return number_of_negative_bigrams\n",
        "  \n",
        "  def calculate_negative_tweet_score(tweet):\n",
        "    \"\"\"\n",
        "    Count number of negative words in a tweet.\n",
        "    \"\"\"\n",
        "    score = 0\n",
        "    ngrams = find_ngrams(2, tweet)\n",
        "    for ngram in ngrams:\n",
        "      polarity_score = TextBlob(ngram).polarity\n",
        "      if polarity_score < 0:\n",
        "        score = score + polarity_score\n",
        "    return score\n",
        "  \n",
        "  def calculate_ratio_negative_words(tweet):\n",
        "    \"\"\"\n",
        "    Calculate the ratio of negative words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_negative_bigrams = 0\n",
        "    ngrams = find_ngrams(2, tweet)\n",
        "    for ngram in ngrams:\n",
        "      if TextBlob(ngram).polarity < 0:\n",
        "        number_of_negative_bigrams = number_of_negative_bigrams + 1\n",
        "    return number_of_negative_bigrams/len(ngrams) if len(ngrams) != 0 else 0\n",
        "  \n",
        "  twitter_dataset[\"bigram_number_positive_words\"] = twitter_dataset[\"tweet\"].apply(calculate_positive_words)\n",
        "  twitter_dataset[\"bigram_ratio_positive_words\"] = twitter_dataset[\"tweet\"].apply(calculate_ratio_positive_words)\n",
        "  twitter_dataset[\"bigram_number_negative_words\"] = twitter_dataset[\"tweet\"].apply(calculate_negative_words)\n",
        "  twitter_dataset[\"bigram_ratio_negative_words\"] = twitter_dataset[\"tweet\"].apply(calculate_ratio_negative_words)\n",
        "  twitter_dataset[\"bigram_positive_score\"] = twitter_dataset[\"tweet\"].apply(calculate_positive_tweet_score)\n",
        "  twitter_dataset[\"bigram_negative_score\"] = twitter_dataset[\"tweet\"].apply(calculate_negative_tweet_score)\n",
        "\n",
        "# Calculate new features using sentiment Analysis\n",
        "calculate_bigram_features_using_polarity(labeled_dataset_df)\n",
        "\n",
        "# Show results\n",
        "result = labeled_dataset_df.head(10)\n",
        "print(\"First 10 rows of the DataFrame after removing usernames:\")\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 10 rows of the DataFrame after removing usernames:\n",
            "                     id  ... bigram_negative_score\n",
            "0   1309219160828895233  ...                 -0.80\n",
            "1   1308809031583236096  ...                  0.00\n",
            "2   1308716552229998593  ...                 -0.60\n",
            "3   1308483739584835585  ...                  0.00\n",
            "4   1308351921875345409  ...                 -0.50\n",
            "5   1307763236062650368  ...                  0.00\n",
            "7   1306800980307083267  ...                  0.00\n",
            "8   1305869148463992832  ...                  0.00\n",
            "9   1305659343971311616  ...                  0.00\n",
            "10  1305617685020053512  ...                 -0.75\n",
            "\n",
            "[10 rows x 15 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A04WuxyXxXV9"
      },
      "source": [
        "## **Create features using sentiment analysis and trigrams and Textblob**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_p93z4UixY80",
        "outputId": "59c24575-ffaf-4415-8ef2-5128dea3b0f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "def find_ngrams(n, input_sequence):\n",
        "  # Split sentence into tokens.\n",
        "  tokens = input_sequence.split()\n",
        "  ngrams = []\n",
        "  for i in range(len(tokens) - n + 1):\n",
        "    # Take n consecutive tokens in array.\n",
        "    ngram = tokens[i:i+n]\n",
        "    # Concatenate array items into string.\n",
        "    ngram = ' '.join(ngram)\n",
        "    ngrams.append(ngram)\n",
        "  return ngrams\n",
        "\n",
        "def calculate_trigrams_features_using_polarity(twitter_dataset):\n",
        "  \"\"\"\n",
        "  Use Textblob polarity to calculate the number of positive and negative words.\n",
        "  \"\"\"\n",
        "  def calculate_positive_words(tweet):\n",
        "    \"\"\"\n",
        "    Count number of positive words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_positive_bigrams = 0\n",
        "    ngrams = find_ngrams(3, tweet)\n",
        "    for ngram in ngrams:\n",
        "      if TextBlob(ngram).polarity > 0:\n",
        "        number_of_positive_bigrams = number_of_positive_bigrams + 1\n",
        "    return number_of_positive_bigrams\n",
        "  \n",
        "  def calculate_positive_tweet_score(tweet):\n",
        "    \"\"\"\n",
        "    Count number of positive words in a tweet.\n",
        "    \"\"\"\n",
        "    score = 0\n",
        "    ngrams = find_ngrams(3, tweet)\n",
        "    for ngram in ngrams:\n",
        "      polarity_score = TextBlob(ngram).polarity\n",
        "      if polarity_score > 0:\n",
        "        score = score + polarity_score\n",
        "    return score\n",
        "  \n",
        "  def calculate_ratio_positive_words(tweet):\n",
        "    \"\"\"\n",
        "    Calculate the ratio of positive words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_positive_bigrams = 0\n",
        "    ngrams = find_ngrams(2, tweet)\n",
        "    for ngram in ngrams:\n",
        "      if TextBlob(ngram).polarity >= 0:\n",
        "        number_of_positive_bigrams = number_of_positive_bigrams + 1\n",
        "    return number_of_positive_bigrams/len(ngrams) if len(ngrams) != 0 else 0\n",
        "  \n",
        "  def calculate_negative_words(tweet):\n",
        "    \"\"\"\n",
        "    Count number of negative words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_negative_bigrams = 0\n",
        "    ngrams = find_ngrams(3, tweet)\n",
        "    for ngram in ngrams:\n",
        "      if TextBlob(ngram).polarity < 0:\n",
        "        number_of_negative_bigrams = number_of_negative_bigrams + 1\n",
        "    return number_of_negative_bigrams\n",
        "  \n",
        "  def calculate_negative_tweet_score(tweet):\n",
        "    \"\"\"\n",
        "    Count number of negative words in a tweet.\n",
        "    \"\"\"\n",
        "    score = 0\n",
        "    ngrams = find_ngrams(3, tweet)\n",
        "    for ngram in ngrams:\n",
        "      polarity_score = TextBlob(ngram).polarity\n",
        "      if polarity_score < 0:\n",
        "        score = score + polarity_score\n",
        "    return score\n",
        "  \n",
        "  def calculate_ratio_negative_words(tweet):\n",
        "    \"\"\"\n",
        "    Calculate the ratio of negative words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_negative_bigrams = 0\n",
        "    ngrams = find_ngrams(3, tweet)\n",
        "    for ngram in ngrams:\n",
        "      if TextBlob(ngram).polarity < 0:\n",
        "        number_of_negative_bigrams = number_of_negative_bigrams + 1\n",
        "    return number_of_negative_bigrams/len(ngrams) if len(ngrams) != 0 else 0\n",
        "  \n",
        "  twitter_dataset[\"trigrams_number_positive_words\"] = twitter_dataset[\"tweet\"].apply(calculate_positive_words)\n",
        "  twitter_dataset[\"trigrams_ratio_positive_words\"] = twitter_dataset[\"tweet\"].apply(calculate_ratio_positive_words)\n",
        "  twitter_dataset[\"trigrams_number_negative_words\"] = twitter_dataset[\"tweet\"].apply(calculate_negative_words)\n",
        "  twitter_dataset[\"trigrams_ratio_negative_words\"] = twitter_dataset[\"tweet\"].apply(calculate_ratio_negative_words)\n",
        "  twitter_dataset[\"trigrams_positive_score\"] = twitter_dataset[\"tweet\"].apply(calculate_positive_tweet_score)\n",
        "  twitter_dataset[\"trigrams_negative_score\"] = twitter_dataset[\"tweet\"].apply(calculate_negative_tweet_score)\n",
        "\n",
        "# Calculate new features using sentiment Analysis\n",
        "calculate_trigrams_features_using_polarity(labeled_dataset_df)\n",
        "\n",
        "# Show results\n",
        "result = labeled_dataset_df.head(10)\n",
        "print(\"First 10 rows of the DataFrame after removing usernames:\")\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 10 rows of the DataFrame after removing usernames:\n",
            "                     id  ... trigrams_negative_score\n",
            "0   1309219160828895233  ...                   -1.20\n",
            "1   1308809031583236096  ...                    0.00\n",
            "2   1308716552229998593  ...                   -0.67\n",
            "3   1308483739584835585  ...                    0.00\n",
            "4   1308351921875345409  ...                   -0.75\n",
            "5   1307763236062650368  ...                    0.00\n",
            "7   1306800980307083267  ...                    0.00\n",
            "8   1305869148463992832  ...                    0.00\n",
            "9   1305659343971311616  ...                    0.00\n",
            "10  1305617685020053512  ...                   -0.80\n",
            "\n",
            "[10 rows x 21 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcmx83MY2wKy"
      },
      "source": [
        "## **Create features using sentiment analysis and unigrams and Vader**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7KUiYVY2zQk"
      },
      "source": [
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "nltk.download('vader_lexicon')\n",
        "sentiment_analyzer = SentimentIntensityAnalyzer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOsUQl4X23xl",
        "outputId": "d3c50c2b-4207-41d2-ed9e-c41c56070277",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def calculate_features_using_polarity_vader(twitter_dataset):\n",
        "  \"\"\"\n",
        "  Use Vader polarity to calculate the number of positive and negative words.\n",
        "  \"\"\"\n",
        "  def calculate_positive_words(tweet):\n",
        "    \"\"\"\n",
        "    Count number of positive words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_positive_words = 0\n",
        "    for word in tweet.split():\n",
        "      if sentiment_analyzer.polarity_scores(word)[\"compound\"] > 0:\n",
        "        number_of_positive_words = number_of_positive_words + 1\n",
        "    return number_of_positive_words\n",
        "  \n",
        "  def calculate_ratio_positive_words(tweet):\n",
        "    \"\"\"\n",
        "    Calculate the ratio of positive words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_positive_words = 0\n",
        "    for word in tweet.split():\n",
        "      if sentiment_analyzer.polarity_scores(word)[\"compound\"] > 0:\n",
        "        number_of_positive_words = number_of_positive_words + 1\n",
        "    return number_of_positive_words/len(tweet.split()) if len(tweet.split()) != 0 else 0\n",
        "  \n",
        "  def calculate_negative_words(tweet):\n",
        "    \"\"\"\n",
        "    Count number of negative words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_negative_words = 0\n",
        "    for word in tweet.split():\n",
        "      if sentiment_analyzer.polarity_scores(word)[\"compound\"] < 0:\n",
        "        number_of_negative_words = number_of_negative_words + 1\n",
        "    return number_of_negative_words\n",
        "  \n",
        "  def calculate_ratio_negative_words(tweet):\n",
        "    \"\"\"\n",
        "    Calculate the ratio of negative words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_negative_words = 0\n",
        "    for word in tweet.split():\n",
        "      if sentiment_analyzer.polarity_scores(word)[\"compound\"] < 0:\n",
        "        number_of_negative_words = number_of_negative_words + 1\n",
        "    return number_of_negative_words/len(tweet.split()) if len(tweet.split()) != 0 else 0\n",
        "  \n",
        "  twitter_dataset[\"unigram_vader_positive_words\"] = twitter_dataset[\"tweet\"].apply(calculate_positive_words)\n",
        "  twitter_dataset[\"unigram_vader_ratio_positive_words\"] = twitter_dataset[\"tweet\"].apply(calculate_ratio_positive_words)\n",
        "  twitter_dataset[\"unigram_vader_negative_words\"] = twitter_dataset[\"tweet\"].apply(calculate_negative_words)\n",
        "  twitter_dataset[\"unigram_vader_ratio_negative_words\"] = twitter_dataset[\"tweet\"].apply(calculate_ratio_negative_words)\n",
        "\n",
        "# Calculate new features using sentiment Analysis\n",
        "calculate_features_using_polarity_vader(labeled_dataset_df)\n",
        "\n",
        "# Show results\n",
        "result = labeled_dataset_df.head(10)\n",
        "print(\"First 10 rows of the DataFrame after removing usernames:\")\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 10 rows of the DataFrame after removing usernames:\n",
            "                     id  ... unigram_vader_ratio_negative_words\n",
            "0   1309219160828895233  ...                           0.111111\n",
            "1   1308809031583236096  ...                           0.047619\n",
            "2   1308716552229998593  ...                           0.173913\n",
            "3   1308483739584835585  ...                           0.142857\n",
            "4   1308351921875345409  ...                           0.181818\n",
            "5   1307763236062650368  ...                           0.047619\n",
            "7   1306800980307083267  ...                           0.000000\n",
            "8   1305869148463992832  ...                           0.055556\n",
            "9   1305659343971311616  ...                           0.000000\n",
            "10  1305617685020053512  ...                           0.111111\n",
            "\n",
            "[10 rows x 25 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6mkfEjM13cm"
      },
      "source": [
        "## **Create features using sentiment analysis and bigrams and Vader**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sy7Cjdjb0Aoc",
        "outputId": "f526178b-849c-485f-9d5a-20c1447a08e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def find_ngrams(n, input_sequence):\n",
        "  # Split sentence into tokens.\n",
        "  tokens = input_sequence.split()\n",
        "  ngrams = []\n",
        "  for i in range(len(tokens) - n + 1):\n",
        "    # Take n consecutive tokens in array.\n",
        "    ngram = tokens[i:i+n]\n",
        "    # Concatenate array items into string.\n",
        "    ngram = ' '.join(ngram)\n",
        "    ngrams.append(ngram)\n",
        "  return ngrams\n",
        "\n",
        "def calculate_bigram_vader_features_using_polarity(twitter_dataset):\n",
        "  \"\"\"\n",
        "  Use Vader polarity to calculate the number of positive and negative words.\n",
        "  \"\"\"\n",
        "  def calculate_positive_words(tweet):\n",
        "    \"\"\"\n",
        "    Count number of positive words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_positive_bigrams = 0\n",
        "    ngrams = find_ngrams(2, tweet)\n",
        "    for ngram in ngrams:\n",
        "      if sentiment_analyzer.polarity_scores(ngram)[\"compound\"] > 0:\n",
        "        number_of_positive_bigrams = number_of_positive_bigrams + 1\n",
        "    return number_of_positive_bigrams\n",
        "  \n",
        "  def calculate_positive_tweet_score(tweet):\n",
        "    \"\"\"\n",
        "    Count number of positive words in a tweet.\n",
        "    \"\"\"\n",
        "    score = 0\n",
        "    ngrams = find_ngrams(2, tweet)\n",
        "    for ngram in ngrams:\n",
        "      polarity_score = sentiment_analyzer.polarity_scores(ngram)[\"compound\"]\n",
        "      if polarity_score > 0:\n",
        "        score = score + polarity_score\n",
        "    return score\n",
        "  \n",
        "  def calculate_ratio_positive_words(tweet):\n",
        "    \"\"\"\n",
        "    Calculate the ratio of positive words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_positive_bigrams = 0\n",
        "    ngrams = find_ngrams(2, tweet)\n",
        "    for ngram in ngrams:\n",
        "      if sentiment_analyzer.polarity_scores(ngram)[\"compound\"] >= 0:\n",
        "        number_of_positive_bigrams = number_of_positive_bigrams + 1\n",
        "    return number_of_positive_bigrams/len(ngrams) if len(ngrams) != 0 else 0\n",
        "  \n",
        "  def calculate_negative_words(tweet):\n",
        "    \"\"\"\n",
        "    Count number of negative words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_negative_bigrams = 0\n",
        "    ngrams = find_ngrams(2, tweet)\n",
        "    for ngram in ngrams:\n",
        "      if sentiment_analyzer.polarity_scores(ngram)[\"compound\"] < 0:\n",
        "        number_of_negative_bigrams = number_of_negative_bigrams + 1\n",
        "    return number_of_negative_bigrams\n",
        "  \n",
        "  def calculate_negative_tweet_score(tweet):\n",
        "    \"\"\"\n",
        "    Count number of negative words in a tweet.\n",
        "    \"\"\"\n",
        "    score = 0\n",
        "    ngrams = find_ngrams(2, tweet)\n",
        "    for ngram in ngrams:\n",
        "      polarity_score = sentiment_analyzer.polarity_scores(ngram)[\"compound\"]\n",
        "      if polarity_score < 0:\n",
        "        score = score + polarity_score\n",
        "    return score\n",
        "  \n",
        "  def calculate_ratio_negative_words(tweet):\n",
        "    \"\"\"\n",
        "    Calculate the ratio of negative words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_negative_bigrams = 0\n",
        "    ngrams = find_ngrams(2, tweet)\n",
        "    for ngram in ngrams:\n",
        "      if sentiment_analyzer.polarity_scores(ngram)[\"compound\"] < 0:\n",
        "        number_of_negative_bigrams = number_of_negative_bigrams + 1\n",
        "    return number_of_negative_bigrams/len(ngrams) if len(ngrams) != 0 else 0\n",
        "  \n",
        "  twitter_dataset[\"bigram_vader_number_positive_words\"] = twitter_dataset[\"tweet\"].apply(calculate_positive_words)\n",
        "  twitter_dataset[\"bigram_vader_ratio_positive_words\"] = twitter_dataset[\"tweet\"].apply(calculate_ratio_positive_words)\n",
        "  twitter_dataset[\"bigram_vader_number_negative_words\"] = twitter_dataset[\"tweet\"].apply(calculate_negative_words)\n",
        "  twitter_dataset[\"bigram_vader_ratio_negative_words\"] = twitter_dataset[\"tweet\"].apply(calculate_ratio_negative_words)\n",
        "  twitter_dataset[\"bigram_vader_positive_score\"] = twitter_dataset[\"tweet\"].apply(calculate_positive_tweet_score)\n",
        "  twitter_dataset[\"bigram_vader_negative_score\"] = twitter_dataset[\"tweet\"].apply(calculate_negative_tweet_score)\n",
        "\n",
        "# Calculate new features using sentiment Analysis\n",
        "calculate_bigram_vader_features_using_polarity(labeled_dataset_df)\n",
        "\n",
        "# Show results\n",
        "result = labeled_dataset_df.head(10)\n",
        "print(\"First 10 rows of the DataFrame after removing usernames:\")\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 10 rows of the DataFrame after removing usernames:\n",
            "                     id  ... bigram_vader_negative_score\n",
            "0   1309219160828895233  ...                     -0.9102\n",
            "1   1308809031583236096  ...                     -0.8038\n",
            "2   1308716552229998593  ...                     -2.9129\n",
            "3   1308483739584835585  ...                     -0.3818\n",
            "4   1308351921875345409  ...                     -0.3759\n",
            "5   1307763236062650368  ...                     -0.5464\n",
            "7   1306800980307083267  ...                      0.0000\n",
            "8   1305869148463992832  ...                     -0.3062\n",
            "9   1305659343971311616  ...                      0.0000\n",
            "10  1305617685020053512  ...                     -1.1657\n",
            "\n",
            "[10 rows x 31 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqRYxbaw15mE"
      },
      "source": [
        "## **Create features using sentiment analysis and trigrams and Vader**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HewTgEZ18lS",
        "outputId": "11a98d23-8ee6-4c5e-8c2d-c2c7196a9b32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def find_ngrams(n, input_sequence):\n",
        "  # Split sentence into tokens.\n",
        "  tokens = input_sequence.split()\n",
        "  ngrams = []\n",
        "  for i in range(len(tokens) - n + 1):\n",
        "    # Take n consecutive tokens in array.\n",
        "    ngram = tokens[i:i+n]\n",
        "    # Concatenate array items into string.\n",
        "    ngram = ' '.join(ngram)\n",
        "    ngrams.append(ngram)\n",
        "  return ngrams\n",
        "\n",
        "def calculate_trigrams_vader_features_using_polarity(twitter_dataset):\n",
        "  \"\"\"\n",
        "  Use Vader polarity to calculate the number of positive and negative words.\n",
        "  \"\"\"\n",
        "  def calculate_positive_words(tweet):\n",
        "    \"\"\"\n",
        "    Count number of positive words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_positive_trigrams = 0\n",
        "    ngrams = find_ngrams(3, tweet)\n",
        "    for ngram in ngrams:\n",
        "      if sentiment_analyzer.polarity_scores(ngram)[\"compound\"] > 0:\n",
        "        number_of_positive_trigrams = number_of_positive_trigrams + 1\n",
        "    return number_of_positive_trigrams\n",
        "  \n",
        "  def calculate_positive_tweet_score(tweet):\n",
        "    \"\"\"\n",
        "    Count number of positive words in a tweet.\n",
        "    \"\"\"\n",
        "    score = 0\n",
        "    ngrams = find_ngrams(3, tweet)\n",
        "    for ngram in ngrams:\n",
        "      polarity_score = sentiment_analyzer.polarity_scores(ngram)[\"compound\"]\n",
        "      if polarity_score > 0:\n",
        "        score = score + polarity_score\n",
        "    return score\n",
        "  \n",
        "  def calculate_ratio_positive_words(tweet):\n",
        "    \"\"\"\n",
        "    Calculate the ratio of positive words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_positive_trigrams = 0\n",
        "    ngrams = find_ngrams(3, tweet)\n",
        "    for ngram in ngrams:\n",
        "      if sentiment_analyzer.polarity_scores(ngram)[\"compound\"] >= 0:\n",
        "        number_of_positive_trigrams = number_of_positive_trigrams + 1\n",
        "    return number_of_positive_trigrams/len(ngrams) if len(ngrams) != 0 else 0\n",
        "  \n",
        "  def calculate_negative_words(tweet):\n",
        "    \"\"\"\n",
        "    Count number of negative words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_negative_trigrams = 0\n",
        "    ngrams = find_ngrams(3, tweet)\n",
        "    for ngram in ngrams:\n",
        "      if sentiment_analyzer.polarity_scores(ngram)[\"compound\"] < 0:\n",
        "        number_of_negative_trigrams = number_of_negative_trigrams + 1\n",
        "    return number_of_negative_trigrams\n",
        "  \n",
        "  def calculate_negative_tweet_score(tweet):\n",
        "    \"\"\"\n",
        "    Count number of negative words in a tweet.\n",
        "    \"\"\"\n",
        "    score = 0\n",
        "    ngrams = find_ngrams(3, tweet)\n",
        "    for ngram in ngrams:\n",
        "      polarity_score = sentiment_analyzer.polarity_scores(ngram)[\"compound\"]\n",
        "      if polarity_score < 0:\n",
        "        score = score + polarity_score\n",
        "    return score\n",
        "  \n",
        "  def calculate_ratio_negative_words(tweet):\n",
        "    \"\"\"\n",
        "    Calculate the ratio of negative words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_negative_trigrams = 0\n",
        "    ngrams = find_ngrams(3, tweet)\n",
        "    for ngram in ngrams:\n",
        "      if sentiment_analyzer.polarity_scores(ngram)[\"compound\"] < 0:\n",
        "        number_of_negative_trigrams = number_of_negative_trigrams + 1\n",
        "    return number_of_negative_trigrams/len(ngrams) if len(ngrams) != 0 else 0\n",
        "  \n",
        "  twitter_dataset[\"trigrams_vader_number_positive_words\"] = twitter_dataset[\"tweet\"].apply(calculate_positive_words)\n",
        "  twitter_dataset[\"trigrams_vader_ratio_positive_words\"] = twitter_dataset[\"tweet\"].apply(calculate_ratio_positive_words)\n",
        "  twitter_dataset[\"trigrams_vader_number_negative_words\"] = twitter_dataset[\"tweet\"].apply(calculate_negative_words)\n",
        "  twitter_dataset[\"trigrams_vader_ratio_negative_words\"] = twitter_dataset[\"tweet\"].apply(calculate_ratio_negative_words)\n",
        "  twitter_dataset[\"trigrams_vader_positive_score\"] = twitter_dataset[\"tweet\"].apply(calculate_positive_tweet_score)\n",
        "  twitter_dataset[\"trigrams_vader_negative_score\"] = twitter_dataset[\"tweet\"].apply(calculate_negative_tweet_score)\n",
        "\n",
        "# Calculate new features using sentiment Analysis\n",
        "calculate_trigrams_vader_features_using_polarity(labeled_dataset_df)\n",
        "\n",
        "# Show results\n",
        "result = labeled_dataset_df.head(10)\n",
        "print(\"First 10 rows of the DataFrame after removing usernames:\")\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 10 rows of the DataFrame after removing usernames:\n",
            "                     id  ... trigrams_vader_negative_score\n",
            "0   1309219160828895233  ...                       -1.2062\n",
            "1   1308809031583236096  ...                       -1.2057\n",
            "2   1308716552229998593  ...                       -4.2286\n",
            "3   1308483739584835585  ...                        0.0000\n",
            "4   1308351921875345409  ...                       -0.3050\n",
            "5   1307763236062650368  ...                       -0.8196\n",
            "7   1306800980307083267  ...                        0.0000\n",
            "8   1305869148463992832  ...                       -0.4593\n",
            "9   1305659343971311616  ...                        0.0000\n",
            "10  1305617685020053512  ...                       -1.2557\n",
            "\n",
            "[10 rows x 37 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twy_Fe-elW5O"
      },
      "source": [
        "## **Create features using sentiment analysis and trigrams and Flair**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yME73_WjqXAy",
        "outputId": "c2a4f45a-ebb7-41b9-b4fa-03632af25244",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import flair\n",
        "flair_sentiment = flair.models.TextClassifier.load('en-sentiment')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-11-09 15:39:27,682 https://nlp.informatik.hu-berlin.de/resources/models/sentiment-curated-distilbert/sentiment-en-mix-distillbert_3.1.pt not found in cache, downloading to /tmp/tmpmzj63kpy\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 266147697/266147697 [00:45<00:00, 5905403.55B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-11-09 15:40:13,579 copying /tmp/tmpmzj63kpy to cache at /root/.flair/models/sentiment-en-mix-distillbert_3.1.pt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-11-09 15:40:14,244 removing temp file /tmp/tmpmzj63kpy\n",
            "2020-11-09 15:40:14,277 loading file /root/.flair/models/sentiment-en-mix-distillbert_3.1.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGGMS4MT_CR7",
        "outputId": "4d1e7082-1adf-4bd6-df8f-db14e2f33db5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def calculate_features_using_polarity_flair(twitter_dataset):\n",
        "  \"\"\"\n",
        "  Use flair polarity to calculate the number of positive and negative words.\n",
        "  \"\"\"\n",
        "  def calculate_positive_words(tweet):\n",
        "    \"\"\"\n",
        "    Count number of positive words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_positive_words = 0\n",
        "    for word in tweet.split():\n",
        "      s = flair.data.Sentence(word)\n",
        "      flair_sentiment.predict(s)\n",
        "      if s.labels[0].score > 0:\n",
        "        number_of_positive_words = number_of_positive_words + 1\n",
        "    return number_of_positive_words\n",
        "  \n",
        "  def calculate_positive_words_score(tweet):\n",
        "    \"\"\"\n",
        "    Count number of positive words in a tweet.\n",
        "    \"\"\"\n",
        "    positive_words_score = 0\n",
        "    for word in tweet.split():\n",
        "      s = flair.data.Sentence(word)\n",
        "      flair_sentiment.predict(s)\n",
        "      if s.labels[0].score > 0:\n",
        "        positive_words_score = positive_words_score + s.labels[0].score\n",
        "    return positive_words_score\n",
        "  \n",
        "  def calculate_ratio_positive_words(tweet):\n",
        "    \"\"\"\n",
        "    Calculate the ratio of positive words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_positive_words = 0\n",
        "    for word in tweet.split():\n",
        "      s = flair.data.Sentence(word)\n",
        "      flair_sentiment.predict(s)\n",
        "      if s.labels[0].score > 0:\n",
        "        number_of_positive_words = number_of_positive_words + 1\n",
        "    return number_of_positive_words/len(tweet.split()) if len(tweet.split()) != 0 else 0\n",
        "  \n",
        "  def calculate_negative_words(tweet):\n",
        "    \"\"\"\n",
        "    Count number of negative words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_negative_words = 0\n",
        "    for word in tweet.split():\n",
        "      s = flair.data.Sentence(word)\n",
        "      flair_sentiment.predict(s)\n",
        "      if s.labels[0].score < 0:\n",
        "        number_of_negative_words = number_of_negative_words + 1\n",
        "    return number_of_negative_words\n",
        "  \n",
        "  def calculate_negative_words_score(tweet):\n",
        "    \"\"\"\n",
        "    Count number of negative words in a tweet.\n",
        "    \"\"\"\n",
        "    negative_words_score = 0\n",
        "    for word in tweet.split():\n",
        "      s = flair.data.Sentence(word)\n",
        "      flair_sentiment.predict(s)\n",
        "      if s.labels[0].score < 0:\n",
        "        negative_words_score = negative_words_score + s.labels[0].score\n",
        "    return negative_words_score\n",
        "  \n",
        "  def calculate_ratio_negative_words(tweet):\n",
        "    \"\"\"\n",
        "    Calculate the ratio of negative words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_negative_words = 0\n",
        "    for word in tweet.split():\n",
        "      s = flair.data.Sentence(word)\n",
        "      flair_sentiment.predict(s)\n",
        "      if s.labels[0].score < 0:\n",
        "        number_of_negative_words = number_of_negative_words + 1\n",
        "    return number_of_negative_words/len(tweet.split()) if len(tweet.split()) != 0 else 0\n",
        "  \n",
        "  twitter_dataset[\"unigram_flair_positive_words\"] = twitter_dataset[\"tweet\"].apply(calculate_positive_words)\n",
        "  twitter_dataset[\"unigram_flair_positive_words_score\"] = twitter_dataset[\"tweet\"].apply(calculate_positive_words_score)\n",
        "  twitter_dataset[\"unigram_flair_ratio_positive_words\"] = twitter_dataset[\"tweet\"].apply(calculate_ratio_positive_words)\n",
        "  twitter_dataset[\"unigram_flair_negative_words\"] = twitter_dataset[\"tweet\"].apply(calculate_negative_words)\n",
        "  twitter_dataset[\"unigram_flair_negative_words_score\"] = twitter_dataset[\"tweet\"].apply(calculate_negative_words_score)\n",
        "  twitter_dataset[\"unigram_flair_ratio_negative_words\"] = twitter_dataset[\"tweet\"].apply(calculate_ratio_negative_words)\n",
        "\n",
        "# Calculate new features using sentiment Analysis\n",
        "calculate_features_using_polarity_flair(labeled_dataset_df)\n",
        "\n",
        "# Show results\n",
        "result = labeled_dataset_df.head(10)\n",
        "print(\"First 10 rows of the DataFrame after removing usernames:\")\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 10 rows of the DataFrame after removing usernames:\n",
            "                     id  ... unigram_flair_ratio_negative_words\n",
            "0   1309219160828895233  ...                                0.0\n",
            "1   1308809031583236096  ...                                0.0\n",
            "2   1308716552229998593  ...                                0.0\n",
            "3   1308483739584835585  ...                                0.0\n",
            "4   1308351921875345409  ...                                0.0\n",
            "5   1307763236062650368  ...                                0.0\n",
            "7   1306800980307083267  ...                                0.0\n",
            "8   1305869148463992832  ...                                0.0\n",
            "9   1305659343971311616  ...                                0.0\n",
            "10  1305617685020053512  ...                                0.0\n",
            "\n",
            "[10 rows x 43 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MykQ89zQcXi6"
      },
      "source": [
        "## **Download Pandas Dataframe that contains all the extracted features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmtIIlvRo_ps",
        "outputId": "1ceedd9e-2c46-4355-cdd5-b072841f3879",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "from google.colab import files\n",
        "labeled_dataset_df.to_csv('dropping_out_tweets_part1_labeled_with_features.csv')\n",
        "files.download('dropping_out_tweets_part1_labeled_with_features.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_cfe4a76c-7fc4-4c21-8a0e-308a26524ea7\", \"dropping_out_tweets_part1_labeled_with_features.csv\", 951470)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSvwXVWBchv7"
      },
      "source": [
        " ## **Add Polarity Score of the entire tweet as a feature**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXi_xpgjqVKG"
      },
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "def calculate_features_using_polarity_score(twitter_dataset):\n",
        "  \"\"\"\n",
        "  Use Textblob polarity to calculate the number of positive and negative words.\n",
        "  \"\"\"\n",
        "  def calculate_textblob_score(tweet):\n",
        "    \"\"\"\n",
        "    Count number of positive words in a tweet.\n",
        "    \"\"\"\n",
        "    return TextBlob(tweet).polarity\n",
        "  \n",
        "  def calculate_vader_score(tweet):\n",
        "    \"\"\"\n",
        "    Count number of positive words in a tweet.\n",
        "    \"\"\"\n",
        "    return sentiment_analyzer.polarity_scores(tweet)[\"compound\"]\n",
        "  \n",
        "  def calculate_flair_score(tweet):\n",
        "    \"\"\"\n",
        "    Count number of positive words in a tweet.\n",
        "    \"\"\"\n",
        "    s = flair.data.Sentence(tweet)\n",
        "    flair_sentiment.predict(s)\n",
        "    return s.labels[0].score if len(s.labels) > 0 else 0\n",
        "  \n",
        "  twitter_dataset[\"unigram_textblob_score\"] = twitter_dataset[\"tweet\"].apply(calculate_textblob_score)\n",
        "  twitter_dataset[\"unigram_vader_score\"] = twitter_dataset[\"tweet\"].apply(calculate_vader_score)\n",
        "  twitter_dataset[\"unigram_flair_score\"] = twitter_dataset[\"tweet\"].apply(calculate_flair_score)\n",
        "\n",
        "# Calculate new features using sentiment Analysis\n",
        "calculate_features_using_polarity_score(labeled_dataset_df)\n",
        "\n",
        "# Show results\n",
        "result = labeled_dataset_df.head(10)\n",
        "print(\"First 10 rows of the DataFrame after removing usernames:\")\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IGMp5yjcubT"
      },
      "source": [
        "## **Download Pandas Dataframe that contains all the extracted features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHwPlmi8tsBr",
        "outputId": "b4c70fb6-05a9-4330-8817-3c1f73f63e36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "from google.colab import files\n",
        "labeled_dataset_df.to_csv('dropping_out_tweets_part1_labeled_with_features.csv')\n",
        "files.download('dropping_out_tweets_part1_labeled_with_features.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_a5c550c2-2cf7-42dd-be34-f21e7dbec064\", \"dropping_out_tweets_part1_labeled_with_features.csv\", 1042271)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b19SLj-UdA9s"
      },
      "source": [
        "## **Extract features from words using Bag of Words, TF-IDF and Word2Vec**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP1XqhN6dT9u"
      },
      "source": [
        "### **Tokenization of all the tweets before normalization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PC5opO-dLT0",
        "outputId": "842b5f55-73b1-4f07-d32d-11cfb6608f20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokenized_tweet_df = labeled_dataset_df.tweet.apply(lambda tweet: tweet.split())\n",
        "tokenized_tweet_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [lost, jobs, pandemic, got, 2, new, jobs, augu...\n",
              "1    [curveball, spector, work, area, pandemic, cli...\n",
              "2    [socioeconomic, fallout, pandemic, billions, c...\n",
              "3    [imagine, dropping, school, host, parties, pan...\n",
              "4    [drop, school, challenge, naming, past, school...\n",
              "Name: tweet, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 291
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tzdz7MQd9g6"
      },
      "source": [
        "### **Normalization of the tokenized tweets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4OzQUu4eCXp",
        "outputId": "d5656130-9393-44c1-dcd1-9a61c2e2e48e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk.stem.porter import * \n",
        "\n",
        "stemmer = PorterStemmer() \n",
        "tokenized_tweet_df = tokenized_tweet_df.apply(lambda x: [stemmer.stem(i) for i in x])\n",
        "tokenized_tweet_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [lost, job, pandem, got, 2, new, job, august, ...\n",
              "1    [curvebal, spector, work, area, pandem, client...\n",
              "2    [socioeconom, fallout, pandem, billion, childr...\n",
              "3    [imagin, drop, school, host, parti, pandem, em...\n",
              "4    [drop, school, challeng, name, past, school, d...\n",
              "Name: tweet, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 292
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHxtxDG4eNpE"
      },
      "source": [
        "### **Stitch tokens back together into tweets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dlpjeAHeR3i",
        "outputId": "788cbb75-9f20-4673-f188-c362aeba4e42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for i in range(len(tokenized_tweet_df)):\n",
        "    tokenized_tweet_df.iloc[i] = ' '.join(tokenized_tweet_df.iloc[i])    \n",
        "labeled_dataset_df['tweet'] = tokenized_tweet_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "S:  ['lost', 'job', 'pandem', 'got', '2', 'new', 'job', 'august', 'let', 'budget', 'cut', 'hour', 'decreas', 'strugglingim', 'think', 'drop', 'school', 'jbchanceholi']\n",
            "S:  ['curvebal', 'spector', 'work', 'area', 'pandem', 'client', 'come', 'issu', 'drop', 'workschool', 'social', 'isol', 'pandem', 'detriment', 'domain', 'lot', 'peopl', 'look', 'like', 'game', 'addict']\n",
            "S:  ['socioeconom', 'fallout', 'pandem', 'billion', 'children', 'high', 'risk', 'forc', 'labor', 'market', 'experienc', 'sexual', 'exploit', 'teenag', 'pregnanc', 'earli', 'marriag', 'domest', 'violenc', 'amp', 'fall', 'drop', 'school']\n",
            "S:  ['imagin', 'drop', 'school', 'host', 'parti', 'pandem', 'embarrass']\n",
            "S:  ['drop', 'school', 'challeng', 'name', 'past', 'school', 'drop', 'school', 'pandem', 'choic', 'idk']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     lost job pandem got 2 new job august let budge...\n",
              "1     curvebal spector work area pandem client come ...\n",
              "2     socioeconom fallout pandem billion children hi...\n",
              "3        imagin drop school host parti pandem embarrass\n",
              "4     drop school challeng name past school drop sch...\n",
              "5     girl quit school work covidhit rural india nep...\n",
              "7     pandem rob lowincom american colleg degre anal...\n",
              "8     india privat school head teacher particularli ...\n",
              "9     19 guarante wasnt drop school open restaur glo...\n",
              "10    drop grad school bc man tinder told pandem ove...\n",
              "Name: tweet, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 293
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkeVFpWxhHqK"
      },
      "source": [
        "### **Bag of words features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bbbYW-FhKa1",
        "outputId": "49681622-28a3-4387-aa23-ed8953b388dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer \n",
        "import gensim\n",
        "\n",
        "bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
        "bow = bow_vectorizer.fit_transform(labeled_dataset_df['tweet'])\n",
        "bow.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2577, 1000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 296
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsNlKhMEhVl2"
      },
      "source": [
        "### **TD-IDF features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qzZ4f0hhX3O",
        "outputId": "989347cf-9eb4-40a7-a029-4b2a05df3ed0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
        "tfidf = tfidf_vectorizer.fit_transform(labeled_dataset_df['tweet'])\n",
        "tfidf.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2577, 1000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 297
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qhIp63hllEF"
      },
      "source": [
        "### **Word2Vec features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEJmL89Llm43",
        "outputId": "30f05320-15da-4578-9e97-02e432be9576",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "tokenized_tweet = labeled_dataset_df['tweet'].apply(lambda x: x.split()) # tokenizing \n",
        "\n",
        "model_w2v = gensim.models.Word2Vec(\n",
        "            tokenized_tweet,\n",
        "            size=200, # desired no. of features/independent variables\n",
        "            window=5, # context window size\n",
        "            min_count=2, # Ignores all words with total frequency lower than 2.                                  \n",
        "            sg = 1, # 1 for skip-gram model\n",
        "            hs = 0,\n",
        "            negative = 10, # for negative sampling\n",
        "            workers= 32, # no.of cores\n",
        "            seed = 34\n",
        ") \n",
        "\n",
        "model_w2v.train(tokenized_tweet, total_examples= len(labeled_dataset_df['tweet']), epochs=20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 11.6 s, sys: 146 ms, total: 11.7 s\n",
            "Wall time: 6.64 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIXKfT6nmvtP"
      },
      "source": [
        "def word_vector(tokens, size):\n",
        "    vec = np.zeros(size).reshape((1, size))\n",
        "    count = 0\n",
        "    for word in tokens:\n",
        "        try:\n",
        "            vec += model_w2v[word].reshape((1, size))\n",
        "            count += 1.\n",
        "        except KeyError:  # handling the case where the token is not in vocabulary\n",
        "            continue\n",
        "    if count != 0:\n",
        "        vec /= count\n",
        "    return vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sH7NJE-6mzNW",
        "outputId": "24c9ca22-0c3e-4229-9836-0df5189f8abd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "wordvec_arrays = np.zeros((len(tokenized_tweet), 200)) \n",
        "for i in range(len(tokenized_tweet)):\n",
        "    wordvec_arrays[i,:] = word_vector(tokenized_tweet.iloc[i], 200)\n",
        "wordvec_df = pd.DataFrame(wordvec_arrays)\n",
        "wordvec_df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2577, 200)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 310
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feXiH33hjTkS"
      },
      "source": [
        "### **Lead with imbalance classes using Bag of Words Features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYsgEUBMjal3"
      },
      "source": [
        "# Transform the dataset\n",
        "oversample = SMOTE()\n",
        "X, Y = oversample.fit_resample(bow, labeled_dataset_df.label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVmJkONTinoc"
      },
      "source": [
        "### **Training models with Bag of Words features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkASKs1Iiqkd",
        "outputId": "8bfe33ce-5a8d-41ee-d333-19371955753c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Train the model\n",
        "X_train, X_test, Y_train, Y_test = tts(X, Y, test_size = 0.2)\n",
        "logistic_classifier = LogisticRegression(solver='lbfgs').fit(X_train,\n",
        "                                                             Y_train)\n",
        "\n",
        "# Calculate training accuracy\n",
        "Y_pred = logistic_classifier.predict(X_train)\n",
        "print(\"Training Accuracy: \", accuracy_score(Y_train, Y_pred))\n",
        "print(\"Training Precision: \", precision_score(Y_train, Y_pred))\n",
        "print(\"Training Recall: \", recall_score(Y_train, Y_pred))\n",
        "print(\"Training F-measure: \", f1_score(Y_train, Y_pred))\n",
        "\n",
        "# Calculate test accuracy\n",
        "Y_pred = logistic_classifier.predict(X_test)\n",
        "print(\"\\nTest Accuracy: \", accuracy_score(Y_test, Y_pred))\n",
        "print(\"Test Precision: \", precision_score(Y_test, Y_pred))\n",
        "print(\"Test Recall: \", recall_score(Y_test, Y_pred))\n",
        "print(\"Test F-measure: \", f1_score(Y_test, Y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy:  0.9185570844179992\n",
            "Training Precision:  0.950366151342555\n",
            "Training Recall:  0.8808446455505279\n",
            "Training F-measure:  0.9142857142857144\n",
            "\n",
            "Test Accuracy:  0.8439821693907875\n",
            "Test Precision:  0.8858024691358025\n",
            "Test Recall:  0.8084507042253521\n",
            "Test F-measure:  0.8453608247422681\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ha9Yzxy_kSKi"
      },
      "source": [
        "### **Lead with imbalance classes using TD-IDF Features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDrB_8qDkWrK"
      },
      "source": [
        "# Transform the dataset\n",
        "oversample = SMOTE()\n",
        "X, Y = oversample.fit_resample(tfidf, labeled_dataset_df.label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d3yXWD9kbtx"
      },
      "source": [
        "### **Training models with TD-IDF features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-ZiP1Azkd9Z",
        "outputId": "efbb17d0-5d87-4128-bc93-45753cdd1a9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Train the model\n",
        "X_train, X_test, Y_train, Y_test = tts(X, Y, test_size = 0.2)\n",
        "logistic_classifier = LogisticRegression(solver='lbfgs').fit(X_train,\n",
        "                                                             Y_train)\n",
        "\n",
        "# Calculate training accuracy\n",
        "Y_pred = logistic_classifier.predict(X_train)\n",
        "print(\"Training Accuracy: \", accuracy_score(Y_train, Y_pred))\n",
        "print(\"Training Precision: \", precision_score(Y_train, Y_pred))\n",
        "print(\"Training Recall: \", recall_score(Y_train, Y_pred))\n",
        "print(\"Training F-measure: \", f1_score(Y_train, Y_pred))\n",
        "\n",
        "# Calculate test accuracy\n",
        "Y_pred = logistic_classifier.predict(X_test)\n",
        "print(\"\\nTest Accuracy: \", accuracy_score(Y_test, Y_pred))\n",
        "print(\"Test Precision: \", precision_score(Y_test, Y_pred))\n",
        "print(\"Test Recall: \", recall_score(Y_test, Y_pred))\n",
        "print(\"Test F-measure: \", f1_score(Y_test, Y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy:  0.9025660096690219\n",
            "Training Precision:  0.9058734939759037\n",
            "Training Recall:  0.8977611940298508\n",
            "Training F-measure:  0.9017991004497753\n",
            "\n",
            "Test Accuracy:  0.8543833580980683\n",
            "Test Precision:  0.8521739130434782\n",
            "Test Recall:  0.8621700879765396\n",
            "Test F-measure:  0.8571428571428571\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LrS53tQnLE6"
      },
      "source": [
        "### **Lead with imbalance classes using Word2Vec Features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_3y6_jbnNdo"
      },
      "source": [
        "# Transform the dataset\n",
        "oversample = SMOTE()\n",
        "X, Y = oversample.fit_resample(wordvec_df, labeled_dataset_df.label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Lu38V-3nXTk"
      },
      "source": [
        "### **Training models with Word2Vec features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yp8OlX98nZlI",
        "outputId": "1529042c-d92a-4e03-f627-6b630ae7a7e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Train the model\n",
        "X_train, X_test, Y_train, Y_test = tts(X, Y, test_size = 0.2)\n",
        "logistic_classifier = LogisticRegression(solver='lbfgs').fit(X_train,\n",
        "                                                             Y_train)\n",
        "\n",
        "# Calculate training accuracy\n",
        "Y_pred = logistic_classifier.predict(X_train)\n",
        "print(\"Training Accuracy: \", accuracy_score(Y_train, Y_pred))\n",
        "print(\"Training Precision: \", precision_score(Y_train, Y_pred))\n",
        "print(\"Training Recall: \", recall_score(Y_train, Y_pred))\n",
        "print(\"Training F-measure: \", f1_score(Y_train, Y_pred))\n",
        "\n",
        "# Calculate test accuracy\n",
        "Y_pred = logistic_classifier.predict(X_test)\n",
        "print(\"\\nTest Accuracy: \", accuracy_score(Y_test, Y_pred))\n",
        "print(\"Test Precision: \", precision_score(Y_test, Y_pred))\n",
        "print(\"Test Recall: \", recall_score(Y_test, Y_pred))\n",
        "print(\"Test F-measure: \", f1_score(Y_test, Y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy:  0.8315358869468203\n",
            "Training Precision:  0.8531300160513644\n",
            "Training Recall:  0.7974493623405852\n",
            "Training F-measure:  0.824350523458705\n",
            "\n",
            "Test Accuracy:  0.8410104011887073\n",
            "Test Precision:  0.8685015290519877\n",
            "Test Recall:  0.8160919540229885\n",
            "Test F-measure:  0.8414814814814814\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hfyn2n3pOBrI"
      },
      "source": [
        "## **Select features to train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFfyuXFCJYE6"
      },
      "source": [
        "COMPLETE_LIST_OF_FEATURES = [\"unigram_number_positive_words\", \n",
        "                                                 \"unigram_ratio_positive_words\", \n",
        "                                                 \"unigram_number_negative_words\", \n",
        "                                                 \"unigram_ratio_negative_words\",\n",
        "                                                 \"unigram_positive_score\",\n",
        "                                                 \"unigram_negative_score\",\n",
        "                                                 \"bigram_number_positive_words\", \n",
        "                                                 \"bigram_ratio_positive_words\", \n",
        "                                                 \"bigram_number_negative_words\", \n",
        "                                                 \"bigram_ratio_negative_words\",\n",
        "                                                 \"bigram_positive_score\",\n",
        "                                                 \"bigram_negative_score\",\n",
        "                                                 \"trigrams_number_positive_words\", \n",
        "                                                 \"trigrams_ratio_positive_words\", \n",
        "                                                 \"trigrams_number_negative_words\", \n",
        "                                                 \"trigrams_ratio_negative_words\",\n",
        "                                                 \"trigrams_positive_score\",\n",
        "                                                 \"trigrams_negative_score\",\n",
        "                                                 \"unigram_vader_positive_words\", \n",
        "                                                 \"unigram_vader_ratio_positive_words\", \n",
        "                                                 \"unigram_vader_negative_words\", \n",
        "                                                 \"unigram_vader_ratio_negative_words\",\n",
        "                                                 \"bigram_vader_number_positive_words\", \n",
        "                                                 \"bigram_vader_ratio_positive_words\", \n",
        "                                                 \"bigram_vader_number_negative_words\", \n",
        "                                                 \"bigram_vader_ratio_negative_words\",\n",
        "                                                 \"bigram_vader_positive_score\",\n",
        "                                                 \"bigram_vader_negative_score\",\n",
        "                                                 \"trigrams_vader_number_positive_words\", \n",
        "                                                 \"trigrams_vader_ratio_positive_words\", \n",
        "                                                 \"trigrams_vader_number_negative_words\", \n",
        "                                                 \"trigrams_vader_ratio_negative_words\",\n",
        "                                                 \"trigrams_vader_positive_score\",\n",
        "                                                 \"trigrams_vader_negative_score\",\n",
        "                                                 \"unigram_vader_positive_words\",\n",
        "                                                 \"unigram_flair_positive_words\",\n",
        "                                                 \"unigram_flair_positive_words_score\",\n",
        "                                                 \"unigram_flair_ratio_positive_words\",\n",
        "                                                 \"unigram_textblob_score\", \n",
        "                                                 \"unigram_vader_score\", \n",
        "                                                 \"unigram_flair_score\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZeRL0YaOENd"
      },
      "source": [
        "# Select calculated features\n",
        "dataset_sentiment_features = labeled_dataset_df[[\"trigrams_number_positive_words\", \n",
        "                                                 \"trigrams_ratio_positive_words\", \n",
        "                                                 \"trigrams_number_negative_words\", \n",
        "                                                 \"trigrams_ratio_negative_words\",\n",
        "                                                 \"trigrams_positive_score\",\n",
        "                                                 \"trigrams_negative_score\",\n",
        "                                                 \"unigram_vader_positive_words\", \n",
        "                                                 \"unigram_vader_ratio_positive_words\", \n",
        "                                                 \"unigram_vader_negative_words\", \n",
        "                                                 \"unigram_vader_ratio_negative_words\",\n",
        "                                                 \"bigram_vader_number_positive_words\", \n",
        "                                                 \"bigram_vader_ratio_positive_words\", \n",
        "                                                 \"bigram_vader_number_negative_words\", \n",
        "                                                 \"bigram_vader_ratio_negative_words\",\n",
        "                                                 \"bigram_vader_positive_score\",\n",
        "                                                 \"bigram_vader_negative_score\",\n",
        "                                                 \"unigram_flair_positive_words\",\n",
        "                                                 \"unigram_flair_positive_words_score\",\n",
        "                                                 \"unigram_flair_ratio_positive_words\",\n",
        "                                                 \"unigram_textblob_score\", \n",
        "                                                 \"unigram_vader_score\", \n",
        "                                                 \"unigram_flair_score\"]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2Wpltcvj-AX"
      },
      "source": [
        "## **Normalize calculated features using Pandas**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zanC63BTkBam",
        "outputId": "0cd3efa6-5a7b-4f72-b038-a266c9598ee1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Normalize using the mean value\n",
        "normalized_df = (dataset_sentiment_features - dataset_sentiment_features.mean())/dataset_sentiment_features.std()\n",
        "\n",
        "# Show results\n",
        "result = normalized_df.head(10)\n",
        "print(\"First 10 rows of the DataFrame after removing usernames:\")\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 10 rows of the DataFrame after removing usernames:\n",
            "    trigrams_number_positive_words  ...  unigram_flair_score\n",
            "0                         0.497719  ...             0.296592\n",
            "1                         0.497719  ...             0.422858\n",
            "2                         2.565799  ...             0.425922\n",
            "3                        -0.743128  ...             0.490862\n",
            "4                        -0.743128  ...            -0.024302\n",
            "5                        -0.743128  ...             0.452969\n",
            "7                         0.497719  ...             0.487882\n",
            "8                         1.738567  ...            -1.010269\n",
            "9                        -0.743128  ...            -1.690904\n",
            "10                        0.084103  ...             0.381205\n",
            "\n",
            "[10 rows x 11 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETkakKds9KTx"
      },
      "source": [
        "## **Normalize calculated features using Scaler**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8uJuvse9NMW"
      },
      "source": [
        "# Feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Normalize using the mean value\n",
        "sc = StandardScaler()\n",
        "normalized_df = sc.fit_transform(dataset_sentiment_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTIRTpijLo4U"
      },
      "source": [
        "## **Normalize calculated features using MinMaxScaler**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqEniXM2LqmE"
      },
      "source": [
        "# Feature Scaling\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Normalize using the mean value\n",
        "sc = MinMaxScaler()\n",
        "normalized_df = sc.fit_transform(dataset_sentiment_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZboHv80POY2G"
      },
      "source": [
        "## **Select X and Y**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mk5i1lzAOblu"
      },
      "source": [
        "# Select the data\n",
        "def select_data(normalize_data = False):\n",
        "  if normalize_data:\n",
        "    return normalized_df, labeled_dataset_df.label\n",
        "  return dataset_sentiment_features, labeled_dataset_df.label\n",
        "\n",
        "X, Y = select_data(normalize_data = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KszH2igXfDBk"
      },
      "source": [
        "## **Handle imbalance classes using SMOTE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRylaSqlfGm8",
        "outputId": "f6847ec8-c417-4ad9-9d1d-d89d8dbc34b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"Number of rows with intention of dropout: \", \n",
        "      len(labeled_dataset_df[(labeled_dataset_df['label'] == \"Intention of dropout\")]))\n",
        "print(\"Number of rows with no intention of dropout: \", \n",
        "      len(labeled_dataset_df[(labeled_dataset_df['label'] == \"Not intention of dropout\")]))\n",
        "\n",
        "# Transform the dataset\n",
        "oversample = SMOTE()\n",
        "X, Y = oversample.fit_resample(X, Y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of rows with intention of dropout:  0\n",
            "Number of rows with no intention of dropout:  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVx8PKHrlbfd"
      },
      "source": [
        "## **Train logistic regression model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96YgCYl2liH8",
        "outputId": "33aff8dd-6aa4-409b-8004-aaa11e81d73c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Train the model\n",
        "X_train, X_test, Y_train, Y_test = tts(X, Y, test_size = 0.2)\n",
        "logistic_classifier = LogisticRegression(random_state = 0).fit(X_train,\n",
        "                                                               Y_train)\n",
        "\n",
        "# Calculate training accuracy\n",
        "Y_pred = logistic_classifier.predict(X_train)\n",
        "print(\"Training Accuracy: \", accuracy_score(Y_train, Y_pred))\n",
        "print(\"Training Precision: \", precision_score(Y_train, Y_pred))\n",
        "print(\"Training Recall: \", recall_score(Y_train, Y_pred))\n",
        "print(\"Training F-measure: \", f1_score(Y_train, Y_pred))\n",
        "\n",
        "# Calculate test accuracy\n",
        "Y_pred = logistic_classifier.predict(X_test)\n",
        "print(\"\\nTest Accuracy: \", accuracy_score(Y_test, Y_pred))\n",
        "print(\"Test Precision: \", precision_score(Y_test, Y_pred))\n",
        "print(\"Test Recall: \", recall_score(Y_test, Y_pred))\n",
        "print(\"Test F-measure: \", f1_score(Y_test, Y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy:  0.7013759761993306\n",
            "Training Precision:  0.728448275862069\n",
            "Training Recall:  0.6339084771192798\n",
            "Training F-measure:  0.6778981147212194\n",
            "\n",
            "Test Accuracy:  0.7102526002971769\n",
            "Test Precision:  0.7508196721311475\n",
            "Test Recall:  0.6580459770114943\n",
            "Test F-measure:  0.7013782542113323\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCkMOdAP5z7W"
      },
      "source": [
        "## **Train Random Forest Classifier model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B83DcWr452qk",
        "outputId": "12c66617-11c2-46c6-bbbc-34bdd04d9073",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Train the model\n",
        "X_train, X_test, Y_train, Y_test = tts(X, Y, test_size = 0.2)\n",
        "random_forest_classifier = RandomForestClassifier(n_estimators=100)\n",
        "random_forest_classifier.fit(X_train, Y_train)\n",
        "\n",
        "# Calculate training accuracy\n",
        "Y_pred = random_forest_classifier.predict(X_train)\n",
        "print(\"Training Accuracy: \", accuracy_score(Y_train, Y_pred))\n",
        "print(\"Training Precision: \", precision_score(Y_train, Y_pred))\n",
        "print(\"Training Recall: \", recall_score(Y_train, Y_pred))\n",
        "print(\"Training F-measure: \", f1_score(Y_train, Y_pred))\n",
        "\n",
        "# Calculate test accuracy\n",
        "Y_pred = random_forest_classifier.predict(X_test)\n",
        "print(\"\\nTest Accuracy: \", accuracy_score(Y_test, Y_pred))\n",
        "print(\"Test Precision: \", precision_score(Y_test, Y_pred))\n",
        "print(\"Test Recall: \", recall_score(Y_test, Y_pred))\n",
        "print(\"Test F-measure: \", f1_score(Y_test, Y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy:  0.995165489029379\n",
            "Training Precision:  0.9955290611028316\n",
            "Training Recall:  0.9947877885331348\n",
            "Training F-measure:  0.9951582867783985\n",
            "\n",
            "Test Accuracy:  0.8202080237741456\n",
            "Test Precision:  0.8297872340425532\n",
            "Test Recall:  0.8076923076923077\n",
            "Test F-measure:  0.8185907046476762\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPeHzbtmfcjW"
      },
      "source": [
        "## **Set the grid parameters to be optimized for Random Forest**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y67GdXaYZcAN"
      },
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Number of trees in random forest\n",
        "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
        "# Number of features to consider at every split\n",
        "max_features = ['auto', 'sqrt']\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "max_depth.append(None)\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [2, 5, 10]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "# Method of selecting samples for training each tree\n",
        "bootstrap = [True, False]\n",
        "# Create the random grid\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'bootstrap': bootstrap}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9we2UH3e_fn"
      },
      "source": [
        "## **Find the parameters that optimize the Random Forest Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsq2XsMLab-h",
        "outputId": "74a7c596-39bc-49be-c79c-19026faee7a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Use the random grid to search for best hyperparameters\n",
        "# First create the base model to tune\n",
        "rf = RandomForestClassifier()\n",
        "\n",
        "# Random search of parameters, using 3 fold cross validation, \n",
        "# search across 100 different combinations, and use all available cores\n",
        "#X_train, X_test, Y_train, Y_test = tts(X, Y, test_size = 0.2)\n",
        "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, \n",
        "                               n_iter = 100, cv = 3, verbose=2, random_state=42, \n",
        "                               n_jobs = -1)\n",
        "\n",
        "# Fit the random search model\n",
        "rf_random.fit(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  2.5min\n",
            "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed: 10.2min\n",
            "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed: 18.8min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=3, error_score=nan,\n",
              "                   estimator=RandomForestClassifier(bootstrap=True,\n",
              "                                                    ccp_alpha=0.0,\n",
              "                                                    class_weight=None,\n",
              "                                                    criterion='gini',\n",
              "                                                    max_depth=None,\n",
              "                                                    max_features='auto',\n",
              "                                                    max_leaf_nodes=None,\n",
              "                                                    max_samples=None,\n",
              "                                                    min_impurity_decrease=0.0,\n",
              "                                                    min_impurity_split=None,\n",
              "                                                    min_samples_leaf=1,\n",
              "                                                    min_samples_split=2,\n",
              "                                                    min_weight_fraction_leaf=0.0,\n",
              "                                                    n_estimators=100,\n",
              "                                                    n_jobs...\n",
              "                   param_distributions={'bootstrap': [True, False],\n",
              "                                        'max_depth': [10, 20, 30, 40, 50, 60,\n",
              "                                                      70, 80, 90, 100, 110,\n",
              "                                                      None],\n",
              "                                        'max_features': ['auto', 'sqrt'],\n",
              "                                        'min_samples_leaf': [1, 2, 4],\n",
              "                                        'min_samples_split': [2, 5, 10],\n",
              "                                        'n_estimators': [200, 400, 600, 800,\n",
              "                                                         1000, 1200, 1400, 1600,\n",
              "                                                         1800, 2000]},\n",
              "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
              "                   return_train_score=False, scoring=None, verbose=2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5J6UJmUfI94"
      },
      "source": [
        "## **Show the parameters that has to be use in Random Forest for the selected Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z482wmY0dXZ0",
        "outputId": "2e37e433-aeec-4c76-d68c-afa6e3d72325",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "rf_random.best_params_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bootstrap': True,\n",
              " 'max_depth': 50,\n",
              " 'max_features': 'sqrt',\n",
              " 'min_samples_leaf': 1,\n",
              " 'min_samples_split': 2,\n",
              " 'n_estimators': 200}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISvErwQMexjK"
      },
      "source": [
        "## **Train Random Forest Classifier model after performing Cross-Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBYyRgVfdhx7"
      },
      "source": [
        "# Select best Random Forest Model\n",
        "best_random = rf_random.best_estimator_\n",
        "\n",
        "# Calculate training accuracy\n",
        "Y_pred = best_random.predict(X_train)\n",
        "print(\"Training Accuracy: \", accuracy_score(Y_train, Y_pred))\n",
        "print(\"Training Precision: \", precision_score(Y_train, Y_pred))\n",
        "print(\"Training Recall: \", recall_score(Y_train, Y_pred))\n",
        "print(\"Training F-measure: \", f1_score(Y_train, Y_pred))\n",
        "\n",
        "# Calculate test accuracy\n",
        "Y_pred = best_random.predict(X_test)\n",
        "print(\"\\nTest Accuracy: \", accuracy_score(Y_test, Y_pred))\n",
        "print(\"Test Precision: \", precision_score(Y_test, Y_pred))\n",
        "print(\"Test Recall: \", recall_score(Y_test, Y_pred))\n",
        "print(\"Test F-measure: \", f1_score(Y_test, Y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HWCZOwZlQKY"
      },
      "source": [
        "## **Train Adaboost Classifier model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ah2nhndTlUf8",
        "outputId": "24049e03-b2c8-4374-c79b-b24cd390501d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Train the model\n",
        "X_train, X_test, Y_train, Y_test = tts(X, Y, test_size = 0.2)\n",
        "adaboost_classifier = AdaBoostClassifier(n_estimators = 100)\n",
        "adaboost_classifier.fit(X_train, Y_train)\n",
        "\n",
        "# Calculate training accuracy\n",
        "Y_pred = adaboost_classifier.predict(X_train)\n",
        "print(\"Training Accuracy: \", accuracy_score(Y_train, Y_pred))\n",
        "print(\"Training Precision: \", precision_score(Y_train, Y_pred))\n",
        "print(\"Training Recall: \", recall_score(Y_train, Y_pred))\n",
        "print(\"Training F-measure: \", f1_score(Y_train, Y_pred))\n",
        "\n",
        "# Calculate test accuracy\n",
        "Y_pred = adaboost_classifier.predict(X_test)\n",
        "print(\"\\nTest Accuracy: \", accuracy_score(Y_test, Y_pred))\n",
        "print(\"Test Precision: \", precision_score(Y_test, Y_pred))\n",
        "print(\"Test Recall: \", recall_score(Y_test, Y_pred))\n",
        "print(\"Test F-measure: \", f1_score(Y_test, Y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy:  0.7512086277426553\n",
            "Training Precision:  0.750748502994012\n",
            "Training Recall:  0.7490664675130695\n",
            "Training F-measure:  0.7499065420560747\n",
            "\n",
            "Test Accuracy:  0.7265973254086181\n",
            "Test Precision:  0.7365269461077845\n",
            "Test Recall:  0.7192982456140351\n",
            "Test F-measure:  0.7278106508875739\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6t_EbnvJmNvJ"
      },
      "source": [
        "## **Train Gradient Boost Classifier model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fiI9mi0mQFX",
        "outputId": "45871b28-901e-4c23-f2c7-177d9f0194ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Train the model\n",
        "X_train, X_test, Y_train, Y_test = tts(X, Y, test_size = 0.2)\n",
        "gradient_boost_classifier = GradientBoostingClassifier(n_estimators = 100)\n",
        "gradient_boost_classifier.fit(X_train, Y_train)\n",
        "\n",
        "# Calculate training accuracy\n",
        "Y_pred = gradient_boost_classifier.predict(X_train)\n",
        "print(\"Training Accuracy: \", accuracy_score(Y_train, Y_pred))\n",
        "print(\"Training Precision: \", precision_score(Y_train, Y_pred))\n",
        "print(\"Training Recall: \", recall_score(Y_train, Y_pred))\n",
        "print(\"Training F-measure: \", f1_score(Y_train, Y_pred))\n",
        "\n",
        "# Calculate test accuracy\n",
        "Y_pred = gradient_boost_classifier.predict(X_test)\n",
        "print(\"\\nTest Accuracy: \", accuracy_score(Y_test, Y_pred))\n",
        "print(\"Test Precision: \", precision_score(Y_test, Y_pred))\n",
        "print(\"Test Recall: \", recall_score(Y_test, Y_pred))\n",
        "print(\"Test F-measure: \", f1_score(Y_test, Y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy:  0.8095946448493864\n",
            "Training Precision:  0.8065476190476191\n",
            "Training Recall:  0.811377245508982\n",
            "Training F-measure:  0.8089552238805969\n",
            "\n",
            "Test Accuracy:  0.7548291233283804\n",
            "Test Precision:  0.7631578947368421\n",
            "Test Recall:  0.7565217391304347\n",
            "Test F-measure:  0.7598253275109169\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWIPexS-otho"
      },
      "source": [
        "## **Train XG Gradient Boost Classifier model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKmMlcaSovku",
        "outputId": "ce975702-11b8-4a8f-d5a0-7e52fefd991f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Train the model\n",
        "X_train, X_test, Y_train, Y_test = tts(X, Y, test_size = 0.2)\n",
        "xg_gradient_boost_classifier = xgb.XGBClassifier(objective=\"binary:logistic\", \n",
        "                                                 random_state=42)\n",
        "xg_gradient_boost_classifier.fit(X_train, Y_train)\n",
        "\n",
        "# Calculate training accuracy\n",
        "Y_pred = xg_gradient_boost_classifier.predict(X_train)\n",
        "print(\"Training Accuracy: \", accuracy_score(Y_train, Y_pred))\n",
        "print(\"Training Precision: \", precision_score(Y_train, Y_pred))\n",
        "print(\"Training Recall: \", recall_score(Y_train, Y_pred))\n",
        "print(\"Training F-measure: \", f1_score(Y_train, Y_pred))\n",
        "\n",
        "# Calculate test accuracy\n",
        "Y_pred = xg_gradient_boost_classifier.predict(X_test)\n",
        "print(\"\\nTest Accuracy: \", accuracy_score(Y_test, Y_pred))\n",
        "print(\"Test Precision: \", precision_score(Y_test, Y_pred))\n",
        "print(\"Test Recall: \", recall_score(Y_test, Y_pred))\n",
        "print(\"Test F-measure: \", f1_score(Y_test, Y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy:  0.7995537374488657\n",
            "Training Precision:  0.7940320232896652\n",
            "Training Recall:  0.8099480326651819\n",
            "Training F-measure:  0.8019110621095186\n",
            "\n",
            "Test Accuracy:  0.7429420505200595\n",
            "Test Precision:  0.7417417417417418\n",
            "Test Recall:  0.7395209580838323\n",
            "Test F-measure:  0.7406296851574213\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Q20s8MPK15D"
      },
      "source": [
        "## **Train Multinomial Naive Bayes Classifier model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRpCNGs9K5Xp"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Train the model\n",
        "X_train, X_test, Y_train, Y_test = tts(X, Y, test_size = 0.2)\n",
        "multinomial_naive_bayes_classifier = MultinomialNB()\n",
        "multinomial_naive_bayes_classifier.fit(X_train, Y_train)\n",
        "\n",
        "# Calculate training accuracy\n",
        "Y_pred = multinomial_naive_bayes_classifier.predict(X_train)\n",
        "\n",
        "# Calculate the accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(\"Accuracy: \", accuracy_score(Y_train, Y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-byGimkU4kc"
      },
      "source": [
        "## **Train Decision Tree Classifier model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7YhwL8AU9NT",
        "outputId": "6aa70fd8-1479-46f2-c2eb-be6caca4a80e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Train the model\n",
        "X_train, X_test, Y_train, Y_test = tts(X, Y, test_size = 0.2)\n",
        "decision_tree_classifier = DecisionTreeClassifier()\n",
        "decision_tree_classifier.fit(X_train, Y_train)\n",
        "\n",
        "# Calculate training accuracy\n",
        "Y_pred = decision_tree_classifier.predict(X_train)\n",
        "print(\"Training Accuracy: \", accuracy_score(Y_train, Y_pred))\n",
        "print(\"Training Precision: \", precision_score(Y_train, Y_pred))\n",
        "print(\"Training Recall: \", recall_score(Y_train, Y_pred))\n",
        "print(\"Training F-measure: \", f1_score(Y_train, Y_pred))\n",
        "\n",
        "# Calculate test accuracy\n",
        "Y_pred = decision_tree_classifier.predict(X_test)\n",
        "print(\"\\nTest Accuracy: \", accuracy_score(Y_test, Y_pred))\n",
        "print(\"Test Precision: \", precision_score(Y_test, Y_pred))\n",
        "print(\"Test Recall: \", recall_score(Y_test, Y_pred))\n",
        "print(\"Test F-measure: \", f1_score(Y_test, Y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy:  0.995909259947936\n",
            "Training Precision:  0.9992559523809523\n",
            "Training Recall:  0.9926090169992609\n",
            "Training F-measure:  0.9959213941416389\n",
            "\n",
            "Test Accuracy:  0.7533432392273403\n",
            "Test Precision:  0.7396449704142012\n",
            "Test Recall:  0.7621951219512195\n",
            "Test F-measure:  0.7507507507507507\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuzNgfS0g7vB"
      },
      "source": [
        "## **Set the grid parameters to be optimized for Decision Tree**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9S_wb69kg-WR"
      },
      "source": [
        "from scipy.stats import randint\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Setup the parameters and distributions to sample from: param_dist\n",
        "param_dist = {\"criterion\": [\"gini\", \"entropy\"],\n",
        "              \"min_samples_split\": randint(1, 20),\n",
        "              \"max_depth\": randint(1, 20),\n",
        "              \"min_samples_leaf\": randint(1, 20),\n",
        "              \"max_leaf_nodes\": randint(2, 20)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIgZHE0MiMnW"
      },
      "source": [
        "## **Find the parameters that optimize the Decision Tree Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfHvYQ7jiO2w"
      },
      "source": [
        "# Use the random grid to search for best hyperparameters\n",
        "# First create the base model to tune\n",
        "decision_tree = DecisionTreeClassifier()\n",
        "\n",
        "# Random search of parameters, using 3 fold cross validation, \n",
        "# search across 100 different combinations, and use all available cores\n",
        "#X_train, X_test, Y_train, Y_test = tts(X, Y, test_size = 0.2)\n",
        "decision_tree_random = RandomizedSearchCV(estimator = decision_tree, param_distributions = param_dist, \n",
        "                               n_iter = 100, cv = 5, verbose=2, random_state=42, \n",
        "                               n_jobs = -1)\n",
        "\n",
        "# Fit the random search model\n",
        "decision_tree_random.fit(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oX-5vMhuj9ig"
      },
      "source": [
        "## **Show the parameters that has to be use in Decision Tree for the selected Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Bejt6DhkCp3",
        "outputId": "622c1530-538a-454f-dedd-e6d49618d726",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "decision_tree_random.best_params_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'criterion': 'entropy',\n",
              " 'max_depth': 18,\n",
              " 'max_leaf_nodes': 18,\n",
              " 'min_samples_leaf': 7,\n",
              " 'min_samples_split': 5}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 246
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-qB4kMairRJ"
      },
      "source": [
        "## **Train Decision Tree Classifier model after performing Cross-Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6vAZuGPit6b",
        "outputId": "8698150e-2e99-471d-a87f-99043662d9c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Select best Random Forest Model\n",
        "best_decision_tree_random = decision_tree_random.best_estimator_\n",
        "\n",
        "# Calculate training accuracy\n",
        "Y_pred = best_decision_tree_random.predict(X_train)\n",
        "print(\"Training Accuracy: \", accuracy_score(Y_train, Y_pred))\n",
        "print(\"Training Precision: \", precision_score(Y_train, Y_pred))\n",
        "print(\"Training Recall: \", recall_score(Y_train, Y_pred))\n",
        "print(\"Training F-measure: \", f1_score(Y_train, Y_pred))\n",
        "\n",
        "# Calculate test accuracy\n",
        "Y_pred = best_decision_tree_random.predict(X_test)\n",
        "print(\"\\nTest Accuracy: \", accuracy_score(Y_test, Y_pred))\n",
        "print(\"Test Precision: \", precision_score(Y_test, Y_pred))\n",
        "print(\"Test Recall: \", recall_score(Y_test, Y_pred))\n",
        "print(\"Test F-measure: \", f1_score(Y_test, Y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy:  0.7281517292673857\n",
            "Training Precision:  0.7177871148459384\n",
            "Training Recall:  0.7575757575757576\n",
            "Training F-measure:  0.7371449119021936\n",
            "\n",
            "Test Accuracy:  0.7028231797919762\n",
            "Test Precision:  0.6787709497206704\n",
            "Test Recall:  0.7408536585365854\n",
            "Test F-measure:  0.7084548104956268\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkcyuOJ9Ug8m"
      },
      "source": [
        "## **Train SVM model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ab3T5gt0Uj7a",
        "outputId": "335dd0fd-23e8-497e-81b2-c2f9039814a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Train the model\n",
        "X_train, X_test, Y_train, Y_test = tts(X, Y, test_size = 0.2)\n",
        "svm_classifier = SVC()\n",
        "svm_classifier.fit(X_train, Y_train)\n",
        "\n",
        "# Calculate training accuracy\n",
        "Y_pred = svm_classifier.predict(X_train)\n",
        "print(\"Training Accuracy: \", accuracy_score(Y_train, Y_pred))\n",
        "print(\"Training Precision: \", precision_score(Y_train, Y_pred))\n",
        "print(\"Training Recall: \", recall_score(Y_train, Y_pred))\n",
        "print(\"Training F-measure: \", f1_score(Y_train, Y_pred))\n",
        "\n",
        "# Calculate test accuracy\n",
        "Y_pred = svm_classifier.predict(X_test)\n",
        "print(\"\\nTest Accuracy: \", accuracy_score(Y_test, Y_pred))\n",
        "print(\"Test Precision: \", precision_score(Y_test, Y_pred))\n",
        "print(\"Test Recall: \", recall_score(Y_test, Y_pred))\n",
        "print(\"Test F-measure: \", f1_score(Y_test, Y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy:  0.7370769802900706\n",
            "Training Precision:  0.7434885556432518\n",
            "Training Recall:  0.7114803625377644\n",
            "Training F-measure:  0.7271323813199536\n",
            "\n",
            "Test Accuracy:  0.7161961367013373\n",
            "Test Precision:  0.7455621301775148\n",
            "Test Recall:  0.7058823529411765\n",
            "Test F-measure:  0.725179856115108\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVk2LCDf7IpQ"
      },
      "source": [
        "## **Train Ridge Classifier model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oD7L8og6wGg",
        "outputId": "b5974c09-c139-4f3f-eb34-38bb4bbe364f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.linear_model import RidgeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Train the model\n",
        "X_train, X_test, Y_train, Y_test = tts(X, Y, test_size = 0.2)\n",
        "ridge_classifier = RidgeClassifier()\n",
        "ridge_classifier.fit(X_train, Y_train)\n",
        "\n",
        "# Calculate training accuracy\n",
        "Y_pred = ridge_classifier.predict(X_train)\n",
        "print(\"Training Accuracy: \", accuracy_score(Y_train, Y_pred))\n",
        "print(\"Training Precision: \", precision_score(Y_train, Y_pred))\n",
        "print(\"Training Recall: \", recall_score(Y_train, Y_pred))\n",
        "print(\"Training F-measure: \", f1_score(Y_train, Y_pred))\n",
        "\n",
        "# Calculate test accuracy\n",
        "Y_pred = ridge_classifier.predict(X_test)\n",
        "print(\"\\nTest Accuracy: \", accuracy_score(Y_test, Y_pred))\n",
        "print(\"Test Precision: \", precision_score(Y_test, Y_pred))\n",
        "print(\"Test Recall: \", recall_score(Y_test, Y_pred))\n",
        "print(\"Test F-measure: \", f1_score(Y_test, Y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy:  0.7076980290070658\n",
            "Training Precision:  0.7412060301507538\n",
            "Training Recall:  0.6497797356828194\n",
            "Training F-measure:  0.6924882629107982\n",
            "\n",
            "Test Accuracy:  0.6864784546805349\n",
            "Test Precision:  0.6812080536912751\n",
            "Test Recall:  0.6363636363636364\n",
            "Test F-measure:  0.6580226904376013\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZTykP8IZs0j"
      },
      "source": [
        "## **Train Stacking Ensemble model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCVOhqDNZymJ",
        "outputId": "9db91875-9f7e-4eeb-f05b-4f907da52ac3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define the base learners models\n",
        "base_learners = list()\n",
        "base_learners.append(('bayes', GaussianNB()))\n",
        "base_learners.append(('rf', RandomForestClassifier()))\n",
        "base_learners.append(('cart', DecisionTreeClassifier()))\n",
        "base_learners.append(('svm', SVC()))\n",
        "\n",
        "# Define the meta learner model\n",
        "meta_learner = LogisticRegression()\n",
        "\n",
        "# Define the stacking ensemble\n",
        "stacking_ensemble = StackingClassifier(estimators = base_learners, \n",
        "                                       final_estimator = meta_learner, \n",
        "                                       cv = 10)\n",
        "\n",
        "# Train the model\n",
        "X_train, X_test, Y_train, Y_test = tts(X, Y, test_size = 0.2)\n",
        "stacking_ensemble.fit(X_train, Y_train)\n",
        "\n",
        "# Calculate training accuracy\n",
        "Y_pred = stacking_ensemble.predict(X_train)\n",
        "print(\"Training Accuracy: \", accuracy_score(Y_train, Y_pred))\n",
        "print(\"Training Precision: \", precision_score(Y_train, Y_pred))\n",
        "print(\"Training Recall: \", recall_score(Y_train, Y_pred))\n",
        "print(\"Training F-measure: \", f1_score(Y_train, Y_pred))\n",
        "\n",
        "# Calculate test accuracy\n",
        "Y_pred = stacking_ensemble.predict(X_test)\n",
        "print(\"\\nTest Accuracy: \", accuracy_score(Y_test, Y_pred))\n",
        "print(\"Test Precision: \", precision_score(Y_test, Y_pred))\n",
        "print(\"Test Recall: \", recall_score(Y_test, Y_pred))\n",
        "print(\"Test F-measure: \", f1_score(Y_test, Y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy:  0.995909259947936\n",
            "Training Precision:  0.9977511244377811\n",
            "Training Recall:  0.9940253920836445\n",
            "Training F-measure:  0.9958847736625515\n",
            "\n",
            "Test Accuracy:  0.7890044576523031\n",
            "Test Precision:  0.7923976608187134\n",
            "Test Recall:  0.7923976608187134\n",
            "Test F-measure:  0.7923976608187134\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJmr9O0vKzZA"
      },
      "source": [
        "# **Student Dropout Classification using BERT Features**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9p0Cn7HdqYo"
      },
      "source": [
        "## **Install transformers library**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6lPRwl5K6GA"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcql58-Udt52"
      },
      "source": [
        "## **Import necessary libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMSDqcdRLD43"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import torch\n",
        "import transformers as ppb\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GqAVs-Xd9fA"
      },
      "source": [
        "## **Load pretrained BERT models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6rV5m2eLetG",
        "outputId": "04874767-8b66-4cd4-af63-4587a1dee141",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164,
          "referenced_widgets": [
            "2b1f996a6fc2497093fafb9b03936c8f",
            "cb5b4fded69d4ead96092a6206f5eafc",
            "3574d68568b64fb486053f5c72fc710c",
            "452b341f817b40c984424b5ec804d6f0",
            "66808ee5b9c242e093646d1c0f40a2d8",
            "86f427375e8747948708f3f3bc6b3198",
            "81697c4bc7914f60b74332ed03d796ca",
            "0dfc82ee5d074be496f866306e633ede",
            "565d0dd58484448882ad9ec16e96984f",
            "565abd3638eb447683ddbe1869f1f9f6",
            "ab40f1ecacbd4143aa841bea29c7ed79",
            "7d65ba2cd6e048f0a15cadccfd564fc3",
            "b9f63b9311064451a357f88cf996a73e",
            "f293b33a124f4d52ab501b7d03123147",
            "6267b0a4c1e64a659aa4c7402df60086",
            "d9eb54310deb44b7bc2ef083abf8673f",
            "83c95d8a5f764b1f971dee8f463a519b",
            "09a0350bf81b4a04be05955f729ab559",
            "9abb4109c19346cb89362883aa954c37",
            "adccc33d13f0437f8ded07eaa23c13dc",
            "413c1a724e874765badc16c2964dd88c",
            "7dc87e8f5a9043f78c6e93dd25e4b354",
            "276cc59c95434c809a4c5885fbda9042",
            "2e7e3ee38b3943f98512e2361bc484c1"
          ]
        }
      },
      "source": [
        "# For DistilBERT:\n",
        "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
        "\n",
        "## Want BERT instead of distilBERT? Uncomment the following line:\n",
        "#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
        "\n",
        "# Load pretrained model/tokenizer\n",
        "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "model = model_class.from_pretrained(pretrained_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2b1f996a6fc2497093fafb9b03936c8f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "565d0dd58484448882ad9ec16e96984f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=442.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "83c95d8a5f764b1f971dee8f463a519b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=267967963.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYiLOsmMeDl2"
      },
      "source": [
        "## **Tokenize tweets using BERT Tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-255LCVLmnm"
      },
      "source": [
        "tokenized = labeled_dataset_df[\"tweet\"].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IkH_w9-eLdn"
      },
      "source": [
        "## **Padded tokenize values**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wS1Z9KDELyh2"
      },
      "source": [
        "max_len = 0\n",
        "for i in tokenized.values:\n",
        "    if len(i) > max_len:\n",
        "        max_len = len(i)\n",
        "\n",
        "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wULavkcRePdP"
      },
      "source": [
        "## **Add the attention mask**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqB5nSK3L4EF"
      },
      "source": [
        "attention_mask = np.where(padded != 0, 1, 0)\n",
        "attention_mask.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PydrWqaGgFG4"
      },
      "source": [
        "## **Calculate the BERT Features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhmwVXyiL-1s"
      },
      "source": [
        "input_ids = torch.tensor(padded)  \n",
        "attention_mask = torch.tensor(attention_mask)\n",
        "\n",
        "with torch.no_grad():\n",
        "    last_hidden_states = model(input_ids, attention_mask=attention_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7IbX5qXfEpb"
      },
      "source": [
        "## **Select the BERT features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKljPIcPM-MS"
      },
      "source": [
        "features = last_hidden_states[0][:,0,:].numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeP9fA2NfJ3N"
      },
      "source": [
        "## **Select the labels**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KjaJasbNBEy"
      },
      "source": [
        "labels = labeled_dataset_df[\"label\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEiFF36DfMik"
      },
      "source": [
        "## **Split the train and test data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdyHROrbNIYq"
      },
      "source": [
        "train_features, test_features, train_labels, test_labels = train_test_split(features, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2crydowYfeHC"
      },
      "source": [
        "## **Train a Logistic Regression Model using BERT Features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRTIoZIdNZc0",
        "outputId": "2a19b573-05bb-4acf-e22b-fa296423c38c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "lr_clf = LogisticRegression()\n",
        "lr_clf.fit(train_features, train_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 203
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9AGr5SRNeBA",
        "outputId": "adb6cd9e-b291-4766-d57c-3b14e1975b79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "lr_clf.score(test_features, test_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8325581395348837"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 204
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fGtfSNlawZc"
      },
      "source": [
        "## **Training and test different models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4jfOU0qO7Vb",
        "outputId": "74f4f92d-8d38-4f68-ea7d-9d12aa8e6322",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "decision_tree = DecisionTreeClassifier()\n",
        "decision_tree.fit(train_features, train_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
              "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                       random_state=None, splitter='best')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 205
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwPl2MtqO_h9",
        "outputId": "70056471-dfeb-4a3b-8ce5-59261ee96285",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "decision_tree.score(test_features, test_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7348837209302326"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 206
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Nb2ZUQ4QlGu",
        "outputId": "fd40d045-4152-43e8-896c-4d220beeb333",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Train the model\n",
        "random_forest_classifier = RandomForestClassifier(n_estimators=100)\n",
        "random_forest_classifier.fit(train_features, train_labels)\n",
        "\n",
        "# Calculate training accuracy\n",
        "random_forest_classifier.score(test_features, test_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8155038759689922"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 207
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICI6H5HFQ54k",
        "outputId": "5d762488-df6d-4289-e2de-5c21944be032",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Train the model\n",
        "svm_classifier = SVC()\n",
        "svm_classifier.fit(train_features, train_labels)\n",
        "\n",
        "svm_classifier.score(test_features, test_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8046511627906977"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 208
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0rFnQ2eUeTZ",
        "outputId": "8102a61e-7397-412c-c882-8f30b93e9e62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define the base learners models\n",
        "base_learners = list()\n",
        "base_learners.append(('bayes', GaussianNB()))\n",
        "base_learners.append(('rf', RandomForestClassifier()))\n",
        "base_learners.append(('lr', LogisticRegression()))\n",
        "base_learners.append(('svm', SVC()))\n",
        "\n",
        "# Define the meta learner model\n",
        "meta_learner = LogisticRegression()\n",
        "\n",
        "# Define the stacking ensemble\n",
        "stacking_ensemble = StackingClassifier(estimators = base_learners, \n",
        "                                       final_estimator = meta_learner, \n",
        "                                       cv = 4)\n",
        "\n",
        "stacking_ensemble.fit(train_features, train_labels)\n",
        "\n",
        "stacking_ensemble.score(test_features, test_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8325581395348837"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euTWIIj8XxV8",
        "outputId": "5afcb20d-ad91-4ac1-a64e-1c1b488b6c9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Train the model\n",
        "xg_gradient_boost_classifier = xgb.XGBClassifier(objective=\"binary:logistic\", \n",
        "                                                 random_state=42)\n",
        "xg_gradient_boost_classifier.fit(train_features, train_labels)\n",
        "\n",
        "xg_gradient_boost_classifier.score(test_features, test_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8170542635658915"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 210
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6s9uStQYXgu",
        "outputId": "f0fb6541-e893-4075-da18-8a060a4bd4d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Train the model\n",
        "gradient_boost_classifier = GradientBoostingClassifier(n_estimators = 100)\n",
        "gradient_boost_classifier.fit(train_features, train_labels)\n",
        "\n",
        "gradient_boost_classifier.score(test_features, test_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8108527131782945"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kM2FKHJGbZZH",
        "outputId": "078e9e0c-c26e-4fde-fc61-73a5c47e2e1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "\n",
        "clf1 = LogisticRegression(random_state=1)\n",
        "clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n",
        "clf3 = GaussianNB()\n",
        "\n",
        "eclf1 = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')\n",
        "eclf1 = eclf1.fit(train_features, train_labels)\n",
        "\n",
        "eclf1.score(test_features, test_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8248062015503876"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1PjjxajcWit",
        "outputId": "8579f601-0c8b-4918-c96d-7e90c309b7b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.linear_model import RidgeClassifier\n",
        "\n",
        "clf = RidgeClassifier()\n",
        "clf.fit(train_features, train_labels)\n",
        "\n",
        "clf.score(test_features, test_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8418604651162791"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywxuRgiTcpHk",
        "outputId": "5c7dbe25-03c8-46d8-aac7-f24e200ae49e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "\n",
        "clf = PassiveAggressiveClassifier(max_iter=1000, random_state=0,tol=1e-3)\n",
        "clf.fit(train_features, train_labels)\n",
        "\n",
        "clf.score(test_features, test_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7410852713178294"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 214
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrpmS9B7gQ4I"
      },
      "source": [
        "# **Fine-tuning BERT for Student Dropout Classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdQBOHpqhfqn"
      },
      "source": [
        "## **Install the Transformers Library**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Hxdw6Bzgam0"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSNksPrGhjQj"
      },
      "source": [
        "## **Install necessary libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oc6siaEFhmOh"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import transformers\n",
        "from transformers import AutoModel, BertTokenizerFast\n",
        "\n",
        "# Specify GPU\n",
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ns7emaTqh1OU"
      },
      "source": [
        "## **Load the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GecLnQ6h3W-"
      },
      "source": [
        "DATASET_PATH = \"/content/drive/My Drive/AI_Project_CS_534/Datasets/dropping_out_tweets_part1_labeled.csv\"\n",
        "dataset_df = pd.read_csv(DATASET_PATH, sep=\";\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgxHyCUoicUx"
      },
      "source": [
        "## **Convert label to numerical value**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3EWMg-6iZJg"
      },
      "source": [
        "# Convert label categorical value to numerical value\n",
        "label_numerical_value = dataset_df[\"label\"].astype('category').cat.codes\n",
        "dataset_df[\"label\"] = label_numerical_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7rB6sHOiA1Q"
      },
      "source": [
        "## **Split the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWNqJxrfiC-n"
      },
      "source": [
        "train_text, temp_text, train_labels, temp_labels = train_test_split(dataset_df['tweet'], dataset_df['label'], \n",
        "                                                                    random_state=2018, \n",
        "                                                                    test_size=0.3, \n",
        "                                                                    stratify=dataset_df['label'])\n",
        "\n",
        "# We will use temp_text and temp_labels to create validation and test set\n",
        "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
        "                                                                random_state=2018, \n",
        "                                                                test_size=0.5, \n",
        "                                                                stratify=temp_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMqwG3kfinNC"
      },
      "source": [
        "## **Import BERT model and BERT Tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhyWYkRmiqFu",
        "outputId": "a2b5ede2-f301-43b6-8d79-bcd061ce273a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213,
          "referenced_widgets": [
            "dcbb89c8757a419eada992f4b5d4cabf",
            "a218b4a195a24402a18332255459ce24",
            "ee473f20642e43e6acbd9578fcf0391b",
            "5debfe1864e34f9ea1863902312264a3",
            "1acb1936b86c4da5928abcb4639c7e8f",
            "2418d3363e3f4424af3d547ecfe9b3cc",
            "1802d3a94d614375963eaadd2a64c365",
            "8f844c88fa274cd581336b19f4bf081c",
            "6360d01d610a4b49a7e2df00c1f848e3",
            "23b7f0b138424229968bbde93b8cb181",
            "e185a9aaa1af4c76abf0695bf9fc0068",
            "74e6987787a34cd9a566f5fce63e0978",
            "b6e3b7f50ccf4b039bf875542aafc3e7",
            "989198b15f0a455f8eecd4a5df9c46e8",
            "16a45d659d864cca8b8d87f5d7a57269",
            "5162dc435e7340b9abf6f63a30f15f3e",
            "e3d4013bd36746a5851199ce29ddffcd",
            "a38019a618574af9a8398fe9e9e394c2",
            "cef930adce3246dcbda3d9e7a6068e73",
            "d5e8c2bc71f347ea9a1e309f18819786",
            "edf2ff01e4984573aa5f3555aa3aba2e",
            "5d1addb8d76e4c5587aca54278b9f43c",
            "27c1692d5cb644879b212598a155b5b6",
            "9512f0d222eb45b59840adf7462e28d2",
            "abde53c680d04725a9b6090b295cf04a",
            "e58fe2ae9c6d4b22870b77ab069c81f8",
            "2c61880677f54dc6b81bea0a81c6d0d5",
            "b2be9e2683f54677a5ff6676153acd86",
            "d846146036ec442cafa0bbc4b8d4f232",
            "2bf1c8907b784be0b54b80c4673d3e89",
            "c26a558797f64b7f9fefd741747d21b8",
            "2d7d35b80ee240ea830f60daa6afbe9e"
          ]
        }
      },
      "source": [
        "# Import BERT-base pretrained model\n",
        "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dcbb89c8757a419eada992f4b5d4cabf",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6360d01d610a4b49a7e2df00c1f848e3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e3d4013bd36746a5851199ce29ddffcd",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "abde53c680d04725a9b6090b295cf04a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IsWGK_9i6hU"
      },
      "source": [
        "## **Tokenize the tweets**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbX54g5-jNah"
      },
      "source": [
        "### **Check the length of all the tweets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nl__NgEGi9bm",
        "outputId": "4beed274-00fb-45a0-f568-6f6833d5a500",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "# Get length of all the tweets in the train set\n",
        "seq_len = [len(i.split()) for i in train_text]\n",
        "\n",
        "pd.Series(seq_len).hist(bins = 30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7feb4c800cc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGXIC9pOjTSL"
      },
      "source": [
        "### **Set the padding length**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-vW4QSKjVyT"
      },
      "source": [
        "max_seq_len = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZpm8KR7jd_l"
      },
      "source": [
        "### **Tokenize the tweets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8yVo2A6jgsF",
        "outputId": "0f96a0ff-1053-45cf-c181-3367196d302b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Tokenize and encode sequences in the training set\n",
        "tokens_train = tokenizer.batch_encode_plus(\n",
        "    train_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "# Tokenize and encode sequences in the validation set\n",
        "tokens_val = tokenizer.batch_encode_plus(\n",
        "    val_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "# Tokenize and encode sequences in the test set\n",
        "tokens_test = tokenizer.batch_encode_plus(\n",
        "    test_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWcdnwdJjuS2"
      },
      "source": [
        "## **Convert integer sequences to tensors**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTw2jccdjyH_"
      },
      "source": [
        "# For train set\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train_labels.tolist())\n",
        "\n",
        "# For validation set\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "val_y = torch.tensor(val_labels.tolist())\n",
        "\n",
        "# For test set\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "test_y = torch.tensor(test_labels.tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AFdNrmvk1YM"
      },
      "source": [
        "## **Create Data Loaders**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBzYjMiYk3zA"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Define a batch size\n",
        "batch_size = 32\n",
        "\n",
        "# Wrap tensors\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "\n",
        "# Sampler for sampling the data during training\n",
        "train_sampler = RandomSampler(train_data)\n",
        "\n",
        "# DataLoader for train set\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Wrap tensors\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "\n",
        "# Sampler for sampling the data during training\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "\n",
        "# DataLoader for validation set\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sPOBYvdlHgk"
      },
      "source": [
        "## **Freeze BERT parameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ym0RhikClKQ2"
      },
      "source": [
        "# freeze all the parameters\n",
        "for param in bert.parameters():\n",
        "    param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YhHgpQPlRQE"
      },
      "source": [
        "## **Define model architecture**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INJhi3UblUJu"
      },
      "source": [
        "class BERT_Arch(nn.Module):\n",
        "\n",
        "    def __init__(self, bert):\n",
        "      \n",
        "      super(BERT_Arch, self).__init__()\n",
        "\n",
        "      self.bert = bert \n",
        "      \n",
        "      # dropout layer\n",
        "      self.dropout = nn.Dropout(0.1)\n",
        "      \n",
        "      # relu activation function\n",
        "      self.relu =  nn.ReLU()\n",
        "\n",
        "      # dense layer 1\n",
        "      self.fc1 = nn.Linear(768,512)\n",
        "      \n",
        "      # dense layer 2 (Output layer)\n",
        "      self.fc2 = nn.Linear(512,2)\n",
        "\n",
        "      #softmax activation function\n",
        "      self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    #define the forward pass\n",
        "    def forward(self, sent_id, mask):\n",
        "\n",
        "      #pass the inputs to the model  \n",
        "      _, cls_hs = self.bert(sent_id, attention_mask=mask)\n",
        "      \n",
        "      x = self.fc1(cls_hs)\n",
        "\n",
        "      x = self.relu(x)\n",
        "\n",
        "      x = self.dropout(x)\n",
        "\n",
        "      # output layer\n",
        "      x = self.fc2(x)\n",
        "      \n",
        "      # apply softmax activation\n",
        "      x = self.softmax(x)\n",
        "\n",
        "      return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir-IDA9NleSP"
      },
      "source": [
        "# pass the pre-trained BERT to our define architecture\n",
        "model = BERT_Arch(bert)\n",
        "\n",
        "# push the model to GPU\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKMwzu7-xUex"
      },
      "source": [
        "# optimizer from hugging face transformers\n",
        "from transformers import AdamW\n",
        "\n",
        "# define the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr = 1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-J92kNcxYVy"
      },
      "source": [
        "## **Find class weights**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DV1BLQjwxa2z",
        "outputId": "2da97689-1a44-4870-b5ca-a7ef75e3ad01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "#compute the class weights\n",
        "class_wts = compute_class_weight('balanced', np.unique(train_labels), train_labels)\n",
        "\n",
        "print(class_wts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.42493473 0.77028934]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dH2rGE2xgE7"
      },
      "source": [
        "# convert class weights to tensor\n",
        "weights= torch.tensor(class_wts,dtype=torch.float)\n",
        "weights = weights.to(device)\n",
        "\n",
        "# loss function\n",
        "cross_entropy  = nn.NLLLoss(weight=weights) \n",
        "\n",
        "# number of training epochs\n",
        "epochs = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bz0IWUExoWG"
      },
      "source": [
        "## **Fine-tune BERT**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhXUs0Wuxqq0"
      },
      "source": [
        "# function to train the model\n",
        "def train():\n",
        "  \n",
        "  model.train()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save model predictions\n",
        "  total_preds=[]\n",
        "  \n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(train_dataloader):\n",
        "    \n",
        "    # progress update after every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [r.to(device) for r in batch]\n",
        " \n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # clear previously calculated gradients \n",
        "    model.zero_grad()        \n",
        "\n",
        "    # get model predictions for the current batch\n",
        "    preds = model(sent_id, mask)\n",
        "\n",
        "    # compute the loss between actual and predicted values\n",
        "    loss = cross_entropy(preds, labels)\n",
        "\n",
        "    # add on to the total loss\n",
        "    total_loss = total_loss + loss.item()\n",
        "\n",
        "    # backward pass to calculate the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # model predictions are stored on GPU. So, push it to CPU\n",
        "    preds=preds.detach().cpu().numpy()\n",
        "\n",
        "    # append the model predictions\n",
        "    total_preds.append(preds)\n",
        "\n",
        "  # compute the training loss of the epoch\n",
        "  avg_loss = total_loss / len(train_dataloader)\n",
        "  \n",
        "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  #returns the loss and predictions\n",
        "  return avg_loss, total_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-cFUyDXxwp4"
      },
      "source": [
        "# function for evaluating the model\n",
        "def evaluate():\n",
        "  \n",
        "  print(\"\\nEvaluating...\")\n",
        "  \n",
        "  # deactivate dropout layers\n",
        "  model.eval()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save the model predictions\n",
        "  total_preds = []\n",
        "\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(val_dataloader):\n",
        "    \n",
        "    # Progress update every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      \n",
        "      # Calculate elapsed time in minutes.\n",
        "      elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "      # Report progress.\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [t.to(device) for t in batch]\n",
        "\n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # deactivate autograd\n",
        "    with torch.no_grad():\n",
        "      \n",
        "      # model predictions\n",
        "      preds = model(sent_id, mask)\n",
        "\n",
        "      # compute the validation loss between actual and predicted values\n",
        "      loss = cross_entropy(preds,labels)\n",
        "\n",
        "      total_loss = total_loss + loss.item()\n",
        "\n",
        "      preds = preds.detach().cpu().numpy()\n",
        "\n",
        "      total_preds.append(preds)\n",
        "\n",
        "  # compute the validation loss of the epoch\n",
        "  avg_loss = total_loss / len(val_dataloader) \n",
        "\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  return avg_loss, total_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-Ws9Ekdx2BL"
      },
      "source": [
        "## **Start model training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeyQbRcIx63j",
        "outputId": "723a17c9-ef07-4ea0-d5e3-057e03caa0a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# set initial loss to infinite\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "# empty lists to store training and validation loss of each epoch\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "\n",
        "#for each epoch\n",
        "for epoch in range(epochs):\n",
        "     \n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "    \n",
        "    #train model\n",
        "    train_loss, _ = train()\n",
        "    \n",
        "    #evaluate model\n",
        "    valid_loss, _ = evaluate()\n",
        "    \n",
        "    #save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    # append training and validation loss\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    \n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch 1 / 10\n",
            "  Batch    50  of     69.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.643\n",
            "Validation Loss: 0.512\n",
            "\n",
            " Epoch 2 / 10\n",
            "  Batch    50  of     69.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.536\n",
            "Validation Loss: 0.474\n",
            "\n",
            " Epoch 3 / 10\n",
            "  Batch    50  of     69.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.523\n",
            "Validation Loss: 0.459\n",
            "\n",
            " Epoch 4 / 10\n",
            "  Batch    50  of     69.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.513\n",
            "Validation Loss: 0.525\n",
            "\n",
            " Epoch 5 / 10\n",
            "  Batch    50  of     69.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.498\n",
            "Validation Loss: 0.420\n",
            "\n",
            " Epoch 6 / 10\n",
            "  Batch    50  of     69.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.509\n",
            "Validation Loss: 0.453\n",
            "\n",
            " Epoch 7 / 10\n",
            "  Batch    50  of     69.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.492\n",
            "Validation Loss: 0.442\n",
            "\n",
            " Epoch 8 / 10\n",
            "  Batch    50  of     69.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.485\n",
            "Validation Loss: 0.405\n",
            "\n",
            " Epoch 9 / 10\n",
            "  Batch    50  of     69.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.495\n",
            "Validation Loss: 0.438\n",
            "\n",
            " Epoch 10 / 10\n",
            "  Batch    50  of     69.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.483\n",
            "Validation Loss: 0.393\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md9JKkEiyvyz"
      },
      "source": [
        "## **Load saved model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1zfYOtOyyFi",
        "outputId": "1c227000-d88a-40b1-d685-7f3f1a9ff9a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#load weights of best model\n",
        "path = 'saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBCAieAZy3bI"
      },
      "source": [
        "## **Get predictions for test data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TWj7leDy56B"
      },
      "source": [
        "# get predictions for test data\n",
        "with torch.no_grad():\n",
        "  preds = model(test_seq.to(device), test_mask.to(device))\n",
        "  preds = preds.detach().cpu().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxIllDNny-wh",
        "outputId": "7a6d4d04-2844-4b4f-de0a-a05aa169fb58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# model's performance\n",
        "preds = np.argmax(preds, axis = 1)\n",
        "print(classification_report(test_y, preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.88      0.79       164\n",
            "           1       0.93      0.81      0.87       304\n",
            "\n",
            "    accuracy                           0.84       468\n",
            "   macro avg       0.82      0.85      0.83       468\n",
            "weighted avg       0.85      0.84      0.84       468\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Am2xuiFv4jzI"
      },
      "source": [
        "# **Fine-tuning XLNet for Student Dropout Classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LtpPLmB4t_8"
      },
      "source": [
        "## **Install the Transformers Library**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ei3UEyqj4uwI"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlsIBftC40HH"
      },
      "source": [
        "## **Import necessary libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gq51iCDg426A"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from transformers import XLNetForSequenceClassification, XLNetModel, XLNetTokenizer\n",
        "from transformers import AdamW\n",
        "\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyNq4-dn49bc"
      },
      "source": [
        "## **Set GPU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DohZDuF94_JK",
        "outputId": "d5534459-339c-4aaa-8801-1c10f2a416ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Tesla K80'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4gkByiv5UNX"
      },
      "source": [
        "## **Load the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9b7CvvH5U6e"
      },
      "source": [
        "DATASET_PATH = \"/content/drive/My Drive/AI_Project_CS_534/Datasets/dropping_out_tweets_part1_labeled.csv\"\n",
        "dataset_df = pd.read_csv(DATASET_PATH, sep=\";\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xdard8pn5gix"
      },
      "source": [
        "## **Convert label to numerical value**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ooZP_cH5hYm"
      },
      "source": [
        "# Convert label categorical value to numerical value\n",
        "label_numerical_value = dataset_df[\"label\"].astype('category').cat.codes\n",
        "dataset_df[\"label\"] = label_numerical_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yshReQv6UTj"
      },
      "source": [
        "## **Add XLNet special tokens**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iif5wruS6Xd8"
      },
      "source": [
        "# Create tweet and label lists\n",
        "tweets = dataset_df.tweet.values\n",
        "\n",
        "tweets = [tweet + \" [SEP] [CLS]\" for tweet in tweets]\n",
        "labels = dataset_df.label.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpEJ_E1E7qQz"
      },
      "source": [
        "## **Tokenize tweets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXQBC2Rn7sw7",
        "outputId": "c911b3ed-0326-4283-e818-0a1c095d0126",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120,
          "referenced_widgets": [
            "9150eaebc2a4441fa37016bcf8f7787c",
            "ec95ff112f79497f9c81258bf4db5c25",
            "f82b46ef7c4946e2b8a0e5114d1d3ecd",
            "9b506b1e0045408396f22a07c4208acc",
            "8333ad69b2e842a4ad28ff6a1082583d",
            "04acc9ca63c7438dbec920adb46ed6e9",
            "49173aa493b945d7bfe1ea1c87fd675b",
            "362929522401404dbb07a2cc060ffb54"
          ]
        }
      },
      "source": [
        "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased', do_lower_case=True)\n",
        "\n",
        "tokenized_tweets = [tokenizer.tokenize(tweet) for tweet in tweets]\n",
        "print (\"Tokenize the first sentence:\")\n",
        "print (tokenized_tweets[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9150eaebc2a4441fa37016bcf8f7787c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=798011.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Tokenize the first sentence:\n",
            "['▁', '@', 'just', 'in', 'bie', 'ber', '▁', '@', 'ch', 'ance', 'the', 'ra', 'pper', '▁', 'i', '▁lost', '▁both', '▁my', '▁jobs', '▁during', '▁the', '▁pandemic', '.', '▁', 'i', '▁then', '▁got', '▁2', '▁new', '▁jobs', '▁during', '▁a', 'ugu', 'st', '.', '▁', 'i', '▁had', '▁to', '▁be', '▁let', '▁go', '▁of', '▁one', '▁because', '▁of', '▁budget', '▁cuts', ',', '▁and', '▁my', '▁hours', '▁were', '▁decreased', '▁at', '▁the', '▁other', '.', '▁', 'i', '▁am', '▁struggling', ',', 'i', '’', 'm', '▁thinking', '▁of', '▁dropping', '▁out', '▁of', '▁school', '.', '▁', '#', 'j', 'b', 'ch', 'ance', 'ho', 'ly', '▁[', 's', 'ep', ']', '▁[', 'cl', 's', ']']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX4xdTen76hj"
      },
      "source": [
        "## **Set maximum sequence length**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnur-Wzw79xR"
      },
      "source": [
        "MAX_LEN = 128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lINHoyhi8EGj"
      },
      "source": [
        "# Use the XLNet tokenizer to convert the tokens to their index numbers in the XLNet vocabulary\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_tweets]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_-2q4m-8FAb"
      },
      "source": [
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWcZSPUP8R38"
      },
      "source": [
        "## **Create attention masks**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIBBWtAY8Uku"
      },
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9p3KgPa8ZP9"
      },
      "source": [
        "## **Split train and test data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAVu01mT8cGe"
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for training\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=2018, test_size=0.1)\n",
        "\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
        "                                             random_state=2018, test_size=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bgHQvSl8jrZ"
      },
      "source": [
        "## **Convert data into Torch Tensors**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUTo_ObY8mkg"
      },
      "source": [
        "# Convert all of our data into torch tensors, the required datatype for our model\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
        "validation_labels = torch.tensor(validation_labels, dtype=torch.long)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iugGw_wP8xx9"
      },
      "source": [
        "# Select a batch size for training. For fine-tuning with XLNet, the authors recommend a batch size of 32, 48, or 128. We will use 32 here to avoid memory issues.\n",
        "batch_size = 32\n",
        "\n",
        "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
        "# with an iterator the entire dataset does not need to be loaded into memory\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nVyxVPk832S"
      },
      "source": [
        "## **Training the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lpx0YlAW86cx",
        "outputId": "2ae0a333-e36b-426e-fd94-cecb91414363",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Load XLNEtForSequenceClassification, the pretrained XLNet model with a single linear classification layer on top. \n",
        "\n",
        "model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=2)\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
            "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XLNetForSequenceClassification(\n",
              "  (transformer): XLNetModel(\n",
              "    (word_embedding): Embedding(32000, 768)\n",
              "    (layer): ModuleList(\n",
              "      (0): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (3): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (4): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (5): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (6): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (7): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (8): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (9): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (10): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (11): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (sequence_summary): SequenceSummary(\n",
              "    (summary): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (first_dropout): Identity()\n",
              "    (last_dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (logits_proj): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3oF5RYe9Apj"
      },
      "source": [
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDyPNAp59DmR"
      },
      "source": [
        "# This variable contains all of the hyperparemeter information our training loop needs\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BBBw6S29J0i"
      },
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHutBDsx9Kdy",
        "outputId": "bc3f77ec-ac1b-4a1f-8531-6bd2857e2106",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Store our loss and accuracy for plotting\n",
        "train_loss_set = []\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# trange is a tqdm wrapper around the normal python range\n",
        "for _ in trange(epochs, desc=\"Epoch\"):\n",
        "  \n",
        "  \n",
        "  # Training\n",
        "  \n",
        "  # Set our model to training mode (as opposed to evaluation mode)\n",
        "  model.train()\n",
        "  \n",
        "  # Tracking variables\n",
        "  tr_loss = 0\n",
        "  nb_tr_examples, nb_tr_steps = 0, 0\n",
        "  \n",
        "  # Train the data for one epoch\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    # Clear out the gradients (by default they accumulate)\n",
        "    optimizer.zero_grad()\n",
        "    # Forward pass\n",
        "    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "    loss = outputs[0]\n",
        "    logits = outputs[1]\n",
        "    train_loss_set.append(loss.item())    \n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    # Update parameters and take a step using the computed gradient\n",
        "    optimizer.step()\n",
        "    \n",
        "    \n",
        "    # Update tracking variables\n",
        "    tr_loss += loss.item()\n",
        "    nb_tr_examples += b_input_ids.size(0)\n",
        "    nb_tr_steps += 1\n",
        "\n",
        "  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "    \n",
        "    \n",
        "  # Validation\n",
        "\n",
        "  # Put model in evaluation mode to evaluate loss on the validation set\n",
        "  model.eval()\n",
        "\n",
        "  # Tracking variables \n",
        "  eval_loss, eval_accuracy = 0, 0\n",
        "  nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "  # Evaluate data for one epoch\n",
        "  for batch in validation_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "    with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      output = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "      logits = output[0]\n",
        "    \n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "    \n",
        "    eval_accuracy += tmp_eval_accuracy\n",
        "    nb_eval_steps += 1\n",
        "\n",
        "  print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:   0%|          | 0/4 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.45156860182231123\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  25%|██▌       | 1/4 [02:32<07:37, 152.41s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.8489583333333334\n",
            "Train loss: 0.29824153740297665\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  50%|█████     | 2/4 [05:04<05:04, 152.32s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.8625\n",
            "Train loss: 0.24690783159299332\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  75%|███████▌  | 3/4 [07:36<02:32, 152.20s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.8760416666666666\n",
            "Train loss: 0.20996623681011525\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 100%|██████████| 4/4 [10:08<00:00, 152.10s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.8927083333333334\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}