{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI_Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fmejias/CS534-ArtificialIntelligenceProject/blob/main/AI_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pj5-J2tBsoGW"
      },
      "source": [
        "# **CS 534 - Artificial Intelligence**\n",
        "\n",
        "## **Project Title: Text Mining and Sentiment Analysis on Twitter for predicting students dropping out during the pandemic.**\n",
        "\n",
        "### **Students**\n",
        "\n",
        "\n",
        "*   Merzia Adamjee\n",
        "*   Alketa Guxha\n",
        "*   Felipe Mejias\n",
        "*   Nikita Boguslavskii\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ia8Bio_dt21i"
      },
      "source": [
        "# **Initial configuration of the environment for the development of the project**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OIfGOowqmL3"
      },
      "source": [
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from imblearn.over_sampling import SMOTE"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHwMawOhc6wI"
      },
      "source": [
        "# **Install Textblob and Imbalanced Learn**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_JgmJsnc9rW"
      },
      "source": [
        "!pip install textblob\n",
        "!pip install imbalanced-learn\n",
        "!pip3 install flair"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_FDX7NI0dTp"
      },
      "source": [
        "# **Google Authentication to read CSV File from Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3moATrP0lQ_",
        "outputId": "7c98f3de-6de3-4235-b54e-41493fa3b573",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Needed for Google Authentication Step\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzTa-Jj31bEs"
      },
      "source": [
        "# **Upload Dataset from Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DR3T3xz-1erF"
      },
      "source": [
        "DATASET_PATH = \"/content/drive/My Drive/AI_Project_CS_534/Datasets/dropping_out_tweets_part1_labeled.csv\"\n",
        "dataset_df = pd.read_csv(DATASET_PATH, sep=\";\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYx-KyA43FAu"
      },
      "source": [
        "# **Dataset Information**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDtxCzzf3K6d"
      },
      "source": [
        "# Print a summary of the Dataset\n",
        "result = dataset_df.head(10)\n",
        "print(\"First 10 rows of the DataFrame:\")\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbbjj9bAOYQS"
      },
      "source": [
        "# **Dataset Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8rCPzSqPLp9"
      },
      "source": [
        "## **Select labeled dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNL3ca_PPPDJ"
      },
      "source": [
        "# NOTE: This selection is because they are the only label rows\n",
        "labeled_dataset_df = dataset_df.head(1200)"
      ],
      "execution_count": 279,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKc6gdgtOjJi"
      },
      "source": [
        "## **Filtering irrelevant examples**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WdCZhSjOawL",
        "outputId": "dac3a4b8-64bd-43c5-ff10-c0b02588b258",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "IRRELEVANT_KEYWORDS = [\"Bernie\", \"Trump\", \"Sanders\", \"to become\", \n",
        "                       \"to pursue\", \"and becoming\", \"and going\",\n",
        "                       \"and be\", \"so I can\", \"so i can\", \"to run\",\n",
        "                       \"to spend\", \"to focus\", \"and living\", \"marry\",\n",
        "                       \"stripper\", \"and joining\", \"and pursuing\",\n",
        "                       \"bts\", \"BTS\", \"and running\", \"to go\", \"and making\",\n",
        "                       \"to dedicate\"]\n",
        "\n",
        "def filtering_irrelevant_examples(twitter_dataset):\n",
        "  \"\"\"\n",
        "  Filtering irrelevant tweets from the Twitter dataset.\n",
        "  \"\"\"\n",
        "  def check_tweet_relevance(tweet):\n",
        "    \"\"\"\n",
        "    Filtering irrelevant tweets from the Twitter dataset.\n",
        "    \"\"\"\n",
        "    if any(indicator in tweet for indicator in IRRELEVANT_KEYWORDS):\n",
        "      return \"irrelevant\"\n",
        "    return \"relevant\"\n",
        "  return twitter_dataset[twitter_dataset[\"tweet\"].apply(check_tweet_relevance) \\\n",
        "                         != \"irrelevant\"]\n",
        "\n",
        "# Filter the irrelevant tweets\n",
        "labeled_dataset_df = filtering_irrelevant_examples(labeled_dataset_df)\n",
        "rows, columns = labeled_dataset_df.shape\n",
        "print(\"New number of rows: \", rows)"
      ],
      "execution_count": 280,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "New number of rows:  936\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aW0f3PDbT4X8"
      },
      "source": [
        "## **Convert all letters to lower case**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJ2YCm5cT8ST",
        "outputId": "c51223c9-166c-443f-db9c-393d2b4bcac8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def convert_letters_to_lower_case(twitter_dataset):\n",
        "  \"\"\"\n",
        "  Convert all letters to lower case.\n",
        "  \"\"\"\n",
        "  def tweet_to_lower_case(tweet):\n",
        "    \"\"\"\n",
        "    Convert tweet text to lower case.\n",
        "    \"\"\"\n",
        "    return tweet.lower()\n",
        "\n",
        "  twitter_dataset[\"tweet\"] = twitter_dataset[\"tweet\"].apply(tweet_to_lower_case)\n",
        "\n",
        "# Convert all tweets to lower case\n",
        "convert_letters_to_lower_case(labeled_dataset_df)\n",
        "\n",
        "# Show results\n",
        "result = labeled_dataset_df.head(10)\n",
        "print(\"First 10 rows of the DataFrame after lower case:\")\n",
        "print(result)"
      ],
      "execution_count": 281,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 10 rows of the DataFrame after lower case:\n",
            "                     id  ...                     label\n",
            "0   1309219160828895233  ...      Intention of dropout\n",
            "1   1308809031583236096  ...  Not intention of dropout\n",
            "2   1308716552229998593  ...  Not intention of dropout\n",
            "3   1308483739584835585  ...  Not intention of dropout\n",
            "4   1308351921875345409  ...  Not intention of dropout\n",
            "5   1307763236062650368  ...  Not intention of dropout\n",
            "7   1306800980307083267  ...  Not intention of dropout\n",
            "8   1305869148463992832  ...  Not intention of dropout\n",
            "9   1305659343971311616  ...  Not intention of dropout\n",
            "10  1305617685020053512  ...  Not intention of dropout\n",
            "\n",
            "[10 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDE-F9yJVifX"
      },
      "source": [
        "## **Remove usernames that appear within a tweet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpJ-HfSPVm_s",
        "outputId": "debdbd1c-6b9c-4a2c-f55a-84f7a241cfc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def remove_usernames_from_tweets(twitter_dataset):\n",
        "  \"\"\"\n",
        "  Remove all usernames that appear on a tweet.\n",
        "  \"\"\"\n",
        "  def remove_username(tweet):\n",
        "    \"\"\"\n",
        "    Remove username from tweet.\n",
        "    \"\"\"\n",
        "    return re.sub('@[\\w]+','', tweet)\n",
        "\n",
        "  twitter_dataset[\"tweet\"] = twitter_dataset[\"tweet\"].apply(remove_username)\n",
        "\n",
        "# Remove all usernames that appear in a tweet\n",
        "remove_usernames_from_tweets(labeled_dataset_df)\n",
        "\n",
        "# Show results\n",
        "result = labeled_dataset_df.head(10)\n",
        "print(\"First 10 rows of the DataFrame after removing usernames:\")\n",
        "print(result)"
      ],
      "execution_count": 282,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 10 rows of the DataFrame after removing usernames:\n",
            "                     id  ...                     label\n",
            "0   1309219160828895233  ...      Intention of dropout\n",
            "1   1308809031583236096  ...  Not intention of dropout\n",
            "2   1308716552229998593  ...  Not intention of dropout\n",
            "3   1308483739584835585  ...  Not intention of dropout\n",
            "4   1308351921875345409  ...  Not intention of dropout\n",
            "5   1307763236062650368  ...  Not intention of dropout\n",
            "7   1306800980307083267  ...  Not intention of dropout\n",
            "8   1305869148463992832  ...  Not intention of dropout\n",
            "9   1305659343971311616  ...  Not intention of dropout\n",
            "10  1305617685020053512  ...  Not intention of dropout\n",
            "\n",
            "[10 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEFpLAeDbxLM"
      },
      "source": [
        "## **Remove hashtags that appear within a tweet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGTsQQ8Hbzav",
        "outputId": "ed8efc3c-d11f-4f13-d577-291d3919b49e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def remove_hashtags_from_tweets(twitter_dataset):\n",
        "  \"\"\"\n",
        "  Remove all hashtags that appear on a tweet.\n",
        "  \"\"\"\n",
        "  def remove_hashtags(tweet):\n",
        "    \"\"\"\n",
        "    Remove hashtags from tweet.\n",
        "    \"\"\"\n",
        "    return tweet.replace(\"#\", \"\").replace(\"_\", \" \")\n",
        "\n",
        "  twitter_dataset[\"tweet\"] = twitter_dataset[\"tweet\"].apply(remove_hashtags)\n",
        "\n",
        "# Remove all hashtags that appear in a tweet\n",
        "remove_hashtags_from_tweets(labeled_dataset_df)\n",
        "\n",
        "# Show results\n",
        "result = labeled_dataset_df.head(10)\n",
        "print(\"First 10 rows of the DataFrame after removing usernames:\")\n",
        "print(result)"
      ],
      "execution_count": 283,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 10 rows of the DataFrame after removing usernames:\n",
            "                     id  ...                     label\n",
            "0   1309219160828895233  ...      Intention of dropout\n",
            "1   1308809031583236096  ...  Not intention of dropout\n",
            "2   1308716552229998593  ...  Not intention of dropout\n",
            "3   1308483739584835585  ...  Not intention of dropout\n",
            "4   1308351921875345409  ...  Not intention of dropout\n",
            "5   1307763236062650368  ...  Not intention of dropout\n",
            "7   1306800980307083267  ...  Not intention of dropout\n",
            "8   1305869148463992832  ...  Not intention of dropout\n",
            "9   1305659343971311616  ...  Not intention of dropout\n",
            "10  1305617685020053512  ...  Not intention of dropout\n",
            "\n",
            "[10 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qXBpdT2Z0HB"
      },
      "source": [
        "## **Remove special characters and punctuation that appear within a tweet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8F884syZ3d2",
        "outputId": "9a00561f-b469-4cbb-8fd4-7e9d35a6350b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def remove_special_characters_and_punctuation_from_tweets(twitter_dataset):\n",
        "  \"\"\"\n",
        "  Remove all special characters and punctuation that appear on a tweet.\n",
        "  \"\"\"\n",
        "  def remove_special_characters_and_punctuation(tweet):\n",
        "    \"\"\"\n",
        "    Remove special characters and punctuation from tweet.\n",
        "    \"\"\"\n",
        "    return re.sub('[^A-Za-z0-9 ]+', '', tweet)\n",
        "\n",
        "  twitter_dataset[\"tweet\"] = twitter_dataset[\"tweet\"].apply(remove_special_characters_and_punctuation)\n",
        "\n",
        "# Remove all special characters and punctuation that appear in a tweet\n",
        "remove_special_characters_and_punctuation_from_tweets(labeled_dataset_df)\n",
        "\n",
        "# Show results\n",
        "result = labeled_dataset_df.head(10)\n",
        "print(\"First 10 rows of the DataFrame after removing usernames:\")\n",
        "print(result)"
      ],
      "execution_count": 284,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 10 rows of the DataFrame after removing usernames:\n",
            "                     id  ...                     label\n",
            "0   1309219160828895233  ...      Intention of dropout\n",
            "1   1308809031583236096  ...  Not intention of dropout\n",
            "2   1308716552229998593  ...  Not intention of dropout\n",
            "3   1308483739584835585  ...  Not intention of dropout\n",
            "4   1308351921875345409  ...  Not intention of dropout\n",
            "5   1307763236062650368  ...  Not intention of dropout\n",
            "7   1306800980307083267  ...  Not intention of dropout\n",
            "8   1305869148463992832  ...  Not intention of dropout\n",
            "9   1305659343971311616  ...  Not intention of dropout\n",
            "10  1305617685020053512  ...  Not intention of dropout\n",
            "\n",
            "[10 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alLP6ZB6aq3t"
      },
      "source": [
        "## **Remove URLs that appear within a tweet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQ2MbNsFat1g",
        "outputId": "453701b1-60fb-49f6-9c5f-d968c95203d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def remove_urls_from_tweets(twitter_dataset):\n",
        "  \"\"\"\n",
        "  Remove all urls that appear on a tweet.\n",
        "  \"\"\"\n",
        "  def remove_urls(tweet):\n",
        "    \"\"\"\n",
        "    Remove urls from tweet.\n",
        "    \"\"\"\n",
        "    return re.sub(r'http\\S+', '', tweet)\n",
        "\n",
        "  twitter_dataset[\"tweet\"] = twitter_dataset[\"tweet\"].apply(remove_urls)\n",
        "\n",
        "# Remove all urls that appear in a tweet\n",
        "remove_urls_from_tweets(labeled_dataset_df)\n",
        "\n",
        "# Show results\n",
        "result = labeled_dataset_df.head(10)\n",
        "print(\"First 10 rows of the DataFrame after removing usernames:\")\n",
        "print(result)"
      ],
      "execution_count": 285,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 10 rows of the DataFrame after removing usernames:\n",
            "                     id  ...                     label\n",
            "0   1309219160828895233  ...      Intention of dropout\n",
            "1   1308809031583236096  ...  Not intention of dropout\n",
            "2   1308716552229998593  ...  Not intention of dropout\n",
            "3   1308483739584835585  ...  Not intention of dropout\n",
            "4   1308351921875345409  ...  Not intention of dropout\n",
            "5   1307763236062650368  ...  Not intention of dropout\n",
            "7   1306800980307083267  ...  Not intention of dropout\n",
            "8   1305869148463992832  ...  Not intention of dropout\n",
            "9   1305659343971311616  ...  Not intention of dropout\n",
            "10  1305617685020053512  ...  Not intention of dropout\n",
            "\n",
            "[10 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjQ2xaSrck31"
      },
      "source": [
        "# **Approach using Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCmqa8TicrPF"
      },
      "source": [
        "## **Create features using sentiment analysis and unigrams and Textblob**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbFqaoU3cq2D",
        "outputId": "c8704c80-cd44-4a10-aac5-a6683e6f69e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "def calculate_features_using_polarity(twitter_dataset):\n",
        "  \"\"\"\n",
        "  Use Textblob polarity to calculate the number of positive and negative words.\n",
        "  \"\"\"\n",
        "  def calculate_positive_words(tweet):\n",
        "    \"\"\"\n",
        "    Count number of positive words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_positive_words = 0\n",
        "    for word in tweet.split():\n",
        "      if TextBlob(word).polarity >= 0:\n",
        "        number_of_positive_words = number_of_positive_words + 1\n",
        "    return number_of_positive_words\n",
        "  \n",
        "  def calculate_ratio_positive_words(tweet):\n",
        "    \"\"\"\n",
        "    Calculate the ratio of positive words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_positive_words = 0\n",
        "    for word in tweet.split():\n",
        "      if TextBlob(word).polarity > 0:\n",
        "        number_of_positive_words = number_of_positive_words + 1\n",
        "    return number_of_positive_words/len(tweet.split())\n",
        "  \n",
        "  def calculate_negative_words(tweet):\n",
        "    \"\"\"\n",
        "    Count number of negative words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_negative_words = 0\n",
        "    for word in tweet.split():\n",
        "      if TextBlob(word).polarity < 0:\n",
        "        number_of_negative_words = number_of_negative_words + 1\n",
        "    return number_of_negative_words\n",
        "  \n",
        "  def calculate_ratio_negative_words(tweet):\n",
        "    \"\"\"\n",
        "    Calculate the ratio of negative words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_negative_words = 0\n",
        "    for word in tweet.split():\n",
        "      if TextBlob(word).polarity < 0:\n",
        "        number_of_negative_words = number_of_negative_words + 1\n",
        "    return number_of_negative_words/len(tweet.split())\n",
        "  \n",
        "  twitter_dataset[\"unigram_number_positive_words\"] = twitter_dataset[\"tweet\"].apply(calculate_positive_words)\n",
        "  twitter_dataset[\"unigram_ratio_positive_words\"] = twitter_dataset[\"tweet\"].apply(calculate_ratio_positive_words)\n",
        "  twitter_dataset[\"unigram_number_negative_words\"] = twitter_dataset[\"tweet\"].apply(calculate_negative_words)\n",
        "  twitter_dataset[\"unigram_ratio_negative_words\"] = twitter_dataset[\"tweet\"].apply(calculate_ratio_negative_words)\n",
        "\n",
        "# Calculate new features using sentiment Analysis\n",
        "calculate_features_using_polarity(labeled_dataset_df)\n",
        "\n",
        "# Show results\n",
        "result = labeled_dataset_df.head(10)\n",
        "print(\"First 10 rows of the DataFrame after removing usernames:\")\n",
        "print(result)"
      ],
      "execution_count": 286,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 10 rows of the DataFrame after removing usernames:\n",
            "                     id  ... unigram_ratio_negative_words\n",
            "0   1309219160828895233  ...                     0.043478\n",
            "1   1308809031583236096  ...                     0.020833\n",
            "2   1308716552229998593  ...                     0.046512\n",
            "3   1308483739584835585  ...                     0.000000\n",
            "4   1308351921875345409  ...                     0.034483\n",
            "5   1307763236062650368  ...                     0.000000\n",
            "7   1306800980307083267  ...                     0.000000\n",
            "8   1305869148463992832  ...                     0.000000\n",
            "9   1305659343971311616  ...                     0.000000\n",
            "10  1305617685020053512  ...                     0.028571\n",
            "\n",
            "[10 rows x 7 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmwSKyfZ1det"
      },
      "source": [
        "## **Create features using sentiment analysis and bigrams and Textblob**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GacX5i7k1eJd",
        "outputId": "e63487e5-bc4e-4f37-febd-cd3e3bce2cf8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "def find_ngrams(n, input_sequence):\n",
        "  # Split sentence into tokens.\n",
        "  tokens = input_sequence.split()\n",
        "  ngrams = []\n",
        "  for i in range(len(tokens) - n + 1):\n",
        "    # Take n consecutive tokens in array.\n",
        "    ngram = tokens[i:i+n]\n",
        "    # Concatenate array items into string.\n",
        "    ngram = ' '.join(ngram)\n",
        "    ngrams.append(ngram)\n",
        "  return ngrams\n",
        "\n",
        "def calculate_bigram_features_using_polarity(twitter_dataset):\n",
        "  \"\"\"\n",
        "  Use Textblob polarity to calculate the number of positive and negative words.\n",
        "  \"\"\"\n",
        "  def calculate_positive_words(tweet):\n",
        "    \"\"\"\n",
        "    Count number of positive words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_positive_bigrams = 0\n",
        "    ngrams = find_ngrams(2, tweet)\n",
        "    for ngram in ngrams:\n",
        "      if TextBlob(ngram).polarity >= 0:\n",
        "        number_of_positive_bigrams = number_of_positive_bigrams + 1\n",
        "    return number_of_positive_bigrams\n",
        "  \n",
        "  def calculate_ratio_positive_words(tweet):\n",
        "    \"\"\"\n",
        "    Calculate the ratio of positive words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_positive_bigrams = 0\n",
        "    ngrams = find_ngrams(2, tweet)\n",
        "    for ngram in ngrams:\n",
        "      if TextBlob(ngram).polarity >= 0:\n",
        "        number_of_positive_bigrams = number_of_positive_bigrams + 1\n",
        "    return number_of_positive_bigrams/len(ngrams)\n",
        "  \n",
        "  def calculate_negative_words(tweet):\n",
        "    \"\"\"\n",
        "    Count number of negative words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_negative_bigrams = 0\n",
        "    ngrams = find_ngrams(2, tweet)\n",
        "    for ngram in ngrams:\n",
        "      if TextBlob(ngram).polarity < 0:\n",
        "        number_of_negative_bigrams = number_of_negative_bigrams + 1\n",
        "    return number_of_negative_bigrams\n",
        "  \n",
        "  def calculate_ratio_negative_words(tweet):\n",
        "    \"\"\"\n",
        "    Calculate the ratio of negative words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_negative_bigrams = 0\n",
        "    ngrams = find_ngrams(2, tweet)\n",
        "    for ngram in ngrams:\n",
        "      if TextBlob(ngram).polarity < 0:\n",
        "        number_of_negative_bigrams = number_of_negative_bigrams + 1\n",
        "    return number_of_negative_bigrams/len(ngrams)\n",
        "  \n",
        "  twitter_dataset[\"bigram_number_positive_words\"] = twitter_dataset[\"tweet\"].apply(calculate_positive_words)\n",
        "  twitter_dataset[\"bigram_ratio_positive_words\"] = twitter_dataset[\"tweet\"].apply(calculate_ratio_positive_words)\n",
        "  twitter_dataset[\"bigram_number_negative_words\"] = twitter_dataset[\"tweet\"].apply(calculate_negative_words)\n",
        "  twitter_dataset[\"bigram_ratio_negative_words\"] = twitter_dataset[\"tweet\"].apply(calculate_ratio_negative_words)\n",
        "\n",
        "# Calculate new features using sentiment Analysis\n",
        "calculate_bigram_features_using_polarity(labeled_dataset_df)\n",
        "\n",
        "# Show results\n",
        "result = labeled_dataset_df.head(10)\n",
        "print(\"First 10 rows of the DataFrame after removing usernames:\")\n",
        "print(result)"
      ],
      "execution_count": 287,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 10 rows of the DataFrame after removing usernames:\n",
            "                     id  ... bigram_ratio_negative_words\n",
            "0   1309219160828895233  ...                    0.088889\n",
            "1   1308809031583236096  ...                    0.042553\n",
            "2   1308716552229998593  ...                    0.095238\n",
            "3   1308483739584835585  ...                    0.000000\n",
            "4   1308351921875345409  ...                    0.071429\n",
            "5   1307763236062650368  ...                    0.000000\n",
            "7   1306800980307083267  ...                    0.000000\n",
            "8   1305869148463992832  ...                    0.000000\n",
            "9   1305659343971311616  ...                    0.000000\n",
            "10  1305617685020053512  ...                    0.058824\n",
            "\n",
            "[10 rows x 11 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcmx83MY2wKy"
      },
      "source": [
        "## **Create features using sentiment analysis and unigrams and Vader**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7KUiYVY2zQk",
        "outputId": "a6b97c6b-73e3-476d-c780-2714570725b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "nltk.download('vader_lexicon')\n",
        "sentiment_analyzer = SentimentIntensityAnalyzer()"
      ],
      "execution_count": 288,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOsUQl4X23xl",
        "outputId": "2e4d4456-982d-4387-cbe9-38603079e615",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def calculate_features_using_polarity_vader(twitter_dataset):\n",
        "  \"\"\"\n",
        "  Use Vader polarity to calculate the number of positive and negative words.\n",
        "  \"\"\"\n",
        "  def calculate_positive_words(tweet):\n",
        "    \"\"\"\n",
        "    Count number of positive words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_positive_words = 0\n",
        "    for word in tweet.split():\n",
        "      if sentiment_analyzer.polarity_scores(word)[\"compound\"] > 0:\n",
        "        number_of_positive_words = number_of_positive_words + 1\n",
        "    return number_of_positive_words\n",
        "  \n",
        "  def calculate_ratio_positive_words(tweet):\n",
        "    \"\"\"\n",
        "    Calculate the ratio of positive words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_positive_words = 0\n",
        "    for word in tweet.split():\n",
        "      if sentiment_analyzer.polarity_scores(word)[\"compound\"] > 0:\n",
        "        number_of_positive_words = number_of_positive_words + 1\n",
        "    return number_of_positive_words/len(tweet.split())\n",
        "  \n",
        "  def calculate_negative_words(tweet):\n",
        "    \"\"\"\n",
        "    Count number of negative words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_negative_words = 0\n",
        "    for word in tweet.split():\n",
        "      if sentiment_analyzer.polarity_scores(word)[\"compound\"] < 0:\n",
        "        number_of_negative_words = number_of_negative_words + 1\n",
        "    return number_of_negative_words\n",
        "  \n",
        "  def calculate_ratio_negative_words(tweet):\n",
        "    \"\"\"\n",
        "    Calculate the ratio of negative words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_negative_words = 0\n",
        "    for word in tweet.split():\n",
        "      if sentiment_analyzer.polarity_scores(word)[\"compound\"] < 0:\n",
        "        number_of_negative_words = number_of_negative_words + 1\n",
        "    return number_of_negative_words/len(tweet.split())\n",
        "  \n",
        "  twitter_dataset[\"unigram_vader_positive_words\"] = twitter_dataset[\"tweet\"].apply(calculate_positive_words)\n",
        "  twitter_dataset[\"unigram_vader_ratio_positive_words\"] = twitter_dataset[\"tweet\"].apply(calculate_ratio_positive_words)\n",
        "  twitter_dataset[\"unigram_vader_negative_words\"] = twitter_dataset[\"tweet\"].apply(calculate_negative_words)\n",
        "  twitter_dataset[\"unigram_vader_ratio_negative_words\"] = twitter_dataset[\"tweet\"].apply(calculate_ratio_negative_words)\n",
        "\n",
        "# Calculate new features using sentiment Analysis\n",
        "calculate_features_using_polarity_vader(labeled_dataset_df)\n",
        "\n",
        "# Show results\n",
        "result = labeled_dataset_df.head(10)\n",
        "print(\"First 10 rows of the DataFrame after removing usernames:\")\n",
        "print(result)"
      ],
      "execution_count": 289,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 10 rows of the DataFrame after removing usernames:\n",
            "                     id  ... unigram_vader_ratio_negative_words\n",
            "0   1309219160828895233  ...                           0.043478\n",
            "1   1308809031583236096  ...                           0.041667\n",
            "2   1308716552229998593  ...                           0.093023\n",
            "3   1308483739584835585  ...                           0.066667\n",
            "4   1308351921875345409  ...                           0.068966\n",
            "5   1307763236062650368  ...                           0.023256\n",
            "7   1306800980307083267  ...                           0.000000\n",
            "8   1305869148463992832  ...                           0.028571\n",
            "9   1305659343971311616  ...                           0.000000\n",
            "10  1305617685020053512  ...                           0.057143\n",
            "\n",
            "[10 rows x 15 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2Wpltcvj-AX"
      },
      "source": [
        "## **Normalize calculated features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zanC63BTkBam",
        "outputId": "4ee1931b-e131-4357-a5ef-ec22fdf877d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Select calculated features\n",
        "dataset_sentiment_features = labeled_dataset_df[[\"unigram_number_positive_words\", \n",
        "                                                 \"unigram_ratio_positive_words\", \n",
        "                                                 \"unigram_number_negative_words\", \n",
        "                                                 \"unigram_ratio_negative_words\",\n",
        "                                                 \"bigram_number_positive_words\", \n",
        "                                                 \"bigram_ratio_positive_words\", \n",
        "                                                 \"bigram_number_negative_words\", \n",
        "                                                 \"bigram_ratio_negative_words\",\n",
        "                                                 \"unigram_vader_positive_words\", \n",
        "                                                 \"unigram_vader_ratio_positive_words\", \n",
        "                                                 \"unigram_vader_negative_words\", \n",
        "                                                 \"unigram_vader_ratio_negative_words\"]]\n",
        "\n",
        "# Normalize using the mean value\n",
        "normalized_df = (dataset_sentiment_features - dataset_sentiment_features.mean())/dataset_sentiment_features.std()\n",
        "\n",
        "# Show results\n",
        "result = normalized_df.head(10)\n",
        "print(\"First 10 rows of the DataFrame after removing usernames:\")\n",
        "print(result)"
      ],
      "execution_count": 290,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 10 rows of the DataFrame after removing usernames:\n",
            "    unigram_number_positive_words  ...  unigram_vader_ratio_negative_words\n",
            "0                        1.124937  ...                            0.185303\n",
            "1                        1.351068  ...                            0.138183\n",
            "2                        0.898806  ...                            1.473973\n",
            "3                       -1.060996  ...                            0.788436\n",
            "4                       -0.081095  ...                            0.848229\n",
            "5                        1.049560  ...                           -0.340684\n",
            "7                        0.521921  ...                           -0.945570\n",
            "8                        0.446544  ...                           -0.202425\n",
            "9                       -0.382603  ...                           -0.945570\n",
            "10                       0.371167  ...                            0.540721\n",
            "\n",
            "[10 rows x 12 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KszH2igXfDBk"
      },
      "source": [
        "## **Handle imbalance classes using SMOTE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRylaSqlfGm8"
      },
      "source": [
        "print(\"Number of rows with intention of dropout: \", \n",
        "      len(labeled_dataset_df[(labeled_dataset_df['label'] == \"Intention of dropout\")]))\n",
        "print(\"Number of rows with no intention of dropout: \", \n",
        "      len(labeled_dataset_df[(labeled_dataset_df['label'] == \"Not intention of dropout\")]))\n",
        "\n",
        "# Select the data\n",
        "X = normalized_df\n",
        "Y = labeled_dataset_df.label\n",
        "\n",
        "# Transform the dataset\n",
        "oversample = SMOTE()\n",
        "X, Y = oversample.fit_resample(X, Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVx8PKHrlbfd"
      },
      "source": [
        "## **Train logistic regression model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96YgCYl2liH8",
        "outputId": "fec55420-54fd-4fe1-85f1-ef9cba14e05c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Train the model\n",
        "X_train, X_test, Y_train, Y_test = tts(X, Y, test_size = 0.2)\n",
        "logistic_classifier = LogisticRegression(random_state = 0).fit(X_train,\n",
        "                                                               Y_train)\n",
        "\n",
        "# Calculate training accuracy\n",
        "Y_pred = logistic_classifier.predict(X_train)\n",
        "\n",
        "# Calculate the accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(\"Accuracy: \", accuracy_score(Y_train, Y_pred))"
      ],
      "execution_count": 427,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  0.6949846468781986\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCkMOdAP5z7W"
      },
      "source": [
        "## **Train Random Forest Classifier model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B83DcWr452qk",
        "outputId": "fe70f04d-43a7-4ea0-c3af-6cd96a1ac724",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Train the model\n",
        "X_train, X_test, Y_train, Y_test = tts(X, Y, test_size = 0.3)\n",
        "random_forest_classifier = RandomForestClassifier(max_depth=2, random_state=0)\n",
        "random_forest_classifier.fit(X_train, Y_train)\n",
        "\n",
        "# Calculate training accuracy\n",
        "Y_pred = random_forest_classifier.predict(X_train)\n",
        "\n",
        "# Calculate the accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(\"Accuracy: \", accuracy_score(Y_train, Y_pred))"
      ],
      "execution_count": 422,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  0.7169590643274854\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkcyuOJ9Ug8m"
      },
      "source": [
        "## **Train SVM model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ab3T5gt0Uj7a",
        "outputId": "a4434994-0bc6-4b93-bde7-7f301eb1af84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# Train the model\n",
        "X_train, X_test, Y_train, Y_test = tts(X, Y, test_size = 0.3)\n",
        "svm_classifier = SVC()\n",
        "svm_classifier.fit(X_train, Y_train)\n",
        "\n",
        "# Calculate training accuracy\n",
        "Y_pred = svm_classifier.predict(X_train)\n",
        "\n",
        "# Calculate the accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(\"Accuracy: \", accuracy_score(Y_train, Y_pred))"
      ],
      "execution_count": 407,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  0.7625730994152047\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}