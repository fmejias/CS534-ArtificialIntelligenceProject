{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI_Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fmejias/CS534-ArtificialIntelligenceProject/blob/main/CS534-ArtificialIntelligenceProject/AI_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pj5-J2tBsoGW"
      },
      "source": [
        "# **CS 534 - Artificial Intelligence**\n",
        "\n",
        "## **Project Title: Text Mining and Sentiment Analysis on Twitter for predicting students dropping out during the pandemic.**\n",
        "\n",
        "### **Students**\n",
        "\n",
        "\n",
        "*   Merzia Adamjee\n",
        "*   Alketa Guxha\n",
        "*   Felipe Mejias\n",
        "*   Nikita Boguslavskii\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ia8Bio_dt21i"
      },
      "source": [
        "# **Initial configuration of the environment for the development of the project**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OIfGOowqmL3"
      },
      "source": [
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from imblearn.over_sampling import SMOTE"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHwMawOhc6wI"
      },
      "source": [
        "# **Install Textblob and Imbalanced Learn**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_JgmJsnc9rW"
      },
      "source": [
        "!pip install textblob\n",
        "!pip install imbalanced-learn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_FDX7NI0dTp"
      },
      "source": [
        "# **Google Authentication to read CSV File from Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3moATrP0lQ_",
        "outputId": "6c72a160-39e0-43fa-b91b-33cc68cab83a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Needed for Google Authentication Step\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzTa-Jj31bEs"
      },
      "source": [
        "# **Upload Dataset from Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DR3T3xz-1erF"
      },
      "source": [
        "DATASET_PATH = \"/content/drive/My Drive/AI_Project_CS_534/Datasets/dropping_out_tweets_part1_labeled.csv\"\n",
        "dataset_df = pd.read_csv(DATASET_PATH, sep=\";\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYx-KyA43FAu"
      },
      "source": [
        "# **Dataset Information**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDtxCzzf3K6d"
      },
      "source": [
        "# Print a summary of the Dataset\n",
        "result = dataset_df.head(10)\n",
        "print(\"First 10 rows of the DataFrame:\")\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbbjj9bAOYQS"
      },
      "source": [
        "# **Dataset Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8rCPzSqPLp9"
      },
      "source": [
        "## **Select labeled dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNL3ca_PPPDJ"
      },
      "source": [
        "# NOTE: This selection is because they are the only label rows\n",
        "labeled_dataset_df = dataset_df.head(1200)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKc6gdgtOjJi"
      },
      "source": [
        "## **Filtering irrelevant examples**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WdCZhSjOawL",
        "outputId": "8cce2907-5909-4f96-8524-c49e5fc00b90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "IRRELEVANT_KEYWORDS = [\"Bernie\", \"Trump\", \"Sanders\", \"to become\", \n",
        "                       \"to pursue\", \"and becoming\", \"and going\",\n",
        "                       \"and be\", \"so I can\", \"so i can\", \"to run\",\n",
        "                       \"to spend\", \"to focus\", \"and living\", \"marry\",\n",
        "                       \"stripper\", \"and joining\", \"and pursuing\",\n",
        "                       \"bts\", \"BTS\", \"and running\", \"to go\", \"and making\",\n",
        "                       \"to dedicate\"]\n",
        "\n",
        "def filtering_irrelevant_examples(twitter_dataset):\n",
        "  \"\"\"\n",
        "  Filtering irrelevant tweets from the Twitter dataset.\n",
        "  \"\"\"\n",
        "  def check_tweet_relevance(tweet):\n",
        "    \"\"\"\n",
        "    Filtering irrelevant tweets from the Twitter dataset.\n",
        "    \"\"\"\n",
        "    if any(indicator in tweet for indicator in IRRELEVANT_KEYWORDS):\n",
        "      return \"irrelevant\"\n",
        "    return \"relevant\"\n",
        "  return twitter_dataset[twitter_dataset[\"tweet\"].apply(check_tweet_relevance) \\\n",
        "                         != \"irrelevant\"]\n",
        "\n",
        "# Filter the irrelevant tweets\n",
        "labeled_dataset_df = filtering_irrelevant_examples(labeled_dataset_df)\n",
        "rows, columns = labeled_dataset_df.shape\n",
        "print(\"New number of rows: \", rows)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "New number of rows:  936\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aW0f3PDbT4X8"
      },
      "source": [
        "## **Convert all letters to lower case**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJ2YCm5cT8ST"
      },
      "source": [
        "def convert_letters_to_lower_case(twitter_dataset):\n",
        "  \"\"\"\n",
        "  Convert all letters to lower case.\n",
        "  \"\"\"\n",
        "  def tweet_to_lower_case(tweet):\n",
        "    \"\"\"\n",
        "    Convert tweet text to lower case.\n",
        "    \"\"\"\n",
        "    return tweet.lower()\n",
        "\n",
        "  twitter_dataset[\"tweet\"] = twitter_dataset[\"tweet\"].apply(tweet_to_lower_case)\n",
        "\n",
        "# Convert all tweets to lower case\n",
        "convert_letters_to_lower_case(labeled_dataset_df)\n",
        "\n",
        "# Show results\n",
        "result = labeled_dataset_df.head(10)\n",
        "print(\"First 10 rows of the DataFrame after lower case:\")\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDE-F9yJVifX"
      },
      "source": [
        "## **Remove usernames that appear within a tweet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpJ-HfSPVm_s"
      },
      "source": [
        "def remove_usernames_from_tweets(twitter_dataset):\n",
        "  \"\"\"\n",
        "  Remove all usernames that appear on a tweet.\n",
        "  \"\"\"\n",
        "  def remove_username(tweet):\n",
        "    \"\"\"\n",
        "    Remove username from tweet.\n",
        "    \"\"\"\n",
        "    return re.sub('@[\\w]+','', tweet)\n",
        "\n",
        "  twitter_dataset[\"tweet\"] = twitter_dataset[\"tweet\"].apply(remove_username)\n",
        "\n",
        "# Remove all usernames that appear in a tweet\n",
        "remove_usernames_from_tweets(labeled_dataset_df)\n",
        "\n",
        "# Show results\n",
        "result = labeled_dataset_df.head(10)\n",
        "print(\"First 10 rows of the DataFrame after removing usernames:\")\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEFpLAeDbxLM"
      },
      "source": [
        "## **Remove hashtags that appear within a tweet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGTsQQ8Hbzav"
      },
      "source": [
        "def remove_hashtags_from_tweets(twitter_dataset):\n",
        "  \"\"\"\n",
        "  Remove all hashtags that appear on a tweet.\n",
        "  \"\"\"\n",
        "  def remove_hashtags(tweet):\n",
        "    \"\"\"\n",
        "    Remove hashtags from tweet.\n",
        "    \"\"\"\n",
        "    return tweet.replace(\"#\", \"\").replace(\"_\", \" \")\n",
        "\n",
        "  twitter_dataset[\"tweet\"] = twitter_dataset[\"tweet\"].apply(remove_hashtags)\n",
        "\n",
        "# Remove all hashtags that appear in a tweet\n",
        "remove_hashtags_from_tweets(labeled_dataset_df)\n",
        "\n",
        "# Show results\n",
        "result = labeled_dataset_df.head(10)\n",
        "print(\"First 10 rows of the DataFrame after removing usernames:\")\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qXBpdT2Z0HB"
      },
      "source": [
        "## **Remove special characters and punctuation that appear within a tweet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8F884syZ3d2"
      },
      "source": [
        "def remove_special_characters_and_punctuation_from_tweets(twitter_dataset):\n",
        "  \"\"\"\n",
        "  Remove all special characters and punctuation that appear on a tweet.\n",
        "  \"\"\"\n",
        "  def remove_special_characters_and_punctuation(tweet):\n",
        "    \"\"\"\n",
        "    Remove special characters and punctuation from tweet.\n",
        "    \"\"\"\n",
        "    return re.sub('[^A-Za-z0-9 ]+', '', tweet)\n",
        "\n",
        "  twitter_dataset[\"tweet\"] = twitter_dataset[\"tweet\"].apply(remove_special_characters_and_punctuation)\n",
        "\n",
        "# Remove all special characters and punctuation that appear in a tweet\n",
        "remove_special_characters_and_punctuation_from_tweets(labeled_dataset_df)\n",
        "\n",
        "# Show results\n",
        "result = labeled_dataset_df.head(10)\n",
        "print(\"First 10 rows of the DataFrame after removing usernames:\")\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alLP6ZB6aq3t"
      },
      "source": [
        "## **Remove URLs that appear within a tweet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQ2MbNsFat1g"
      },
      "source": [
        "def remove_urls_from_tweets(twitter_dataset):\n",
        "  \"\"\"\n",
        "  Remove all urls that appear on a tweet.\n",
        "  \"\"\"\n",
        "  def remove_urls(tweet):\n",
        "    \"\"\"\n",
        "    Remove urls from tweet.\n",
        "    \"\"\"\n",
        "    return re.sub(r'http\\S+', '', tweet)\n",
        "\n",
        "  twitter_dataset[\"tweet\"] = twitter_dataset[\"tweet\"].apply(remove_urls)\n",
        "\n",
        "# Remove all urls that appear in a tweet\n",
        "remove_urls_from_tweets(labeled_dataset_df)\n",
        "\n",
        "# Show results\n",
        "result = labeled_dataset_df.head(10)\n",
        "print(\"First 10 rows of the DataFrame after removing usernames:\")\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjQ2xaSrck31"
      },
      "source": [
        "# **Approach using Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCmqa8TicrPF"
      },
      "source": [
        "## **Create features using sentiment analysis and unigrams**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbFqaoU3cq2D"
      },
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "def calculate_features_using_polarity(twitter_dataset):\n",
        "  \"\"\"\n",
        "  Use Textblob polarity to calculate the number of positive and negative words.\n",
        "  \"\"\"\n",
        "  def calculate_positive_words(tweet):\n",
        "    \"\"\"\n",
        "    Count number of positive words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_positive_words = 0\n",
        "    for word in tweet.split():\n",
        "      if TextBlob(word).polarity >= 0:\n",
        "        number_of_positive_words = number_of_positive_words + 1\n",
        "    return number_of_positive_words\n",
        "  \n",
        "  def calculate_ratio_positive_words(tweet):\n",
        "    \"\"\"\n",
        "    Calculate the ratio of positive words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_positive_words = 0\n",
        "    for word in tweet.split():\n",
        "      if TextBlob(word).polarity >= 0:\n",
        "        number_of_positive_words = number_of_positive_words + 1\n",
        "    return number_of_positive_words/len(tweet.split())\n",
        "  \n",
        "  def calculate_negative_words(tweet):\n",
        "    \"\"\"\n",
        "    Count number of negative words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_negative_words = 0\n",
        "    for word in tweet.split():\n",
        "      if TextBlob(word).polarity < 0:\n",
        "        number_of_negative_words = number_of_negative_words + 1\n",
        "    return number_of_negative_words\n",
        "  \n",
        "  def calculate_ratio_negative_words(tweet):\n",
        "    \"\"\"\n",
        "    Calculate the ratio of negative words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_negative_words = 0\n",
        "    for word in tweet.split():\n",
        "      if TextBlob(word).polarity < 0:\n",
        "        number_of_negative_words = number_of_negative_words + 1\n",
        "    return number_of_negative_words/len(tweet.split())\n",
        "  \n",
        "  twitter_dataset[\"positive_words\"] = twitter_dataset[\"tweet\"].apply(calculate_positive_words)\n",
        "  twitter_dataset[\"ratio_positive_words\"] = twitter_dataset[\"tweet\"].apply(calculate_ratio_positive_words)\n",
        "  twitter_dataset[\"negative_words\"] = twitter_dataset[\"tweet\"].apply(calculate_negative_words)\n",
        "  twitter_dataset[\"ratio_negative_words\"] = twitter_dataset[\"tweet\"].apply(calculate_ratio_negative_words)\n",
        "\n",
        "# Calculate new features using sentiment Analysis\n",
        "calculate_features_using_polarity(labeled_dataset_df)\n",
        "\n",
        "# Show results\n",
        "result = labeled_dataset_df.head(10)\n",
        "print(\"First 10 rows of the DataFrame after removing usernames:\")\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2Wpltcvj-AX"
      },
      "source": [
        "## **Normalize calculated features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zanC63BTkBam"
      },
      "source": [
        "# Select calculated features\n",
        "dataset_sentiment_features = labeled_dataset_df[[\"positive_words\", \"ratio_positive_words\", \n",
        "                                                 \"negative_words\", \"ratio_negative_words\"]]\n",
        "\n",
        "# Normalize using the mean value\n",
        "normalized_df = (dataset_sentiment_features - dataset_sentiment_features.mean())/dataset_sentiment_features.std()\n",
        "\n",
        "# Show results\n",
        "result = normalized_df.head(10)\n",
        "print(\"First 10 rows of the DataFrame after removing usernames:\")\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KszH2igXfDBk"
      },
      "source": [
        "## **Handle imbalance classes using SMOTE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRylaSqlfGm8"
      },
      "source": [
        "print(\"Number of rows with intention of dropout: \", len(labeled_dataset_df[(labeled_dataset_df['label'] == \"Intention of dropout\")]))\n",
        "print(\"Number of rows with no intention of dropout: \", len(labeled_dataset_df[(labeled_dataset_df['label'] == \"Not intention of dropout\")]))\n",
        "\n",
        "# Select the data\n",
        "X = normalized_df\n",
        "Y = labeled_dataset_df[[\"label\"]]\n",
        "\n",
        "# Transform the dataset\n",
        "oversample = SMOTE()\n",
        "X, Y = oversample.fit_resample(X, np.ravel(Y))\n",
        "\n",
        "print(\"Number of rows with intention of dropout: \", len(X))\n",
        "print(\"Number of rows with no intention of dropout: \", len(Y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVx8PKHrlbfd"
      },
      "source": [
        "## **Train logistic regression model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96YgCYl2liH8",
        "outputId": "b14de6df-133e-491d-f674-35420c8a3854",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Train the model\n",
        "X_train, X_test, Y_train, Y_test = tts(X, Y, test_size = 0.2)\n",
        "logistic_classifier = LogisticRegression(random_state = 0).fit(X_train,\n",
        "                                                               Y_train)\n",
        "\n",
        "# Calculate predictions\n",
        "Y_pred = logistic_classifier.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(\"Accuracy: \", accuracy_score(Y_test, Y_pred))"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  0.7387755102040816\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCIUnJCrtPbw"
      },
      "source": [
        "## **Create features using sentiment analysis and bigrams**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLE5-vFztTyv"
      },
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "def find_ngrams(n, input_sequence):\n",
        "  # Split sentence into tokens.\n",
        "  tokens = input_sequence.split()\n",
        "  ngrams = []\n",
        "  for i in range(len(tokens) - n + 1):\n",
        "    # Take n consecutive tokens in array.\n",
        "    ngram = tokens[i:i+n]\n",
        "    # Concatenate array items into string.\n",
        "    ngram = ' '.join(ngram)\n",
        "    ngrams.append(ngram)\n",
        "  return ngrams\n",
        "\n",
        "def calculate_bigram_features_using_polarity(twitter_dataset):\n",
        "  \"\"\"\n",
        "  Use Textblob polarity to calculate the number of positive and negative words.\n",
        "  \"\"\"\n",
        "  def calculate_positive_words(tweet):\n",
        "    \"\"\"\n",
        "    Count number of positive words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_positive_bigrams = 0\n",
        "    ngrams = find_ngrams(2, tweet)\n",
        "    for ngram in ngrams:\n",
        "      if TextBlob(ngram).polarity >= 0:\n",
        "        number_of_positive_bigrams = number_of_positive_bigrams + 1\n",
        "    return number_of_positive_bigrams\n",
        "  \n",
        "  def calculate_ratio_positive_words(tweet):\n",
        "    \"\"\"\n",
        "    Calculate the ratio of positive words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_positive_bigrams = 0\n",
        "    ngrams = find_ngrams(2, tweet)\n",
        "    for ngram in ngrams:\n",
        "      if TextBlob(ngram).polarity >= 0:\n",
        "        number_of_positive_bigrams = number_of_positive_bigrams + 1\n",
        "    return number_of_positive_bigrams/len(ngrams)\n",
        "  \n",
        "  def calculate_negative_words(tweet):\n",
        "    \"\"\"\n",
        "    Count number of negative words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_negative_bigrams = 0\n",
        "    ngrams = find_ngrams(2, tweet)\n",
        "    for ngram in ngrams:\n",
        "      if TextBlob(ngram).polarity < 0:\n",
        "        number_of_negative_bigrams = number_of_negative_bigrams + 1\n",
        "    return number_of_negative_bigrams\n",
        "  \n",
        "  def calculate_ratio_negative_words(tweet):\n",
        "    \"\"\"\n",
        "    Calculate the ratio of negative words in a tweet.\n",
        "    \"\"\"\n",
        "    number_of_negative_bigrams = 0\n",
        "    ngrams = find_ngrams(2, tweet)\n",
        "    for ngram in ngrams:\n",
        "      if TextBlob(ngram).polarity < 0:\n",
        "        number_of_negative_bigrams = number_of_negative_bigrams + 1\n",
        "    return number_of_negative_bigrams/len(ngrams)\n",
        "  \n",
        "  twitter_dataset[\"bigram_positive_words\"] = twitter_dataset[\"tweet\"].apply(calculate_positive_words)\n",
        "  twitter_dataset[\"bigram_ratio_positive_words\"] = twitter_dataset[\"tweet\"].apply(calculate_ratio_positive_words)\n",
        "  twitter_dataset[\"bigram_negative_words\"] = twitter_dataset[\"tweet\"].apply(calculate_negative_words)\n",
        "  twitter_dataset[\"bigram_ratio_negative_words\"] = twitter_dataset[\"tweet\"].apply(calculate_ratio_negative_words)\n",
        "\n",
        "# Calculate new features using sentiment Analysis\n",
        "calculate_bigram_features_using_polarity(labeled_dataset_df)\n",
        "\n",
        "# Show results\n",
        "result = labeled_dataset_df.head(10)\n",
        "print(\"First 10 rows of the DataFrame after removing usernames:\")\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVFzOckgu-3S"
      },
      "source": [
        "## **Normalize calculated bigram features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I77EaUY2vCw5"
      },
      "source": [
        "# Select calculated features\n",
        "dataset_sentiment_features = labeled_dataset_df[[\"bigram_positive_words\", \"bigram_ratio_positive_words\",\n",
        "                                                 \"bigram_negative_words\", \"bigram_ratio_negative_words\"]]\n",
        "\n",
        "# Normalize using the mean value\n",
        "normalized_df = (dataset_sentiment_features - dataset_sentiment_features.mean())/dataset_sentiment_features.std()\n",
        "\n",
        "# Show results\n",
        "result = normalized_df.head(10)\n",
        "print(\"First 10 rows of the DataFrame after removing usernames:\")\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ef-OICovSdI"
      },
      "source": [
        "## **Train logistic regression model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKir_3KwvVRi"
      },
      "source": [
        "# Select the data\n",
        "X = normalized_df\n",
        "Y = labeled_dataset_df[[\"label\"]]\n",
        "\n",
        "# Transform the dataset\n",
        "oversample = SMOTE()\n",
        "X, Y = oversample.fit_resample(X, np.ravel(Y))\n",
        "\n",
        "# Train the model\n",
        "X_train, X_test, Y_train, Y_test = tts(X, Y, test_size = 0.2)\n",
        "logistic_classifier = LogisticRegression(random_state = 0).fit(X_train,\n",
        "                                                               Y_train)\n",
        "\n",
        "# Calculate predictions\n",
        "Y_pred = logistic_classifier.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(\"Accuracy: \", accuracy_score(Y_test, Y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}